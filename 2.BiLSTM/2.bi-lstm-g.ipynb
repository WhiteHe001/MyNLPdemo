{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 输入一句话,预测后面的一个单词(只预测一个)\n",
    "## 这个写的有点迷,能运行(大家凑合看吧.....)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "sentence总长度是: 22\n",
      "sentence不同单词的个数是: 20\n"
     ]
    }
   ],
   "source": [
    "sentence = (\n",
    "    'GitHub Actions makes it easy to automate all your software workflows '\n",
    "    'from continuous integration and delivery to issue triage and more END'\n",
    ")\n",
    "s1 = 'GitHub Actions makes it easy to automate all your software workflows from continuous integration and delivery to issue triage and more END'\n",
    "print(sentence == s1)\n",
    "\n",
    "print('sentence总长度是:', len(list(s1.split())))\n",
    "print('sentence不同单词的个数是:', len(set(list(s1.split()))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典是: {'continuous', 'makes', 'Actions', 'more', 'to', 'all', 'workflows', 'and', 'automate', 'triage', 'it', 'integration', 'your', 'from', 'software', 'issue', 'easy', 'GitHub', 'delivery', 'END'}\n",
      "字典长度是: 20\n"
     ]
    }
   ],
   "source": [
    "# word_list去重\n",
    "word_list = set(list(s1.split()))\n",
    "print('字典是:', word_list)\n",
    "print('字典长度是:', len(word_list))\n",
    "# 字典长度是: 20 也就是20分类问题"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'continuous': 0, 'makes': 1, 'Actions': 2, 'more': 3, 'to': 4, 'all': 5, 'workflows': 6, 'and': 7, 'automate': 8, 'triage': 9, 'it': 10, 'integration': 11, 'your': 12, 'from': 13, 'software': 14, 'issue': 15, 'easy': 16, 'GitHub': 17, 'delivery': 18, 'END': 19}\n",
      "{0: 'continuous', 1: 'makes', 2: 'Actions', 3: 'more', 4: 'to', 5: 'all', 6: 'workflows', 7: 'and', 8: 'automate', 9: 'triage', 10: 'it', 11: 'integration', 12: 'your', 13: 'from', 14: 'software', 15: 'issue', 16: 'easy', 17: 'GitHub', 18: 'delivery', 19: 'END'}\n"
     ]
    }
   ],
   "source": [
    "## 构建字典索引\n",
    "word2index = {v: k for k, v in enumerate(word_list)}\n",
    "index2word = {k: v for k, v in enumerate(word_list)}\n",
    "print(word2index)\n",
    "print(index2word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# sen_list句子中所有的单词，不去重\n",
    "sen_list = list(s1.split())\n",
    "print(len(sen_list))  # 22\n",
    "max_len = 22\n",
    "print(max_len)\n",
    "\n",
    "\n",
    "## 构建one-hot形式的字典索引\n",
    "# 句子长度是22 一共有20种不同的词\n",
    "# 也就是有21个输入,对应21个输出\n",
    "# 此时<END>对应的是16\n",
    "def make_sentence(s1):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    words = sentence.split()\n",
    "\n",
    "    for i in range(max_len - 1):\n",
    "        input1 = [word2index[n] for n in words[:(i + 1)]]\n",
    "        input1 = input1 + [0] * (max_len - len(input1))\n",
    "        # 占位符 句子长度是22,输入(最大也就是固定的)有21位\n",
    "        # 预测要从一号(也就是第2个)  开始预测\n",
    "        target1 = word2index[words[i + 1]]\n",
    "        # 20表示字典长度\n",
    "        input_batch.append(np.eye(20)[input1])\n",
    "        target_batch.append(target1)\n",
    "    return torch.Tensor(input_batch), torch.LongTensor(target_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 20)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(20).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) torch.Size([21, 22, 20]) 21\n",
      "tensor([ 2,  1, 10, 16,  4,  8,  5, 12, 14,  6, 13,  0, 11,  7, 18,  4, 15,  9,\n",
      "         7,  3, 19]) torch.Size([21]) 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pycharmshiyong\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = make_sentence(s1)\n",
    "print(input_batch, input_batch.shape, len(input_batch))\n",
    "# input_batch的形状是([21, 21, 20])\n",
    "# 21:sentence总长度是: 22个单词最多有21个输入,也就是有21个句子\n",
    "# 21:每个输入都经过添0后,变为21维,每个句子有21个单词\n",
    "# 20:字典长度是: 20,每个单词被编码为20维\n",
    "\n",
    "print(target_batch, target_batch.shape, len(target_batch))\n",
    "# torch.Size([21])\n",
    "# 21个输入有21个输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset11 = Data.TensorDataset(input_batch, target_batch)\n",
    "dataloader = Data.DataLoader(dataset=dataset11, batch_size=3,\n",
    "                             shuffle=False, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([ 2,  1, 10])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n",
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([16,  4,  8])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n",
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([ 5, 12, 14])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n",
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([ 6, 13,  0])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n",
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([11,  7, 18])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n",
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([ 4, 15,  9])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n",
      "tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) tensor([ 7,  3, 19])\n",
      "data.shape: torch.Size([3, 22, 20]) target.shape: torch.Size([3])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for data, target in dataloader:\n",
    "    print(data, target)\n",
    "    print('data.shape:', data.shape, 'target.shape:', target.shape)\n",
    "    print('----')\n",
    "# data.shape: torch.Size([3, 21, 20]) target.shape: torch.Size([3])\n",
    "#  3:DataLoader中batch_size=3,也就是有三个句子\n",
    "#  21: 每个句子有21个单词\n",
    "#  20: 每个单词被编码为20维\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./lstm.jpg\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 双向LSTM\n",
    "n_hidden = 8\n",
    "\n",
    "\n",
    "class MyBiLstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyBiLstm, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=20,\n",
    "                            hidden_size=n_hidden, bidirectional=True,\n",
    "                            num_layers=1)  # num_layers=1一层\n",
    "        self.quanlianjie = nn.Linear(in_features=n_hidden * 2,\n",
    "                                     out_features=len(word_list))\n",
    "        # word_list是字典长度,也就是去重后有多少种不同的字符\n",
    "\n",
    "    # data.shape: torch.Size([3, 21, 20])\n",
    "    def forward(self, x):\n",
    "        batch_size =x.shape[0]   # x.shape[0]  3\n",
    "        print(\"batch_size\",batch_size)\n",
    "        input22 = x.transpose(0, 1)\n",
    "        #input22:[max_len, batch_size, n_class(每个单词被编码为20维)]\n",
    "\n",
    "        h_0 = torch.zeros(1 * 2, batch_size, n_hidden)\n",
    "        # [层数*方向数, batch_size, n_hidden] # n_hidden = 8\n",
    "        # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        c_0 = torch.zeros(1 * 2, batch_size, n_hidden)\n",
    "        #\n",
    "        output, (hi, ci) = self.lstm(input22, (h_0, c_0))\n",
    "        ## 只要最后的输出 output是所有输出的集合,包括[output1,output2...]\n",
    "        # 此处只要最后一个\n",
    "        #output: [seq_len, batch,hidden_size *num_directions]\n",
    "        output = output[-1]\n",
    "        model = self.quanlianjie(output)\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./lstm1.jpg\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyBiLstm(\n",
      "  (lstm): LSTM(20, 8, bidirectional=True)\n",
      "  (quanlianjie): Linear(in_features=16, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model11 = MyBiLstm()\n",
    "print(model11)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "## 定义损失函数&优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model11.parameters(),lr=0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(2.9104, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(2.9402, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(2.9756, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(3.0353, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(3.2255, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(3.1240, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "0 tensor(3.1508, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(2.7990, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(2.8710, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(2.9452, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(3.0153, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(3.1915, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(3.0973, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "1 tensor(3.1228, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(2.7962, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(2.8489, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(2.9401, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(3.0126, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(3.1548, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(3.0671, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "2 tensor(3.0952, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(2.7962, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(2.8217, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(2.9338, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(3.0140, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(3.1099, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(3.0305, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "3 tensor(3.0651, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(2.7939, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(2.7774, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(2.9252, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(3.0241, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(3.0510, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(2.9819, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "4 tensor(3.0075, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(2.8004, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(2.7142, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(2.9195, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(3.0428, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(2.9914, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(2.9304, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "5 tensor(2.8693, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(2.8189, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(2.6713, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(2.9102, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(3.0368, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(2.9524, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(2.9191, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "6 tensor(2.7076, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(2.8160, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(2.6482, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(2.8830, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(3.0064, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(2.9285, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(2.8173, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "7 tensor(2.5269, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.8003, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.6326, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.8556, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.9643, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.9340, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.8835, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "8 tensor(2.2882, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(2.7762, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(2.6158, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(2.8238, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(2.9061, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(2.9739, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(3.3407, grad_fn=<NllLossBackward0>)\n",
      "batch_size 3\n",
      "torch.Size([3, 22, 20])\n",
      "9 tensor(3.4368, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### 训练\n",
    "for epoch in range(10):\n",
    "    for x, y in dataloader:\n",
    "        ## data和target在上面已经分析过了\n",
    "        pred = model11(x)  # 这里的x也就代表,上面的模型定义中的forward(self, x),也就是batch_size(3)个一组的自变量数据\n",
    "        print(x.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "        print(epoch, loss)\n",
    "        ###\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        optimizer.zero_grad()  # 梯度清零"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 21\n",
      "tensor([[ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 7],\n",
      "        [19]])\n",
      "['to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'and', 'END']\n"
     ]
    }
   ],
   "source": [
    "predict123 = model11(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(predict123)\n",
    "print([index2word[n.item()] for n in predict123.squeeze()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###########################################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# sentence = (\n",
    "#     'GitHub Actions makes it easy to automate all your software workflows '\n",
    "#     'from continuous integration and delivery to issue triage and more END'\n",
    "# )\n",
    "\n",
    "# ##test\n",
    "# input = [sen.split()[:2] for sen in sentences]\n",
    "# print(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./tset.jpg\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makes it easy <class 'str'> 13\n"
     ]
    }
   ],
   "source": [
    "## 构建测试集  要构建input22 h_0 c_0\n",
    "input_ceshi = 'makes it easy'\n",
    "print(input_ceshi, type(input_ceshi), len(input_ceshi))\n",
    "# h_ceshi = torch.zeros(2, 3, 8)\n",
    "# c_ceshi = torch.zeros(2, 3, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- tensor([[[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]]) torch.Size([21, 22, 20])\n",
      "-- tensor([ 2,  1, 10, 16,  4,  8,  5, 12, 14,  6, 13,  0, 11,  7, 18,  4, 15,  9,\n",
      "         7,  3, 19]) torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "## 将GitHub Actions makes it...转为指定的格式\n",
    "# 这里的输出,可能会带有前面函数里的,这里加---来做区别\n",
    "input_batch88, target_batch88 = make_sentence(input_ceshi)\n",
    "print('---', input_batch88, input_batch88.shape)  #torch.Size([21, 21, 20])\n",
    "print('--', target_batch88, target_batch88.shape)  #torch.Size([21])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "dataset88 = Data.TensorDataset(input_batch88, target_batch88)\n",
    "dataloader88 = Data.DataLoader(dataset=dataset88, batch_size=3,\n",
    "                               shuffle=False, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 3\n",
      "batch_size 3\n",
      "batch_size 3\n",
      "batch_size 3\n",
      "batch_size 3\n",
      "batch_size 3\n",
      "batch_size 3\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataloader88:\n",
    "    predict88 = model11(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0331,  0.0312, -0.2143, -0.5183,  0.4288,  0.0123, -0.0616,  0.4196,\n",
      "         -0.0742, -0.3536,  0.0791, -0.1539, -0.0099, -0.1395, -0.0863, -0.2741,\n",
      "         -0.0565, -1.6993, -0.1560, -0.4935],\n",
      "        [-0.1412, -0.0046, -0.4549, -0.0214,  0.2647, -0.0696, -0.1308,  0.5154,\n",
      "         -0.0592,  0.0040,  0.1352, -0.2248, -0.0203, -0.1215, -0.1419, -0.0502,\n",
      "         -0.1509, -1.1719, -0.1177,  0.2544],\n",
      "        [-0.2457, -0.0204, -0.6596,  0.4588,  0.1151, -0.1528, -0.2163,  0.5383,\n",
      "         -0.0196,  0.3041,  0.1751, -0.2678, -0.0287, -0.0762, -0.1436,  0.1523,\n",
      "         -0.2429, -0.6796, -0.1117,  0.9174]], grad_fn=<AddmmBackward0>) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "print(predict88, predict88.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4],\n",
      "        [ 7],\n",
      "        [19]])\n"
     ]
    }
   ],
   "source": [
    "predict = predict88.data.max(1, keepdim=True)[1]\n",
    "print(predict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'and', 'END']\n"
     ]
    }
   ],
   "source": [
    "print([index2word[n.item()] for n in predict.squeeze()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}