{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, modify by wmathor\n",
    "  Reference : https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n",
    "'''\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 13,  3],\n",
      "        [ 8,  0,  1],\n",
      "        [ 6,  5, 11],\n",
      "        [ 2,  9,  3],\n",
      "        [ 4,  7, 10],\n",
      "        [15, 12, 14]])\n",
      "tensor([1, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Bi-LSTM(Attention) Parameters\n",
    "batch_size = 2\n",
    "embedding_dim = 2\n",
    "n_hidden = 5  # number of hidden units in one cell\n",
    "num_classes = 2  # 0 or 1\n",
    "\n",
    "# 3 words sentences (=sequence_length is 3)\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "vocab = list(set(\" \".join(sentences).split()))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "\n",
    "def make_data(sentences):\n",
    "    inputs = []\n",
    "    for sen in sentences:\n",
    "        inputs.append(np.asarray([word2idx[n] for n in sen.split()]))\n",
    "\n",
    "    targets = []\n",
    "    for out in labels:\n",
    "        targets.append(out)  # To using Torch Softmax Loss function\n",
    "\n",
    "    return torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "\n",
    "\n",
    "inputs, targets = make_data(sentences)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "# dataset = TensorDataset(inps, tgts)\n",
    "dataset = Data.TensorDataset(inputs, targets)\n",
    "loader = Data.DataLoader(dataset, batch_size, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  9,  3],\n",
      "        [ 4,  7, 10]])\n",
      "tensor([0, 0])\n",
      "--------\n",
      "tensor([[ 8,  0,  1],\n",
      "        [ 2, 13,  3]])\n",
      "tensor([1, 1])\n",
      "--------\n",
      "tensor([[15, 12, 14],\n",
      "        [ 6,  5, 11]])\n",
      "tensor([0, 1])\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for i, j in loader:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print('--------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=n_hidden, num_layers=1, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=n_hidden * 2, out_features=num_classes)\n",
    "\n",
    "    # 应该的[n_step(句子长度),batch_size,  n_hidden * num_directions(=2)]\n",
    "\n",
    "    # 此处的lstm_output : [batch_size, n_step(句子长度), n_hidden * num_directions(=2)], F matrix----使用batch_first之后output的batch也会在最前\n",
    "    # self.attention_net(output, final_hidden_state)\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        batch_size = len(lstm_output)  # batch_size:\n",
    "        # batch_size = lstm_output.shape[1]  # batch_size:\n",
    "        print('batch_size:', batch_size, lstm_output.shape)\n",
    "        print('hn final_state:', final_state, final_state.shape)# torch.Size([2, 2, 5])\n",
    "\n",
    "        hidden = final_state.view(batch_size, -1, 1)\n",
    "\n",
    "        # lstm_output:torch.Size([2, 3, 10])\n",
    "        # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        #[2, 5*2, 1]\n",
    "        # [2,3,10]bmm[2,10,1]-->[2,3,1]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # attn_weights : [batch_size, n_step(句子长度)] [2,3]\n",
    "        print('attn_weights',attn_weights.shape)# [2,3]\n",
    "        # b = F.softmax(input,dim=0) # 按列SoftMax,列和为1\n",
    "        # c = F.softmax(input,dim=1)  # 按行SoftMax,行和为1\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        print('soft_attn_weights',soft_attn_weights.shape)\n",
    "        #[2,3]\n",
    "        # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        # lstm_output:torch.Size([2, 3, 10])-->[2,10,3]*[2,3,1]\n",
    "        ##[2,10,1]-sq->[2,10]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        print(\"context\",context.shape)# [2,10]\n",
    "        return context, soft_attn_weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X: [batch_size, seq_len]\n",
    "        '''\n",
    "        input = self.embedding(X)  # input : [batch_size, seq_len, embedding_dim]\n",
    "        print(\"input.shape\", input.shape)  # torch.Size([2, 3, 2])\n",
    "        # input = input.transpose(0, 1)\n",
    "        # input : [seq_len, batch_size, embedding_dim]\n",
    "\n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input)\n",
    "        print('lstm中h0的shape', final_hidden_state.shape)\n",
    "        print('lstm中output的shape', output.shape)\n",
    "        # output = output.transpose(0, 1)\n",
    "        # output : [batch_size, seq_len, n_hidden]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.fc(attn_output), attention  # model : [batch_size, num_classes], attention : [batch_size, n_step]\n",
    "\n",
    "\n",
    "model = BiLSTM_Attention().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0430,  0.0105, -0.0860,  0.0919,  0.1130],\n",
      "         [ 0.0343,  0.0550, -0.0297,  0.2401,  0.0551]],\n",
      "\n",
      "        [[-0.0545,  0.1114, -0.2408,  0.2723, -0.2515],\n",
      "         [ 0.0186,  0.0677, -0.2474,  0.1571, -0.1745]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0001 cost = 0.621468\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1865,  0.0725, -0.0467,  0.3273,  0.0246],\n",
      "         [ 0.1054, -0.0171, -0.2518,  0.0575,  0.0955]],\n",
      "\n",
      "        [[ 0.0551,  0.0337, -0.2221,  0.1019, -0.1564],\n",
      "         [-0.1278,  0.0978, -0.0724,  0.3830, -0.3027]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0001 cost = 0.728771\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0660,  0.0146, -0.1411,  0.0524,  0.1188],\n",
      "         [ 0.1397, -0.1361, -0.4176, -0.0195,  0.1110]],\n",
      "\n",
      "        [[-0.0655,  0.0424, -0.0598,  0.1558, -0.3357],\n",
      "         [-0.0911,  0.1065, -0.0848,  0.3824, -0.3254]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0001 cost = 0.677554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1056, -0.0181, -0.2531,  0.0574,  0.0954],\n",
      "         [ 0.1862,  0.0723, -0.0467,  0.3260,  0.0243]],\n",
      "\n",
      "        [[-0.1281,  0.0980, -0.0705,  0.3831, -0.3031],\n",
      "         [ 0.0555,  0.0340, -0.2228,  0.1020, -0.1559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0002 cost = 0.728595\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1401, -0.1390, -0.4196, -0.0191,  0.1107],\n",
      "         [ 0.0655,  0.0142, -0.1410,  0.0522,  0.1187]],\n",
      "\n",
      "        [[-0.0917,  0.1058, -0.0823,  0.3821, -0.3267],\n",
      "         [-0.0654,  0.0425, -0.0599,  0.1556, -0.3355]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0002 cost = 0.674382\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0421,  0.0098, -0.0855,  0.0904,  0.1127],\n",
      "         [ 0.0322,  0.0547, -0.0295,  0.2368,  0.0543]],\n",
      "\n",
      "        [[-0.0520,  0.1134, -0.2458,  0.2728, -0.2477],\n",
      "         [ 0.0205,  0.0691, -0.2507,  0.1577, -0.1712]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0002 cost = 0.620611\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0418,  0.0098, -0.0853,  0.0903,  0.1127],\n",
      "         [ 0.1058, -0.0204, -0.2554,  0.0574,  0.0950]],\n",
      "\n",
      "        [[-0.0514,  0.1139, -0.2471,  0.2730, -0.2467],\n",
      "         [-0.1289,  0.0977, -0.0666,  0.3826, -0.3044]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0003 cost = 0.665200\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1408, -0.1438, -0.4228, -0.0187,  0.1103],\n",
      "         [ 0.0310,  0.0549, -0.0293,  0.2360,  0.0542]],\n",
      "\n",
      "        [[-0.0926,  0.1051, -0.0785,  0.3820, -0.3287],\n",
      "         [ 0.0215,  0.0699, -0.2525,  0.1581, -0.1693]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0003 cost = 0.654292\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1851,  0.0720, -0.0466,  0.3235,  0.0236],\n",
      "         [ 0.0646,  0.0138, -0.1406,  0.0519,  0.1188]],\n",
      "\n",
      "        [[ 0.0563,  0.0345, -0.2242,  0.1021, -0.1549],\n",
      "         [-0.0649,  0.0430, -0.0607,  0.1554, -0.3344]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0003 cost = 0.702481\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0299,  0.0551, -0.0291,  0.2347,  0.0543],\n",
      "         [ 0.1061, -0.0220, -0.2574,  0.0574,  0.0949]],\n",
      "\n",
      "        [[ 0.0226,  0.0707, -0.2545,  0.1585, -0.1672],\n",
      "         [-0.1293,  0.0982, -0.0637,  0.3827, -0.3048]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0004 cost = 0.667593\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1847,  0.0721, -0.0465,  0.3223,  0.0235],\n",
      "         [ 0.1416, -0.1487, -0.4260, -0.0183,  0.1100]],\n",
      "\n",
      "        [[ 0.0569,  0.0350, -0.2252,  0.1023, -0.1540],\n",
      "         [-0.0935,  0.1046, -0.0748,  0.3820, -0.3307]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0004 cost = 0.714240\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0638,  0.0133, -0.1403,  0.0517,  0.1188],\n",
      "         [ 0.0402,  0.0093, -0.0847,  0.0890,  0.1128]],\n",
      "\n",
      "        [[-0.0646,  0.0434, -0.0613,  0.1551, -0.3337],\n",
      "         [-0.0486,  0.1162, -0.2529,  0.2735, -0.2423]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0004 cost = 0.638394\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0400,  0.0094, -0.0845,  0.0888,  0.1128],\n",
      "         [ 0.1063, -0.0240, -0.2596,  0.0574,  0.0947]],\n",
      "\n",
      "        [[-0.0479,  0.1167, -0.2543,  0.2737, -0.2412],\n",
      "         [-0.1300,  0.0984, -0.0603,  0.3825, -0.3055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0005 cost = 0.663037\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1842,  0.0719, -0.0464,  0.3211,  0.0231],\n",
      "         [ 0.0275,  0.0551, -0.0289,  0.2325,  0.0540]],\n",
      "\n",
      "        [[ 0.0576,  0.0354, -0.2263,  0.1024, -0.1531],\n",
      "         [ 0.0246,  0.0721, -0.2579,  0.1592, -0.1637]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0005 cost = 0.681600\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0632,  0.0134, -0.1399,  0.0517,  0.1189],\n",
      "         [ 0.1425, -0.1546, -0.4300, -0.0176,  0.1095]],\n",
      "\n",
      "        [[-0.0640,  0.0440, -0.0624,  0.1552, -0.3323],\n",
      "         [-0.0947,  0.1038, -0.0697,  0.3818, -0.3330]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0005 cost = 0.672822\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1427, -0.1560, -0.4309, -0.0174,  0.1094],\n",
      "         [ 0.1839,  0.0719, -0.0464,  0.3206,  0.0229]],\n",
      "\n",
      "        [[-0.0951,  0.1036, -0.0684,  0.3818, -0.3335],\n",
      "         [ 0.0582,  0.0358, -0.2273,  0.1026, -0.1522]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0006 cost = 0.710419\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0627,  0.0131, -0.1397,  0.0517,  0.1189],\n",
      "         [ 0.0259,  0.0550, -0.0287,  0.2314,  0.0536]],\n",
      "\n",
      "        [[-0.0637,  0.0443, -0.0629,  0.1550, -0.3318],\n",
      "         [ 0.0260,  0.0731, -0.2603,  0.1597, -0.1612]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0006 cost = 0.641259\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1066, -0.0267, -0.2629,  0.0579,  0.0944],\n",
      "         [ 0.0383,  0.0093, -0.0837,  0.0881,  0.1128]],\n",
      "\n",
      "        [[-0.1307,  0.0989, -0.0554,  0.3824, -0.3064],\n",
      "         [-0.0447,  0.1193, -0.2608,  0.2744, -0.2361]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0006 cost = 0.661331\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1068, -0.0272, -0.2635,  0.0580,  0.0944],\n",
      "         [ 0.0623,  0.0132, -0.1392,  0.0516,  0.1191]],\n",
      "\n",
      "        [[-0.1308,  0.0991, -0.0544,  0.3824, -0.3065],\n",
      "         [-0.0632,  0.0447, -0.0639,  0.1551, -0.3307]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0007 cost = 0.685927\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1836,  0.0718, -0.0462,  0.3190,  0.0225],\n",
      "         [ 0.0378,  0.0094, -0.0833,  0.0878,  0.1129]],\n",
      "\n",
      "        [[ 0.0593,  0.0365, -0.2291,  0.1029, -0.1506],\n",
      "         [-0.0433,  0.1206, -0.2637,  0.2749, -0.2337]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0007 cost = 0.677359\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0241,  0.0552, -0.0283,  0.2297,  0.0533],\n",
      "         [ 0.1440, -0.1638, -0.4359, -0.0163,  0.1088]],\n",
      "\n",
      "        [[ 0.0283,  0.0749, -0.2644,  0.1607, -0.1569],\n",
      "         [-0.0968,  0.1025, -0.0616,  0.3817, -0.3366]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0007 cost = 0.649000\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0234,  0.0553, -0.0283,  0.2292,  0.0533],\n",
      "         [ 0.1835,  0.0717, -0.0461,  0.3183,  0.0222]],\n",
      "\n",
      "        [[ 0.0289,  0.0754, -0.2656,  0.1609, -0.1557],\n",
      "         [ 0.0599,  0.0370, -0.2302,  0.1032, -0.1496]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0008 cost = 0.680747\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1072, -0.0294, -0.2660,  0.0583,  0.0942],\n",
      "         [ 0.0368,  0.0096, -0.0827,  0.0874,  0.1129]],\n",
      "\n",
      "        [[-0.1313,  0.0999, -0.0506,  0.3826, -0.3068],\n",
      "         [-0.0410,  0.1226, -0.2684,  0.2756, -0.2300]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0008 cost = 0.659341\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0612,  0.0132, -0.1383,  0.0516,  0.1193],\n",
      "         [ 0.1448, -0.1682, -0.4388, -0.0155,  0.1085]],\n",
      "\n",
      "        [[-0.0619,  0.0459, -0.0663,  0.1552, -0.3278],\n",
      "         [-0.0978,  0.1021, -0.0577,  0.3817, -0.3382]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0008 cost = 0.668757\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1074, -0.0306, -0.2674,  0.0585,  0.0941],\n",
      "         [ 0.1829,  0.0715, -0.0461,  0.3171,  0.0220]],\n",
      "\n",
      "        [[-0.1316,  0.1003, -0.0485,  0.3827, -0.3070],\n",
      "         [ 0.0609,  0.0377, -0.2318,  0.1035, -0.1481]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0009 cost = 0.725319\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0606,  0.0128, -0.1381,  0.0517,  0.1192],\n",
      "         [ 0.0208,  0.0551, -0.0280,  0.2274,  0.0527]],\n",
      "\n",
      "        [[-0.0616,  0.0463, -0.0667,  0.1551, -0.3273],\n",
      "         [ 0.0311,  0.0771, -0.2694,  0.1617, -0.1518]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0009 cost = 0.639047\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1454, -0.1730, -0.4418, -0.0145,  0.1081],\n",
      "         [ 0.0352,  0.0093, -0.0820,  0.0869,  0.1128]],\n",
      "\n",
      "        [[-0.0991,  0.1012, -0.0531,  0.3815, -0.3403],\n",
      "         [-0.0382,  0.1250, -0.2740,  0.2763, -0.2256]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0009 cost = 0.638338\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1458, -0.1749, -0.4429, -0.0144,  0.1080],\n",
      "         [ 0.0349,  0.0093, -0.0818,  0.0866,  0.1128]],\n",
      "\n",
      "        [[-0.0995,  0.1010, -0.0516,  0.3815, -0.3411],\n",
      "         [-0.0374,  0.1257, -0.2757,  0.2765, -0.2243]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0010 cost = 0.637565\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1079, -0.0336, -0.2703,  0.0585,  0.0938],\n",
      "         [ 0.0189,  0.0553, -0.0277,  0.2255,  0.0527]],\n",
      "\n",
      "        [[-0.1324,  0.1008, -0.0439,  0.3826, -0.3080],\n",
      "         [ 0.0331,  0.0788, -0.2731,  0.1625, -0.1480]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0010 cost = 0.660650\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0596,  0.0126, -0.1374,  0.0511,  0.1196],\n",
      "         [ 0.1819,  0.0714, -0.0458,  0.3146,  0.0217]],\n",
      "\n",
      "        [[-0.0606,  0.0474, -0.0688,  0.1552, -0.3249],\n",
      "         [ 0.0626,  0.0390, -0.2347,  0.1040, -0.1454]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0010 cost = 0.702330\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0176,  0.0554, -0.0275,  0.2238,  0.0529],\n",
      "         [ 0.1083, -0.0352, -0.2718,  0.0583,  0.0938]],\n",
      "\n",
      "        [[ 0.0346,  0.0800, -0.2757,  0.1631, -0.1452],\n",
      "         [-0.1328,  0.1013, -0.0417,  0.3827, -0.3083]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0011 cost = 0.659145\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0339,  0.0090, -0.0809,  0.0845,  0.1131],\n",
      "         [ 0.1815,  0.0715, -0.0457,  0.3133,  0.0217]],\n",
      "\n",
      "        [[-0.0340,  0.1288, -0.2828,  0.2776, -0.2188],\n",
      "         [ 0.0634,  0.0396, -0.2360,  0.1043, -0.1442]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0011 cost = 0.675372\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0589,  0.0124, -0.1369,  0.0507,  0.1198],\n",
      "         [ 0.1480, -0.1854, -0.4487, -0.0139,  0.1074]],\n",
      "\n",
      "        [[-0.0598,  0.0482, -0.0701,  0.1553, -0.3232],\n",
      "         [-0.1022,  0.1001, -0.0433,  0.3817, -0.3455]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0011 cost = 0.663774\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1812,  0.0714, -0.0457,  0.3127,  0.0216],\n",
      "         [ 0.0586,  0.0123, -0.1367,  0.0507,  0.1199]],\n",
      "\n",
      "        [[ 0.0641,  0.0401, -0.2370,  0.1045, -0.1432],\n",
      "         [-0.0595,  0.0485, -0.0707,  0.1554, -0.3226]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0012 cost = 0.703449\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0148,  0.0555, -0.0271,  0.2215,  0.0527],\n",
      "         [ 0.1486, -0.1889, -0.4507, -0.0136,  0.1072]],\n",
      "\n",
      "        [[ 0.0373,  0.0823, -0.2803,  0.1643, -0.1406],\n",
      "         [-0.1031,  0.0997, -0.0401,  0.3817, -0.3469]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0012 cost = 0.639259\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1089, -0.0389, -0.2753,  0.0584,  0.0937],\n",
      "         [ 0.0322,  0.0089, -0.0801,  0.0833,  0.1133]],\n",
      "\n",
      "        [[-0.1337,  0.1022, -0.0362,  0.3828, -0.3092],\n",
      "         [-0.0305,  0.1319, -0.2894,  0.2787, -0.2135]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0012 cost = 0.653225\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0577,  0.0122, -0.1360,  0.0506,  0.1202],\n",
      "         [ 0.1090, -0.0397, -0.2761,  0.0584,  0.0937]],\n",
      "\n",
      "        [[-0.0584,  0.0494, -0.0726,  0.1555, -0.3205],\n",
      "         [-0.1339,  0.1024, -0.0350,  0.3829, -0.3094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0013 cost = 0.679537\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1496, -0.1946, -0.4541, -0.0127,  0.1069],\n",
      "         [ 0.0313,  0.0087, -0.0797,  0.0827,  0.1134]],\n",
      "\n",
      "        [[-0.1048,  0.0988, -0.0346,  0.3815, -0.3494],\n",
      "         [-0.0288,  0.1335, -0.2929,  0.2792, -0.2108]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0013 cost = 0.629400\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1803,  0.0712, -0.0454,  0.3110,  0.0214],\n",
      "         [ 0.0114,  0.0556, -0.0268,  0.2191,  0.0528]],\n",
      "\n",
      "        [[ 0.0661,  0.0415, -0.2401,  0.1052, -0.1402],\n",
      "         [ 0.0402,  0.0846, -0.2854,  0.1655, -0.1352]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0013 cost = 0.678421\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1505, -0.1993, -0.4565, -0.0124,  0.1067],\n",
      "         [ 0.0568,  0.0120, -0.1354,  0.0503,  0.1204]],\n",
      "\n",
      "        [[-0.1061,  0.0983, -0.0307,  0.3815, -0.3514],\n",
      "         [-0.0574,  0.0504, -0.0744,  0.1557, -0.3184]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0014 cost = 0.656272\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1800,  0.0712, -0.0453,  0.3102,  0.0214],\n",
      "         [ 0.1096, -0.0434, -0.2793,  0.0584,  0.0936]],\n",
      "\n",
      "        [[ 0.0670,  0.0422, -0.2415,  0.1055, -0.1387],\n",
      "         [-0.1351,  0.1032, -0.0297,  0.3828, -0.3103]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0014 cost = 0.723065\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0299,  0.0075, -0.0791,  0.0811,  0.1135],\n",
      "         [ 0.0093,  0.0553, -0.0266,  0.2173,  0.0525]],\n",
      "\n",
      "        [[-0.0254,  0.1366, -0.2997,  0.2802, -0.2054],\n",
      "         [ 0.0421,  0.0863, -0.2887,  0.1663, -0.1318]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0014 cost = 0.605363\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1800,  0.0709, -0.0453,  0.3099,  0.0211],\n",
      "         [ 0.0294,  0.0075, -0.0789,  0.0808,  0.1136]],\n",
      "\n",
      "        [[ 0.0675,  0.0426, -0.2423,  0.1056, -0.1381],\n",
      "         [-0.0244,  0.1375, -0.3016,  0.2806, -0.2039]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0015 cost = 0.673707\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1103, -0.0467, -0.2818,  0.0587,  0.0934],\n",
      "         [ 0.0559,  0.0112, -0.1348,  0.0502,  0.1206]],\n",
      "\n",
      "        [[-0.1360,  0.1035, -0.0254,  0.3828, -0.3115],\n",
      "         [-0.0562,  0.0516, -0.0764,  0.1559, -0.3159]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0015 cost = 0.678543\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1527, -0.2110, -0.4626, -0.0117,  0.1060],\n",
      "         [ 0.0073,  0.0553, -0.0263,  0.2168,  0.0521]],\n",
      "\n",
      "        [[-0.1092,  0.0971, -0.0212,  0.3816, -0.3562],\n",
      "         [ 0.0444,  0.0883, -0.2924,  0.1673, -0.1278]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0015 cost = 0.628293\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0065,  0.0553, -0.0262,  0.2162,  0.0521],\n",
      "         [ 0.1532, -0.2136, -0.4638, -0.0115,  0.1059]],\n",
      "\n",
      "        [[ 0.0452,  0.0891, -0.2939,  0.1676, -0.1262],\n",
      "         [-0.1099,  0.0969, -0.0192,  0.3817, -0.3573]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0016 cost = 0.630097\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1798,  0.0705, -0.0452,  0.3090,  0.0208],\n",
      "         [ 0.0550,  0.0107, -0.1343,  0.0501,  0.1209]],\n",
      "\n",
      "        [[ 0.0693,  0.0440, -0.2449,  0.1062, -0.1352],\n",
      "         [-0.0550,  0.0528, -0.0785,  0.1562, -0.3134]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0016 cost = 0.703811\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0271,  0.0071, -0.0778,  0.0794,  0.1138],\n",
      "         [ 0.1110, -0.0508, -0.2851,  0.0588,  0.0934]],\n",
      "\n",
      "        [[-0.0191,  0.1425, -0.3114,  0.2824, -0.1957],\n",
      "         [-0.1369,  0.1046, -0.0205,  0.3831, -0.3122]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0016 cost = 0.646039\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0264,  0.0071, -0.0775,  0.0791,  0.1139],\n",
      "         [ 0.1547, -0.2213, -0.4678, -0.0106,  0.1056]],\n",
      "\n",
      "        [[-0.0180,  0.1435, -0.3135,  0.2827, -0.1940],\n",
      "         [-0.1123,  0.0960, -0.0128,  0.3816, -0.3608]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0017 cost = 0.623215\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0021,  0.0554, -0.0258,  0.2135,  0.0524],\n",
      "         [ 0.1113, -0.0528, -0.2869,  0.0591,  0.0933]],\n",
      "\n",
      "        [[ 0.0489,  0.0922, -0.3000,  0.1691, -0.1195],\n",
      "         [-0.1374,  0.1051, -0.0177,  0.3831, -0.3126]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0017 cost = 0.648772\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0535,  0.0103, -0.1337,  0.0501,  0.1214],\n",
      "         [ 0.1785,  0.0704, -0.0450,  0.3075,  0.0210]],\n",
      "\n",
      "        [[-0.0534,  0.0544, -0.0812,  0.1565, -0.3101],\n",
      "         [ 0.0714,  0.0455, -0.2481,  0.1069, -0.1317]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0017 cost = 0.702983\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1558, -0.2285, -0.4719, -0.0089,  0.1052],\n",
      "         [ 0.1114, -0.0548, -0.2888,  0.0594,  0.0933]],\n",
      "\n",
      "        [[-0.1147,  0.0948, -0.0060,  0.3813, -0.3645],\n",
      "         [-0.1380,  0.1055, -0.0146,  0.3831, -0.3133]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0018 cost = 0.665800\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0528,  0.0097, -0.1335,  0.0501,  0.1215],\n",
      "         [-0.0018,  0.0552, -0.0256,  0.2115,  0.0525]],\n",
      "\n",
      "        [[-0.0529,  0.0549, -0.0820,  0.1564, -0.3091],\n",
      "         [ 0.0513,  0.0941, -0.3040,  0.1700, -0.1155]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0018 cost = 0.628942\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1777,  0.0701, -0.0450,  0.3063,  0.0210],\n",
      "         [ 0.0224,  0.0063, -0.0767,  0.0778,  0.1143]],\n",
      "\n",
      "        [[ 0.0728,  0.0464, -0.2499,  0.1073, -0.1300],\n",
      "         [-0.0127,  0.1482, -0.3233,  0.2841, -0.1862]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0018 cost = 0.671457\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1572, -0.2366, -0.4762, -0.0075,  0.1047],\n",
      "         [ 0.0521,  0.0096, -0.1331,  0.0500,  0.1218]],\n",
      "\n",
      "        [[-0.1175,  0.0932,  0.0013,  0.3810, -0.3688],\n",
      "         [-0.0519,  0.0558, -0.0836,  0.1566, -0.3073]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0019 cost = 0.644511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1122, -0.0597, -0.2926,  0.0598,  0.0933],\n",
      "         [ 0.0212,  0.0062, -0.0763,  0.0772,  0.1145]],\n",
      "\n",
      "        [[-0.1397,  0.1059, -0.0080,  0.3829, -0.3150],\n",
      "         [-0.0103,  0.1504, -0.3276,  0.2849, -0.1826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0019 cost = 0.640705\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0060,  0.0553, -0.0252,  0.2091,  0.0528],\n",
      "         [ 0.1774,  0.0699, -0.0448,  0.3054,  0.0210]],\n",
      "\n",
      "        [[ 0.0551,  0.0975, -0.3103,  0.1716, -0.1086],\n",
      "         [ 0.0744,  0.0477, -0.2523,  0.1079, -0.1273]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0019 cost = 0.675492\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0200,  0.0062, -0.0757,  0.0766,  0.1146],\n",
      "         [ 0.0511,  0.0093, -0.1324,  0.0498,  0.1222]],\n",
      "\n",
      "        [[-0.0078,  0.1528, -0.3321,  0.2858, -0.1788],\n",
      "         [-0.0503,  0.0573, -0.0864,  0.1571, -0.3040]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0020 cost = 0.623485\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1593, -0.2463, -0.4810, -0.0064,  0.1044],\n",
      "         [ 0.1771,  0.0699, -0.0447,  0.3049,  0.0211]],\n",
      "\n",
      "        [[-0.1205,  0.0927,  0.0096,  0.3814, -0.3730],\n",
      "         [ 0.0757,  0.0488, -0.2541,  0.1084, -0.1250]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0020 cost = 0.691460\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0091,  0.0555, -0.0248,  0.2075,  0.0529],\n",
      "         [ 0.1131, -0.0635, -0.2958,  0.0600,  0.0933]],\n",
      "\n",
      "        [[ 0.0581,  0.1004, -0.3151,  0.1731, -0.1032],\n",
      "         [-0.1405,  0.1075, -0.0029,  0.3834, -0.3155]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0020 cost = 0.641915\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1133, -0.0648, -0.2968,  0.0601,  0.0933],\n",
      "         [-0.0104,  0.0555, -0.0247,  0.2067,  0.0529]],\n",
      "\n",
      "        [[-0.1409,  0.1077, -0.0013,  0.3835, -0.3159],\n",
      "         [ 0.0590,  0.1013, -0.3166,  0.1735, -0.1016]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0021 cost = 0.642050\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1610, -0.2546, -0.4851, -0.0053,  0.1040],\n",
      "         [ 0.0496,  0.0093, -0.1315,  0.0496,  0.1227]],\n",
      "\n",
      "        [[-0.1231,  0.0918,  0.0164,  0.3814, -0.3771],\n",
      "         [-0.0480,  0.0594, -0.0903,  0.1579, -0.2994]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0021 cost = 0.638268\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0160,  0.0063, -0.0745,  0.0745,  0.1152],\n",
      "         [ 0.1760,  0.0696, -0.0446,  0.3028,  0.0212]],\n",
      "\n",
      "        [[-0.0012,  0.1592, -0.3433,  0.2883, -0.1691],\n",
      "         [ 0.0777,  0.0504, -0.2569,  0.1091, -0.1219]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0021 cost = 0.669207\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0150,  0.0555, -0.0244,  0.2044,  0.0531],\n",
      "         [ 0.1624, -0.2609, -0.4880, -0.0047,  0.1037]],\n",
      "\n",
      "        [[ 0.0620,  0.1040, -0.3213,  0.1748, -0.0965],\n",
      "         [-0.1252,  0.0910,  0.0214,  0.3814, -0.3801]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0022 cost = 0.610960\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1753,  0.0694, -0.0446,  0.3022,  0.0213],\n",
      "         [ 0.0141,  0.0061, -0.0741,  0.0738,  0.1154]],\n",
      "\n",
      "        [[ 0.0789,  0.0513, -0.2584,  0.1095, -0.1202],\n",
      "         [ 0.0014,  0.1617, -0.3477,  0.2892, -0.1654]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0022 cost = 0.668478\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0478,  0.0086, -0.1309,  0.0494,  0.1232],\n",
      "         [ 0.1144, -0.0717, -0.3020,  0.0604,  0.0932]],\n",
      "\n",
      "        [[-0.0459,  0.0614, -0.0936,  0.1584, -0.2954],\n",
      "         [-0.1428,  0.1088,  0.0067,  0.3836, -0.3179]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0022 cost = 0.666371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0123,  0.0061, -0.0736,  0.0732,  0.1155],\n",
      "         [ 0.1642, -0.2698, -0.4924, -0.0034,  0.1033]],\n",
      "\n",
      "        [[ 0.0042,  0.1644, -0.3523,  0.2901, -0.1615],\n",
      "         [-0.1283,  0.0898,  0.0291,  0.3813, -0.3846]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0023 cost = 0.602886\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1748,  0.0690, -0.0446,  0.3017,  0.0212],\n",
      "         [-0.0211,  0.0555, -0.0241,  0.2021,  0.0534]],\n",
      "\n",
      "        [[ 0.0805,  0.0526, -0.2605,  0.1100, -0.1178],\n",
      "         [ 0.0662,  0.1080, -0.3279,  0.1766, -0.0893]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0023 cost = 0.672685\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0466,  0.0082, -0.1305,  0.0497,  0.1236],\n",
      "         [ 0.1150, -0.0755, -0.3050,  0.0611,  0.0932]],\n",
      "\n",
      "        [[-0.0443,  0.0631, -0.0962,  0.1589, -0.2923],\n",
      "         [-0.1439,  0.1095,  0.0117,  0.3837, -0.3191]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0023 cost = 0.664646\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1658, -0.2778, -0.4966, -0.0014,  0.1029],\n",
      "         [ 0.1152, -0.0767, -0.3060,  0.0614,  0.0932]],\n",
      "\n",
      "        [[-0.1315,  0.0884,  0.0370,  0.3810, -0.3889],\n",
      "         [-0.1444,  0.1097,  0.0136,  0.3837, -0.3196]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0024 cost = 0.645997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0084,  0.0055, -0.0728,  0.0726,  0.1158],\n",
      "         [ 0.0458,  0.0076, -0.1303,  0.0500,  0.1237]],\n",
      "\n",
      "        [[ 0.0096,  0.1693, -0.3612,  0.2918, -0.1543],\n",
      "         [-0.0434,  0.0639, -0.0974,  0.1590, -0.2910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0024 cost = 0.615527\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1740,  0.0684, -0.0446,  0.3017,  0.0209],\n",
      "         [-0.0268,  0.0553, -0.0237,  0.2006,  0.0533]],\n",
      "\n",
      "        [[ 0.0826,  0.0541, -0.2631,  0.1106, -0.1151],\n",
      "         [ 0.0702,  0.1116, -0.3341,  0.1783, -0.0829]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0024 cost = 0.671668\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 1.6770e-01, -2.8712e-01, -5.0118e-01,  3.7438e-04,  1.0249e-01],\n",
      "         [ 1.1610e-01, -8.1328e-02, -3.0948e-01,  6.2359e-02,  9.3045e-02]],\n",
      "\n",
      "        [[-1.3512e-01,  8.6414e-02,  4.5701e-02,  3.8052e-01, -3.9412e-01],\n",
      "         [-1.4636e-01,  1.0975e-01,  2.0047e-02,  3.8347e-01, -3.2182e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0025 cost = 0.641583\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1739,  0.0681, -0.0446,  0.3019,  0.0207],\n",
      "         [ 0.0447,  0.0072, -0.1298,  0.0505,  0.1241]],\n",
      "\n",
      "        [[ 0.0835,  0.0548, -0.2642,  0.1108, -0.1140],\n",
      "         [-0.0418,  0.0655, -0.1000,  0.1594, -0.2880]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0025 cost = 0.704333\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0300,  0.0548, -0.0236,  0.2002,  0.0528],\n",
      "         [ 0.0051,  0.0046, -0.0723,  0.0723,  0.1159]],\n",
      "\n",
      "        [[ 0.0727,  0.1140, -0.3380,  0.1793, -0.0788],\n",
      "         [ 0.0147,  0.1742, -0.3696,  0.2935, -0.1474]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0025 cost = 0.579855\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0041,  0.0048, -0.0720,  0.0720,  0.1161],\n",
      "         [ 0.0442,  0.0069, -0.1295,  0.0508,  0.1242]],\n",
      "\n",
      "        [[ 0.0163,  0.1758, -0.3722,  0.2941, -0.1451],\n",
      "         [-0.0405,  0.0666, -0.1019,  0.1597, -0.2858]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0026 cost = 0.612636\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1704, -0.2987, -0.5067,  0.0023,  0.1020],\n",
      "         [ 0.1740,  0.0678, -0.0446,  0.3023,  0.0206]],\n",
      "\n",
      "        [[-0.1397,  0.0845,  0.0564,  0.3803, -0.4005],\n",
      "         [ 0.0854,  0.0563, -0.2666,  0.1115, -0.1110]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0026 cost = 0.679147\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1178, -0.0880, -0.3145,  0.0635,  0.0929],\n",
      "         [-0.0345,  0.0552, -0.0232,  0.1989,  0.0532]],\n",
      "\n",
      "        [[-0.1488,  0.1107,  0.0290,  0.3837, -0.3243],\n",
      "         [ 0.0767,  0.1179, -0.3439,  0.1813, -0.0720]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0026 cost = 0.627954\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1181, -0.0894, -0.3156,  0.0637,  0.0929],\n",
      "         [ 0.1741,  0.0675, -0.0446,  0.3020,  0.0205]],\n",
      "\n",
      "        [[-0.1493,  0.1110,  0.0308,  0.3838, -0.3247],\n",
      "         [ 0.0866,  0.0574, -0.2682,  0.1120, -0.1091]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0027 cost = 0.716717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.6895e-02,  5.5022e-02, -2.3087e-02,  1.9831e-01,  5.2754e-02],\n",
      "         [ 2.1129e-04,  5.5042e-03, -7.0872e-02,  7.1071e-02,  1.1642e-01]],\n",
      "\n",
      "        [[ 7.8476e-02,  1.1975e-01, -3.4655e-01,  1.8211e-01, -6.9126e-02],\n",
      "         [ 2.2884e-02,  1.8224e-01, -3.8213e-01,  2.9662e-01, -1.3601e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0027 cost = 0.574352\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1733, -0.3104, -0.5122,  0.0043,  0.1014],\n",
      "         [ 0.0424,  0.0070, -0.1284,  0.0513,  0.1248]],\n",
      "\n",
      "        [[-0.1441,  0.0827,  0.0666,  0.3801, -0.4067],\n",
      "         [-0.0367,  0.0700, -0.1079,  0.1611, -0.2788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0027 cost = 0.617978\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0021,  0.0058, -0.0703,  0.0704,  0.1166],\n",
      "         [ 0.1739,  0.0670, -0.0447,  0.3018,  0.0200]],\n",
      "\n",
      "        [[ 0.0263,  0.1857, -0.3872,  0.2980, -0.1314],\n",
      "         [ 0.0879,  0.0585, -0.2697,  0.1125, -0.1073]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0028 cost = 0.662839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0419,  0.0552, -0.0227,  0.1970,  0.0527],\n",
      "         [ 0.0415,  0.0070, -0.1280,  0.0514,  0.1251]],\n",
      "\n",
      "        [[ 0.0821,  0.1235, -0.3518,  0.1839, -0.0632],\n",
      "         [-0.0351,  0.0716, -0.1105,  0.1617, -0.2758]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0028 cost = 0.612329\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1198, -0.0962, -0.3207,  0.0647,  0.0928],\n",
      "         [ 0.1754, -0.3185, -0.5159,  0.0053,  0.1012]],\n",
      "\n",
      "        [[-0.1514,  0.1125,  0.0393,  0.3844, -0.3269],\n",
      "         [-0.1473,  0.0823,  0.0737,  0.3805, -0.4108]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0028 cost = 0.638074\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0458,  0.0555, -0.0225,  0.1959,  0.0530],\n",
      "         [ 0.1734,  0.0668, -0.0447,  0.3021,  0.0200]],\n",
      "\n",
      "        [[ 0.0850,  0.1265, -0.3560,  0.1853, -0.0583],\n",
      "         [ 0.0902,  0.0605, -0.2725,  0.1134, -0.1038]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0029 cost = 0.668262\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1205, -0.0991, -0.3228,  0.0653,  0.0928],\n",
      "         [ 0.0399,  0.0073, -0.1272,  0.0517,  0.1256]],\n",
      "\n",
      "        [[-0.1523,  0.1129,  0.0426,  0.3845, -0.3278],\n",
      "         [-0.0322,  0.0741, -0.1151,  0.1628, -0.2707]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0029 cost = 0.655628\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1773, -0.3272, -0.5198,  0.0071,  0.1008],\n",
      "         [-0.0079,  0.0067, -0.0688,  0.0692,  0.1172]],\n",
      "\n",
      "        [[-0.1512,  0.0808,  0.0817,  0.3801, -0.4159],\n",
      "         [ 0.0353,  0.1945, -0.4001,  0.3016, -0.1194]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0029 cost = 0.568757\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0089,  0.0067, -0.0686,  0.0687,  0.1173],\n",
      "         [ 0.1212, -0.1023, -0.3250,  0.0659,  0.0927]],\n",
      "\n",
      "        [[ 0.0372,  0.1964, -0.4028,  0.3023, -0.1169],\n",
      "         [-0.1535,  0.1133,  0.0463,  0.3846, -0.3288]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0030 cost = 0.611158\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0526,  0.0556, -0.0221,  0.1943,  0.0530],\n",
      "         [ 0.0383,  0.0071, -0.1265,  0.0520,  0.1260]],\n",
      "\n",
      "        [[ 0.0902,  0.1317, -0.3634,  0.1878, -0.0500],\n",
      "         [-0.0296,  0.0765, -0.1192,  0.1637, -0.2661]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0030 cost = 0.606731\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1796, -0.3364, -0.5240,  0.0083,  0.1004],\n",
      "         [ 0.1717,  0.0666, -0.0446,  0.3021,  0.0201]],\n",
      "\n",
      "        [[-0.1553,  0.0795,  0.0902,  0.3800, -0.4211],\n",
      "         [ 0.0941,  0.0639, -0.2773,  0.1149, -0.0979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0030 cost = 0.670956\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0570,  0.0559, -0.0218,  0.1922,  0.0535],\n",
      "         [-0.0128,  0.0073, -0.0676,  0.0668,  0.1179]],\n",
      "\n",
      "        [[ 0.0934,  0.1349, -0.3679,  0.1893, -0.0448],\n",
      "         [ 0.0434,  0.2024, -0.4115,  0.3048, -0.1088]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0031 cost = 0.560031\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0360,  0.0076, -0.1256,  0.0513,  0.1268],\n",
      "         [ 0.1224, -0.1080, -0.3294,  0.0662,  0.0929]],\n",
      "\n",
      "        [[-0.0262,  0.0797, -0.1248,  0.1652, -0.2599],\n",
      "         [-0.1552,  0.1150,  0.0530,  0.3852, -0.3300]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0031 cost = 0.647953\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1820, -0.3456, -0.5282,  0.0097,  0.1001],\n",
      "         [ 0.1696,  0.0667, -0.0445,  0.3000,  0.0208]],\n",
      "\n",
      "        [[-0.1595,  0.0784,  0.0986,  0.3799, -0.4265],\n",
      "         [ 0.0972,  0.0665, -0.2812,  0.1161, -0.0931]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0031 cost = 0.669545\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0346,  0.0078, -0.1251,  0.0507,  0.1273],\n",
      "         [-0.0637,  0.0565, -0.0213,  0.1884,  0.0543]],\n",
      "\n",
      "        [[-0.0240,  0.0817, -0.1281,  0.1660, -0.2563],\n",
      "         [ 0.0983,  0.1400, -0.3748,  0.1918, -0.0370]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0032 cost = 0.598203\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1839, -0.3524, -0.5312,  0.0107,  0.0999],\n",
      "         [ 0.1234, -0.1125, -0.3326,  0.0661,  0.0931]],\n",
      "\n",
      "        [[-0.1627,  0.0772,  0.1046,  0.3798, -0.4306],\n",
      "         [-0.1566,  0.1162,  0.0581,  0.3857, -0.3312]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0032 cost = 0.613260\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0189,  0.0084, -0.0661,  0.0628,  0.1191],\n",
      "         [ 0.1683,  0.0666, -0.0444,  0.2975,  0.0213]],\n",
      "\n",
      "        [[ 0.0541,  0.2130, -0.4261,  0.3092, -0.0952],\n",
      "         [ 0.0998,  0.0688, -0.2844,  0.1171, -0.0894]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0032 cost = 0.657452\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0691,  0.0568, -0.0210,  0.1850,  0.0550],\n",
      "         [-0.0197,  0.0083, -0.0659,  0.0624,  0.1192]],\n",
      "\n",
      "        [[ 0.1029,  0.1447, -0.3810,  0.1941, -0.0301],\n",
      "         [ 0.0560,  0.2148, -0.4285,  0.3099, -0.0930]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0033 cost = 0.550885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0322,  0.0080, -0.1240,  0.0495,  0.1284],\n",
      "         [ 0.1871, -0.3634, -0.5361,  0.0123,  0.0995]],\n",
      "\n",
      "        [[-0.0197,  0.0857, -0.1349,  0.1678, -0.2491],\n",
      "         [-0.1679,  0.0750,  0.1143,  0.3794, -0.4373]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0033 cost = 0.600321\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1252, -0.1191, -0.3371,  0.0662,  0.0933],\n",
      "         [ 0.1676,  0.0667, -0.0443,  0.2964,  0.0220]],\n",
      "\n",
      "        [[-0.1589,  0.1174,  0.0656,  0.3862, -0.3334],\n",
      "         [ 0.1028,  0.0713, -0.2879,  0.1183, -0.0852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0033 cost = 0.715025\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0310,  0.0082, -0.1234,  0.0493,  0.1289],\n",
      "         [-0.0235,  0.0092, -0.0649,  0.0605,  0.1200]],\n",
      "\n",
      "        [[-0.0174,  0.0878, -0.1383,  0.1687, -0.2455],\n",
      "         [ 0.0629,  0.2215, -0.4373,  0.3128, -0.0847]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0034 cost = 0.582159\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0770,  0.0575, -0.0204,  0.1816,  0.0560],\n",
      "         [ 0.1675,  0.0666, -0.0443,  0.2966,  0.0219]],\n",
      "\n",
      "        [[ 0.1096,  0.1516, -0.3899,  0.1975, -0.0201],\n",
      "         [ 0.1044,  0.0726, -0.2895,  0.1189, -0.0832]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0034 cost = 0.663219\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1906, -0.3754, -0.5415,  0.0142,  0.0990],\n",
      "         [ 0.1266, -0.1234, -0.3402,  0.0669,  0.0933]],\n",
      "\n",
      "        [[-0.1742,  0.0726,  0.1262,  0.3789, -0.4449],\n",
      "         [-0.1606,  0.1182,  0.0712,  0.3866, -0.3352]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0034 cost = 0.602561\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1274, -0.1254, -0.3415,  0.0672,  0.0933],\n",
      "         [ 0.1918, -0.3791, -0.5432,  0.0148,  0.0989]],\n",
      "\n",
      "        [[-0.1615,  0.1181,  0.0738,  0.3866, -0.3363],\n",
      "         [-0.1761,  0.0715,  0.1297,  0.3787, -0.4473]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0035 cost = 0.613388\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0269,  0.0094, -0.0640,  0.0592,  0.1205],\n",
      "         [ 0.1691,  0.0662, -0.0444,  0.2975,  0.0216]],\n",
      "\n",
      "        [[ 0.0711,  0.2293, -0.4474,  0.3160, -0.0755],\n",
      "         [ 0.1060,  0.0739, -0.2910,  0.1195, -0.0816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0035 cost = 0.653687\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0301,  0.0076, -0.1226,  0.0502,  0.1295],\n",
      "         [-0.0805,  0.0572, -0.0202,  0.1812,  0.0555]],\n",
      "\n",
      "        [[-0.0130,  0.0920, -0.1446,  0.1704, -0.2390],\n",
      "         [ 0.1143,  0.1564, -0.3956,  0.1998, -0.0140]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0035 cost = 0.587031\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1301, -0.1320, -0.3459,  0.0693,  0.0929],\n",
      "         [-0.0281,  0.0090, -0.0637,  0.0593,  0.1205]],\n",
      "\n",
      "        [[-0.1651,  0.1168,  0.0822,  0.3859, -0.3408],\n",
      "         [ 0.0748,  0.2327, -0.4518,  0.3174, -0.0717]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0036 cost = 0.587651\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1960, -0.3938, -0.5498,  0.0182,  0.0981],\n",
      "         [-0.0828,  0.0573, -0.0200,  0.1810,  0.0556]],\n",
      "\n",
      "        [[-0.1842,  0.0663,  0.1441,  0.3769, -0.4573],\n",
      "         [ 0.1173,  0.1595, -0.3994,  0.2012, -0.0100]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0036 cost = 0.541975\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1722,  0.0656, -0.0446,  0.2998,  0.0213],\n",
      "         [ 0.0292,  0.0075, -0.1221,  0.0504,  0.1300]],\n",
      "\n",
      "        [[ 0.1087,  0.0760, -0.2933,  0.1203, -0.0790],\n",
      "         [-0.0098,  0.0950, -0.1492,  0.1717, -0.2342]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0036 cost = 0.701081\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0291,  0.0074, -0.1218,  0.0504,  0.1302],\n",
      "         [ 0.1982, -0.4010, -0.5530,  0.0195,  0.0977]],\n",
      "\n",
      "        [[-0.0086,  0.0961, -0.1510,  0.1721, -0.2324],\n",
      "         [-0.1882,  0.0643,  0.1509,  0.3763, -0.4621]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0037 cost = 0.583665\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1329, -0.1396, -0.3508,  0.0704,  0.0929],\n",
      "         [-0.0866,  0.0574, -0.0197,  0.1786,  0.0561]],\n",
      "\n",
      "        [[-0.1684,  0.1170,  0.0911,  0.3859, -0.3444],\n",
      "         [ 0.1223,  0.1647, -0.4058,  0.2038, -0.0029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0037 cost = 0.591530\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0324,  0.0090, -0.0625,  0.0566,  0.1214],\n",
      "         [ 0.1734,  0.0653, -0.0446,  0.2999,  0.0216]],\n",
      "\n",
      "        [[ 0.0864,  0.2439, -0.4658,  0.3220, -0.0588],\n",
      "         [ 0.1114,  0.0783, -0.2964,  0.1214, -0.0755]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0037 cost = 0.650177\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0333,  0.0090, -0.0623,  0.0563,  0.1216],\n",
      "         [ 0.1745,  0.0652, -0.0446,  0.3008,  0.0216]],\n",
      "\n",
      "        [[ 0.0888,  0.2461, -0.4684,  0.3230, -0.0563],\n",
      "         [ 0.1122,  0.0789, -0.2971,  0.1217, -0.0747]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0038 cost = 0.649578\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0896,  0.0575, -0.0194,  0.1782,  0.0562],\n",
      "         [ 0.1353, -0.1451, -0.3544,  0.0719,  0.0928]],\n",
      "\n",
      "        [[ 0.1271,  0.1697, -0.4116,  0.2063,  0.0034],\n",
      "         [-0.1709,  0.1171,  0.0977,  0.3858, -0.3471]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0038 cost = 0.586018\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2030, -0.4168, -0.5602,  0.0228,  0.0970],\n",
      "         [ 0.0284,  0.0074, -0.1206,  0.0516,  0.1311]],\n",
      "\n",
      "        [[-0.1978,  0.0594,  0.1672,  0.3747, -0.4727],\n",
      "         [-0.0024,  0.1016, -0.1597,  0.1746, -0.2233]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0038 cost = 0.568581\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1370, -0.1488, -0.3569,  0.0732,  0.0927],\n",
      "         [ 0.0282,  0.0074, -0.1203,  0.0518,  0.1313]],\n",
      "\n",
      "        [[-0.1727,  0.1171,  0.1026,  0.3858, -0.3491],\n",
      "         [-0.0011,  0.1028, -0.1616,  0.1751, -0.2214]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0039 cost = 0.627178\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2052, -0.4236, -0.5633,  0.0240,  0.0967],\n",
      "         [-0.0936,  0.0576, -0.0190,  0.1782,  0.0563]],\n",
      "\n",
      "        [[-0.2018,  0.0575,  0.1738,  0.3741, -0.4771],\n",
      "         [ 0.1321,  0.1749, -0.4175,  0.2088,  0.0098]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0039 cost = 0.524592\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0380,  0.0094, -0.0609,  0.0546,  0.1224],\n",
      "         [ 0.1794,  0.0645, -0.0448,  0.3054,  0.0214]],\n",
      "\n",
      "        [[ 0.1010,  0.2577, -0.4819,  0.3279, -0.0436],\n",
      "         [ 0.1162,  0.0823, -0.3007,  0.1233, -0.0702]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0039 cost = 0.646534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1806,  0.0643, -0.0448,  0.3064,  0.0214],\n",
      "         [ 0.1395, -0.1553, -0.3610,  0.0746,  0.0927]],\n",
      "\n",
      "        [[ 0.1171,  0.0830, -0.3015,  0.1236, -0.0692],\n",
      "         [-0.1756,  0.1172,  0.1098,  0.3858, -0.3520]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0040 cost = 0.703874\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0964,  0.0576, -0.0187,  0.1778,  0.0564],\n",
      "         [-0.0390,  0.0090, -0.0604,  0.0544,  0.1225]],\n",
      "\n",
      "        [[ 0.1368,  0.1799, -0.4231,  0.2112,  0.0158],\n",
      "         [ 0.1054,  0.2619, -0.4865,  0.3296, -0.0394]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0040 cost = 0.518987\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2100, -0.4382, -0.5699,  0.0269,  0.0960],\n",
      "         [ 0.0275,  0.0069, -0.1192,  0.0535,  0.1321]],\n",
      "\n",
      "        [[-0.2099,  0.0533,  0.1871,  0.3728, -0.4863],\n",
      "         [ 0.0053,  0.1087, -0.1704,  0.1777, -0.2121]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0040 cost = 0.557957\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0271,  0.0069, -0.1189,  0.0536,  0.1323],\n",
      "         [-0.0410,  0.0094, -0.0598,  0.0533,  0.1229]],\n",
      "\n",
      "        [[ 0.0069,  0.1101, -0.1725,  0.1784, -0.2097],\n",
      "         [ 0.1108,  0.2671, -0.4921,  0.3318, -0.0339]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0041 cost = 0.556225\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1012,  0.0580, -0.0182,  0.1762,  0.0571],\n",
      "         [ 0.1431, -0.1635, -0.3661,  0.0767,  0.0927]],\n",
      "\n",
      "        [[ 0.1428,  0.1865, -0.4301,  0.2144,  0.0238],\n",
      "         [-0.1792,  0.1175,  0.1193,  0.3861, -0.3559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0041 cost = 0.572594\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2131, -0.4478, -0.5741,  0.0282,  0.0957],\n",
      "         [ 0.1850,  0.0638, -0.0448,  0.3098,  0.0217]],\n",
      "\n",
      "        [[-0.2157,  0.0516,  0.1963,  0.3726, -0.4926],\n",
      "         [ 0.1220,  0.0873, -0.3060,  0.1256, -0.0633]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0041 cost = 0.641929\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2145, -0.4516, -0.5759,  0.0290,  0.0956],\n",
      "         [ 0.1445, -0.1675, -0.3687,  0.0771,  0.0928]],\n",
      "\n",
      "        [[-0.2179,  0.0506,  0.1997,  0.3723, -0.4951],\n",
      "         [-0.1809,  0.1180,  0.1239,  0.3865, -0.3576]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0042 cost = 0.559326\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1870,  0.0634, -0.0449,  0.3104,  0.0216],\n",
      "         [ 0.0247,  0.0073, -0.1176,  0.0526,  0.1335]],\n",
      "\n",
      "        [[ 0.1234,  0.0885, -0.3071,  0.1261, -0.0619],\n",
      "         [ 0.0137,  0.1164, -0.1820,  0.1814, -0.1997]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0042 cost = 0.696706\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1063,  0.0580, -0.0178,  0.1734,  0.0575],\n",
      "         [-0.0455,  0.0098, -0.0585,  0.0499,  0.1241]],\n",
      "\n",
      "        [[ 0.1498,  0.1941, -0.4381,  0.2181,  0.0326],\n",
      "         [ 0.1248,  0.2804, -0.5063,  0.3377, -0.0199]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0042 cost = 0.506297\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0248,  0.0070, -0.1172,  0.0528,  0.1339],\n",
      "         [ 0.2195, -0.4643, -0.5817,  0.0318,  0.0950]],\n",
      "\n",
      "        [[ 0.0165,  0.1189, -0.1858,  0.1826, -0.1957],\n",
      "         [-0.2252,  0.0464,  0.2110,  0.3711, -0.5033]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0043 cost = 0.552583\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1490, -0.1772, -0.3745,  0.0789,  0.0927],\n",
      "         [-0.0474,  0.0102, -0.0579,  0.0483,  0.1246]],\n",
      "\n",
      "        [[-0.1858,  0.1172,  0.1348,  0.3862, -0.3630],\n",
      "         [ 0.1308,  0.2861, -0.5123,  0.3403, -0.0138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0043 cost = 0.550923\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1114,  0.0583, -0.0174,  0.1701,  0.0583],\n",
      "         [ 0.1906,  0.0628, -0.0450,  0.3111,  0.0218]],\n",
      "\n",
      "        [[ 0.1561,  0.2010, -0.4453,  0.2215,  0.0408],\n",
      "         [ 0.1272,  0.0917, -0.3105,  0.1277, -0.0575]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0043 cost = 0.650443\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0232,  0.0073, -0.1162,  0.0518,  0.1348],\n",
      "         [ 0.1919,  0.0626, -0.0450,  0.3115,  0.0219]],\n",
      "\n",
      "        [[ 0.0221,  0.1240, -0.1934,  0.1851, -0.1877],\n",
      "         [ 0.1281,  0.0926, -0.3112,  0.1281, -0.0564]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0044 cost = 0.694255\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0501,  0.0108, -0.0570,  0.0461,  0.1253],\n",
      "         [-0.1129,  0.0584, -0.0171,  0.1689,  0.0584]],\n",
      "\n",
      "        [[ 0.1398,  0.2947, -0.5209,  0.3442, -0.0052],\n",
      "         [ 0.1597,  0.2050, -0.4492,  0.2236,  0.0452]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0044 cost = 0.498780\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1525, -0.1847, -0.3792,  0.0802,  0.0928],\n",
      "         [ 0.2254, -0.4800, -0.5890,  0.0350,  0.0944]],\n",
      "\n",
      "        [[-0.1892,  0.1179,  0.1437,  0.3868, -0.3666],\n",
      "         [-0.2360,  0.0418,  0.2271,  0.3699, -0.5145]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0044 cost = 0.560648\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1158,  0.0588, -0.0168,  0.1679,  0.0589],\n",
      "         [ 0.1967,  0.0622, -0.0451,  0.3139,  0.0219]],\n",
      "\n",
      "        [[ 0.1641,  0.2099, -0.4539,  0.2260,  0.0506],\n",
      "         [ 0.1310,  0.0952, -0.3135,  0.1294, -0.0532]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0045 cost = 0.648108\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2275, -0.4861, -0.5918,  0.0369,  0.0940],\n",
      "         [ 0.1546, -0.1887, -0.3817,  0.0817,  0.0927]],\n",
      "\n",
      "        [[-0.2406,  0.0394,  0.2336,  0.3690, -0.5193],\n",
      "         [-0.1913,  0.1176,  0.1484,  0.3868, -0.3691]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0045 cost = 0.538046\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0535,  0.0115, -0.0558,  0.0444,  0.1262],\n",
      "         [ 0.0239,  0.0075, -0.1150,  0.0530,  0.1359]],\n",
      "\n",
      "        [[ 0.1514,  0.3056, -0.5314,  0.3491,  0.0053],\n",
      "         [ 0.0303,  0.1314, -0.2039,  0.1887, -0.1767]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0045 cost = 0.539478\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2035,  0.0615, -0.0454,  0.3176,  0.0217],\n",
      "         [-0.0545,  0.0118, -0.0555,  0.0439,  0.1264]],\n",
      "\n",
      "        [[ 0.1331,  0.0970, -0.3146,  0.1303, -0.0515],\n",
      "         [ 0.1545,  0.3085, -0.5342,  0.3504,  0.0082]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0046 cost = 0.628743\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1585, -0.1952, -0.3857,  0.0845,  0.0926],\n",
      "         [-0.1171,  0.0589, -0.0163,  0.1681,  0.0591]],\n",
      "\n",
      "        [[-0.1951,  0.1167,  0.1565,  0.3866, -0.3737],\n",
      "         [ 0.1713,  0.2178, -0.4612,  0.2300,  0.0588]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0046 cost = 0.548950\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0249,  0.0078, -0.1142,  0.0546,  0.1365],\n",
      "         [ 0.2326, -0.4990, -0.5976,  0.0408,  0.0933]],\n",
      "\n",
      "        [[ 0.0356,  0.1363, -0.2105,  0.1910, -0.1696],\n",
      "         [-0.2500,  0.0341,  0.2468,  0.3673, -0.5293]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0046 cost = 0.531925\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1607, -0.1995, -0.3884,  0.0860,  0.0925],\n",
      "         [-0.1195,  0.0590, -0.0160,  0.1676,  0.0594]],\n",
      "\n",
      "        [[-0.1975,  0.1164,  0.1617,  0.3866, -0.3764],\n",
      "         [ 0.1755,  0.2225, -0.4656,  0.2323,  0.0638]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0047 cost = 0.545249\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2101,  0.0608, -0.0456,  0.3223,  0.0216],\n",
      "         [ 0.0239,  0.0080, -0.1136,  0.0548,  0.1371]],\n",
      "\n",
      "        [[ 0.1371,  0.1004, -0.3175,  0.1320, -0.0472],\n",
      "         [ 0.0396,  0.1400, -0.2157,  0.1928, -0.1642]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0047 cost = 0.690388\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2360, -0.5079, -0.6019,  0.0435,  0.0929],\n",
      "         [-0.0603,  0.0130, -0.0538,  0.0416,  0.1276]],\n",
      "\n",
      "        [[-0.2572,  0.0303,  0.2565,  0.3659, -0.5366],\n",
      "         [ 0.1703,  0.3234, -0.5481,  0.3572,  0.0224]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0047 cost = 0.452284\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2134,  0.0604, -0.0457,  0.3244,  0.0215],\n",
      "         [ 0.1637, -0.2063, -0.3925,  0.0880,  0.0925]],\n",
      "\n",
      "        [[ 0.1392,  0.1024, -0.3192,  0.1329, -0.0449],\n",
      "         [-0.2012,  0.1160,  0.1695,  0.3865, -0.3804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0048 cost = 0.689690\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0247,  0.0078, -0.1129,  0.0563,  0.1376],\n",
      "         [ 0.2389, -0.5147, -0.6051,  0.0455,  0.0925]],\n",
      "\n",
      "        [[ 0.0451,  0.1450, -0.2228,  0.1953, -0.1570],\n",
      "         [-0.2621,  0.0274,  0.2633,  0.3649, -0.5415]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0048 cost = 0.521837\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1237,  0.0588, -0.0155,  0.1674,  0.0593],\n",
      "         [-0.0614,  0.0125, -0.0532,  0.0408,  0.1280]],\n",
      "\n",
      "        [[ 0.1851,  0.2335, -0.4758,  0.2379,  0.0750],\n",
      "         [ 0.1789,  0.3315, -0.5556,  0.3610,  0.0297]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0048 cost = 0.471773\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1253,  0.0590, -0.0153,  0.1665,  0.0596],\n",
      "         [ 0.0245,  0.0079, -0.1123,  0.0568,  0.1380]],\n",
      "\n",
      "        [[ 0.1876,  0.2363, -0.4783,  0.2394,  0.0778],\n",
      "         [ 0.0491,  0.1487, -0.2280,  0.1971, -0.1518]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0049 cost = 0.535736\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2201,  0.0595, -0.0459,  0.3285,  0.0214],\n",
      "         [ 0.1683, -0.2152, -0.3979,  0.0908,  0.0924]],\n",
      "\n",
      "        [[ 0.1432,  0.1059, -0.3223,  0.1348, -0.0407],\n",
      "         [-0.2065,  0.1151,  0.1801,  0.3863, -0.3860]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0049 cost = 0.686722\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2437, -0.5262, -0.6108,  0.0488,  0.0920],\n",
      "         [-0.0655,  0.0139, -0.0521,  0.0375,  0.1291]],\n",
      "\n",
      "        [[-0.2720,  0.0222,  0.2763,  0.3631, -0.5510],\n",
      "         [ 0.1900,  0.3419, -0.5649,  0.3660,  0.0395]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0049 cost = 0.436758\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1303,  0.0595, -0.0148,  0.1630,  0.0608],\n",
      "         [ 0.2451, -0.5294, -0.6123,  0.0497,  0.0918]],\n",
      "\n",
      "        [[ 0.1950,  0.2449, -0.4862,  0.2439,  0.0868],\n",
      "         [-0.2746,  0.0209,  0.2796,  0.3626, -0.5535]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0050 cost = 0.464598\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0674,  0.0144, -0.0515,  0.0348,  0.1299],\n",
      "         [ 0.1716, -0.2216, -0.4020,  0.0924,  0.0925]],\n",
      "\n",
      "        [[ 0.1972,  0.3487, -0.5709,  0.3693,  0.0457],\n",
      "         [-0.2102,  0.1147,  0.1877,  0.3863, -0.3901]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0050 cost = 0.510300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2252,  0.0590, -0.0460,  0.3304,  0.0221],\n",
      "         [ 0.0221,  0.0091, -0.1107,  0.0550,  0.1400]],\n",
      "\n",
      "        [[ 0.1482,  0.1104, -0.3266,  0.1370, -0.0351],\n",
      "         [ 0.0612,  0.1597, -0.2437,  0.2028, -0.1358]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0050 cost = 0.685159\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0222,  0.0093, -0.1104,  0.0552,  0.1403],\n",
      "         [ 0.1741, -0.2258, -0.4049,  0.0938,  0.0926]],\n",
      "\n",
      "        [[ 0.0636,  0.1618, -0.2466,  0.2039, -0.1328],\n",
      "         [-0.2129,  0.1141,  0.1933,  0.3862, -0.3932]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0051 cost = 0.567555\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2509, -0.5412, -0.6188,  0.0540,  0.0912],\n",
      "         [ 0.2288,  0.0587, -0.0460,  0.3323,  0.0221]],\n",
      "\n",
      "        [[-0.2854,  0.0148,  0.2934,  0.3604, -0.5639],\n",
      "         [ 0.1504,  0.1124, -0.3283,  0.1381, -0.0328]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0051 cost = 0.610453\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1360,  0.0599, -0.0141,  0.1577,  0.0621],\n",
      "         [-0.0705,  0.0154, -0.0504,  0.0312,  0.1311]],\n",
      "\n",
      "        [[ 0.2064,  0.2575, -0.4977,  0.2505,  0.0995],\n",
      "         [ 0.2109,  0.3608, -0.5819,  0.3752,  0.0568]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0051 cost = 0.450777\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1783, -0.2324, -0.4093,  0.0967,  0.0925],\n",
      "         [-0.1373,  0.0601, -0.0140,  0.1565,  0.0624]],\n",
      "\n",
      "        [[-0.2177,  0.1125,  0.2023,  0.3858, -0.3986],\n",
      "         [ 0.2087,  0.2601, -0.5000,  0.2519,  0.1021]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0052 cost = 0.515030\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2557, -0.5505, -0.6238,  0.0576,  0.0907],\n",
      "         [ 0.2345,  0.0580, -0.0462,  0.3350,  0.0219]],\n",
      "\n",
      "        [[-0.2932,  0.0101,  0.3032,  0.3588, -0.5714],\n",
      "         [ 0.1531,  0.1146, -0.3298,  0.1393, -0.0305]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0052 cost = 0.606586\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0223,  0.0096, -0.1091,  0.0556,  0.1415],\n",
      "         [-0.0734,  0.0158, -0.0497,  0.0283,  0.1319]],\n",
      "\n",
      "        [[ 0.0742,  0.1714, -0.2595,  0.2088, -0.1198],\n",
      "         [ 0.2209,  0.3699, -0.5897,  0.3797,  0.0648]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0052 cost = 0.490088\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2384,  0.0575, -0.0464,  0.3367,  0.0215],\n",
      "         [-0.1418,  0.0602, -0.0136,  0.1530,  0.0628]],\n",
      "\n",
      "        [[ 0.1547,  0.1160, -0.3304,  0.1400, -0.0293],\n",
      "         [ 0.2153,  0.2677, -0.5064,  0.2559,  0.1093]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0053 cost = 0.627620\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1835, -0.2420, -0.4154,  0.0999,  0.0923],\n",
      "         [ 0.2603, -0.5600, -0.6287,  0.0612,  0.0902]],\n",
      "\n",
      "        [[-0.2236,  0.1108,  0.2134,  0.3856, -0.4056],\n",
      "         [-0.3008,  0.0058,  0.3125,  0.3574, -0.5787]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0053 cost = 0.504966\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0765,  0.0163, -0.0490,  0.0259,  0.1326],\n",
      "         [ 0.0222,  0.0097, -0.1085,  0.0560,  0.1421]],\n",
      "\n",
      "        [[ 0.2311,  0.3793, -0.5973,  0.3845,  0.0728],\n",
      "         [ 0.0808,  0.1775, -0.2673,  0.2119, -0.1118]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0053 cost = 0.494181\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2630, -0.5660, -0.6318,  0.0639,  0.0898],\n",
      "         [-0.1450,  0.0604, -0.0132,  0.1517,  0.0628]],\n",
      "\n",
      "        [[-0.3063,  0.0024,  0.3190,  0.3562, -0.5839],\n",
      "         [ 0.2218,  0.2752, -0.5125,  0.2600,  0.1164]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0054 cost = 0.424312\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0794,  0.0172, -0.0483,  0.0236,  0.1334],\n",
      "         [ 0.0213,  0.0102, -0.1078,  0.0557,  0.1429]],\n",
      "\n",
      "        [[ 0.2390,  0.3866, -0.6031,  0.3882,  0.0793],\n",
      "         [ 0.0864,  0.1826, -0.2743,  0.2146, -0.1046]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0054 cost = 0.489309\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1877, -0.2507, -0.4207,  0.1028,  0.0924],\n",
      "         [ 0.2464,  0.0567, -0.0466,  0.3411,  0.0215]],\n",
      "\n",
      "        [[-0.2284,  0.1103,  0.2225,  0.3857, -0.4112],\n",
      "         [ 0.1610,  0.1220, -0.3351,  0.1432, -0.0223]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0054 cost = 0.679570\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2484,  0.0564, -0.0467,  0.3429,  0.0213],\n",
      "         [ 0.0198,  0.0111, -0.1070,  0.0553,  0.1437]],\n",
      "\n",
      "        [[ 0.1620,  0.1229, -0.3357,  0.1437, -0.0213],\n",
      "         [ 0.0927,  0.1883, -0.2822,  0.2177, -0.0964]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0055 cost = 0.676182\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1904, -0.2548, -0.4233,  0.1055,  0.0922],\n",
      "         [ 0.2679, -0.5771, -0.6373,  0.0680,  0.0893]],\n",
      "\n",
      "        [[-0.2311,  0.1094,  0.2274,  0.3856, -0.4143],\n",
      "         [-0.3164, -0.0027,  0.3306,  0.3544, -0.5932]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0055 cost = 0.492296\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0838,  0.0190, -0.0470,  0.0209,  0.1345],\n",
      "         [-0.1506,  0.0610, -0.0125,  0.1494,  0.0636]],\n",
      "\n",
      "        [[ 0.2535,  0.4000, -0.6134,  0.3951,  0.0911],\n",
      "         [ 0.2338,  0.2895, -0.5238,  0.2678,  0.1299]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0055 cost = 0.427871\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1517,  0.0612, -0.0123,  0.1497,  0.0636],\n",
      "         [ 0.1934, -0.2595, -0.4262,  0.1087,  0.0920]],\n",
      "\n",
      "        [[ 0.2362,  0.2923, -0.5260,  0.2693,  0.1326],\n",
      "         [-0.2343,  0.1079,  0.2329,  0.3851, -0.4182]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0056 cost = 0.488912\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0868,  0.0198, -0.0463,  0.0198,  0.1351],\n",
      "         [ 0.2592,  0.0554, -0.0471,  0.3524,  0.0196]],\n",
      "\n",
      "        [[ 0.2609,  0.4067, -0.6186,  0.3986,  0.0970],\n",
      "         [ 0.1653,  0.1259, -0.3370,  0.1454, -0.0184]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0056 cost = 0.604754\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0219,  0.0119, -0.1056,  0.0601,  0.1445],\n",
      "         [ 0.2726, -0.5876, -0.6427,  0.0735,  0.0885]],\n",
      "\n",
      "        [[ 0.1050,  0.1994, -0.2966,  0.2236, -0.0814],\n",
      "         [-0.3267, -0.0094,  0.3422,  0.3519, -0.6029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0056 cost = 0.466290\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2738, -0.5900, -0.6441,  0.0748,  0.0884],\n",
      "         [-0.0891,  0.0209, -0.0456,  0.0191,  0.1357]],\n",
      "\n",
      "        [[-0.3293, -0.0110,  0.3451,  0.3513, -0.6052],\n",
      "         [ 0.2680,  0.4129, -0.6234,  0.4019,  0.1024]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0057 cost = 0.377038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.1991, -0.2679, -0.4318,  0.1147,  0.0918],\n",
      "         [ 0.2680,  0.0548, -0.0474,  0.3602,  0.0188]],\n",
      "\n",
      "        [[-0.2404,  0.1056,  0.2435,  0.3847, -0.4257],\n",
      "         [ 0.1684,  0.1285, -0.3384,  0.1469, -0.0157]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0057 cost = 0.671534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1548,  0.0617, -0.0116,  0.1516,  0.0637],\n",
      "         [ 0.0233,  0.0124, -0.1048,  0.0630,  0.1450]],\n",
      "\n",
      "        [[ 0.2474,  0.3050, -0.5358,  0.2765,  0.1445],\n",
      "         [ 0.1125,  0.2061, -0.3050,  0.2272, -0.0726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0057 cost = 0.485866\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0916,  0.0220, -0.0447,  0.0172,  0.1365],\n",
      "         [ 0.2024, -0.2726, -0.4348,  0.1181,  0.0915]],\n",
      "\n",
      "        [[ 0.2787,  0.4224, -0.6304,  0.4071,  0.1105],\n",
      "         [-0.2439,  0.1038,  0.2492,  0.3842, -0.4299]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0058 cost = 0.456735\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0231,  0.0130, -0.1042,  0.0641,  0.1456],\n",
      "         [-0.1573,  0.0621, -0.0113,  0.1510,  0.0640]],\n",
      "\n",
      "        [[ 0.1181,  0.2112, -0.3114,  0.2300, -0.0658],\n",
      "         [ 0.2522,  0.3106, -0.5400,  0.2797,  0.1498]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0058 cost = 0.475038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2762,  0.0541, -0.0477,  0.3689,  0.0177],\n",
      "         [ 0.2804, -0.6029, -0.6512,  0.0806,  0.0876]],\n",
      "\n",
      "        [[ 0.1726,  0.1322, -0.3403,  0.1491, -0.0118],\n",
      "         [-0.3422, -0.0191,  0.3594,  0.3484, -0.6169]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0058 cost = 0.594463\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0965,  0.0241, -0.0435,  0.0136,  0.1378],\n",
      "         [ 0.2817, -0.6052, -0.6525,  0.0818,  0.0874]],\n",
      "\n",
      "        [[ 0.2906,  0.4329, -0.6383,  0.4129,  0.1198],\n",
      "         [-0.3448, -0.0208,  0.3622,  0.3478, -0.6191]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0059 cost = 0.383292\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1624,  0.0628, -0.0108,  0.1492,  0.0649],\n",
      "         [ 0.0218,  0.0143, -0.1031,  0.0647,  0.1467]],\n",
      "\n",
      "        [[ 0.2601,  0.3197, -0.5471,  0.2850,  0.1585],\n",
      "         [ 0.1275,  0.2196, -0.3223,  0.2347, -0.0544]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0059 cost = 0.474305\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2821,  0.0537, -0.0479,  0.3748,  0.0173],\n",
      "         [ 0.2084, -0.2815, -0.4415,  0.1240,  0.0916]],\n",
      "\n",
      "        [[ 0.1766,  0.1358, -0.3430,  0.1512, -0.0075],\n",
      "         [-0.2508,  0.1017,  0.2613,  0.3841, -0.4382]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0059 cost = 0.659368\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2856, -0.6116, -0.6566,  0.0858,  0.0870],\n",
      "         [-0.1653,  0.0634, -0.0104,  0.1477,  0.0656]],\n",
      "\n",
      "        [[-0.3529, -0.0259,  0.3708,  0.3457, -0.6259],\n",
      "         [ 0.2654,  0.3258, -0.5519,  0.2886,  0.1643]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0060 cost = 0.379986\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2115, -0.2854, -0.4443,  0.1271,  0.0915],\n",
      "         [-0.1017,  0.0268, -0.0422,  0.0090,  0.1396]],\n",
      "\n",
      "        [[-0.2541,  0.1001,  0.2666,  0.3837, -0.4423],\n",
      "         [ 0.3060,  0.4461, -0.6482,  0.4204,  0.1316]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0060 cost = 0.433854\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0217,  0.0159, -0.1017,  0.0656,  0.1482],\n",
      "         [ 0.2907,  0.0532, -0.0481,  0.3813,  0.0169]],\n",
      "\n",
      "        [[ 0.1396,  0.2305, -0.3362,  0.2409, -0.0399],\n",
      "         [ 0.1796,  0.1386, -0.3445,  0.1529, -0.0050]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0060 cost = 0.662362\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2945,  0.0529, -0.0482,  0.3838,  0.0165],\n",
      "         [ 0.2901, -0.6192, -0.6611,  0.0899,  0.0865]],\n",
      "\n",
      "        [[ 0.1803,  0.1391, -0.3443,  0.1533, -0.0047],\n",
      "         [-0.3608, -0.0314,  0.3791,  0.3436, -0.6328]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0061 cost = 0.586711\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1033,  0.0276, -0.0413,  0.0067,  0.1405],\n",
      "         [ 0.0242,  0.0160, -0.1011,  0.0681,  0.1484]],\n",
      "\n",
      "        [[ 0.3157,  0.4547, -0.6543,  0.4254,  0.1387],\n",
      "         [ 0.1442,  0.2347, -0.3411,  0.2433, -0.0349]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0061 cost = 0.441687\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1647,  0.0640, -0.0097,  0.1478,  0.0661],\n",
      "         [ 0.2187, -0.2937, -0.4499,  0.1343,  0.0911]],\n",
      "\n",
      "        [[ 0.2760,  0.3379, -0.5606,  0.2961,  0.1752],\n",
      "         [-0.2612,  0.0961,  0.2775,  0.3825, -0.4508]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0061 cost = 0.450716\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2944, -0.6262, -0.6653,  0.0943,  0.0860],\n",
      "         [ 0.2204, -0.2956, -0.4513,  0.1361,  0.0911]],\n",
      "\n",
      "        [[-0.3684, -0.0369,  0.3870,  0.3415, -0.6388],\n",
      "         [-0.2630,  0.0952,  0.2803,  0.3822, -0.4530]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0062 cost = 0.417297\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3130,  0.0517, -0.0488,  0.3969,  0.0144],\n",
      "         [-0.1050,  0.0289, -0.0404,  0.0044,  0.1415]],\n",
      "\n",
      "        [[ 0.1819,  0.1399, -0.3425,  0.1543, -0.0054],\n",
      "         [ 0.3256,  0.4634, -0.6605,  0.4305,  0.1461]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0062 cost = 0.571920\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1625,  0.0642, -0.0093,  0.1490,  0.0661],\n",
      "         [ 0.0294,  0.0169, -0.0998,  0.0731,  0.1491]],\n",
      "\n",
      "        [[ 0.2816,  0.3442, -0.5651,  0.3000,  0.1809],\n",
      "         [ 0.1543,  0.2437, -0.3518,  0.2485, -0.0237]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0062 cost = 0.455450\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0302,  0.0174, -0.0994,  0.0737,  0.1495],\n",
      "         [ 0.2994, -0.6338, -0.6697,  0.0988,  0.0854]],\n",
      "\n",
      "        [[ 0.1573,  0.2465, -0.3551,  0.2501, -0.0201],\n",
      "         [-0.3761, -0.0430,  0.3950,  0.3392, -0.6455]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0063 cost = 0.422818\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3258,  0.0510, -0.0491,  0.4054,  0.0134],\n",
      "         [-0.1069,  0.0303, -0.0394,  0.0013,  0.1428]],\n",
      "\n",
      "        [[ 0.1839,  0.1415, -0.3421,  0.1556, -0.0045],\n",
      "         [ 0.3359,  0.4725, -0.6667,  0.4360,  0.1540]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0063 cost = 0.567171\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1624,  0.0647, -0.0087,  0.1477,  0.0669],\n",
      "         [ 0.2300, -0.3053, -0.4581,  0.1454,  0.0908]],\n",
      "\n",
      "        [[ 0.2887,  0.3525, -0.5709,  0.3053,  0.1885],\n",
      "         [-0.2726,  0.0896,  0.2945,  0.3806, -0.4646]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0063 cost = 0.436931\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3346,  0.0505, -0.0493,  0.4112,  0.0125],\n",
      "         [-0.1086,  0.0314, -0.0386, -0.0012,  0.1436]],\n",
      "\n",
      "        [[ 0.1853,  0.1426, -0.3418,  0.1565, -0.0039],\n",
      "         [ 0.3431,  0.4789, -0.6710,  0.4400,  0.1594]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0064 cost = 0.563817\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1613,  0.0650, -0.0083,  0.1475,  0.0670],\n",
      "         [ 0.2338, -0.3087, -0.4607,  0.1494,  0.0906]],\n",
      "\n",
      "        [[ 0.2930,  0.3574, -0.5742,  0.3084,  0.1929],\n",
      "         [-0.2763,  0.0874,  0.3000,  0.3802, -0.4691]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0064 cost = 0.432457\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0351,  0.0191, -0.0974,  0.0773,  0.1510],\n",
      "         [ 0.3060, -0.6436, -0.6758,  0.1052,  0.0847]],\n",
      "\n",
      "        [[ 0.1721,  0.2600, -0.3708,  0.2582, -0.0032],\n",
      "         [-0.3877, -0.0513,  0.4068,  0.3362, -0.6552]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0064 cost = 0.411806\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 3.5438e-02,  1.9475e-02, -9.7007e-02,  7.7534e-02,  1.5136e-01],\n",
      "         [ 3.0731e-01, -6.4547e-01, -6.7713e-01,  1.0645e-01,  8.4538e-02]],\n",
      "\n",
      "        [[ 1.7524e-01,  2.6288e-01, -3.7418e-01,  2.5987e-01,  3.4625e-04],\n",
      "         [-3.9016e-01, -5.3156e-02,  4.0922e-01,  3.3555e-01, -6.5718e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0065 cost = 0.409448\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2389, -0.3140, -0.4647,  0.1545,  0.0905],\n",
      "         [-0.1128,  0.0334, -0.0371, -0.0072,  0.1453]],\n",
      "\n",
      "        [[-0.2819,  0.0842,  0.3083,  0.3793, -0.4760],\n",
      "         [ 0.3574,  0.4913, -0.6796,  0.4478,  0.1701]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0065 cost = 0.394550\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3533,  0.0492, -0.0498,  0.4240,  0.0106],\n",
      "         [-0.1645,  0.0656, -0.0076,  0.1431,  0.0681]],\n",
      "\n",
      "        [[ 0.1893,  0.1456, -0.3416,  0.1590, -0.0018],\n",
      "         [ 0.3028,  0.3686, -0.5823,  0.3157,  0.2034]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0065 cost = 0.585501\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1651,  0.0657, -0.0074,  0.1426,  0.0682],\n",
      "         [ 0.2420, -0.3180, -0.4676,  0.1577,  0.0904]],\n",
      "\n",
      "        [[ 0.3052,  0.3713, -0.5842,  0.3175,  0.2059],\n",
      "         [-0.2858,  0.0820,  0.3138,  0.3787, -0.4805]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0066 cost = 0.420209\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3599,  0.0486, -0.0501,  0.4289,  0.0098],\n",
      "         [ 0.0347,  0.0206, -0.0956,  0.0771,  0.1529]],\n",
      "\n",
      "        [[ 0.1910,  0.1470, -0.3416,  0.1600, -0.0010],\n",
      "         [ 0.1885,  0.2752, -0.3887,  0.2673,  0.0158]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0066 cost = 0.637107\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1180,  0.0348, -0.0358, -0.0134,  0.1469],\n",
      "         [ 0.3140, -0.6551, -0.6837,  0.1129,  0.0838]],\n",
      "\n",
      "        [[ 0.3719,  0.5040, -0.6882,  0.4560,  0.1812],\n",
      "         [-0.4026, -0.0624,  0.4213,  0.3319, -0.6672]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0066 cost = 0.327651\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1670,  0.0660, -0.0068,  0.1402,  0.0685],\n",
      "         [ 0.0360,  0.0211, -0.0948,  0.0783,  0.1533]],\n",
      "\n",
      "        [[ 0.3122,  0.3792, -0.5898,  0.3227,  0.2132],\n",
      "         [ 0.1946,  0.2808, -0.3952,  0.2708,  0.0227]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0067 cost = 0.426945\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 3.7088e-01,  4.7892e-02, -5.0385e-02,  4.3669e-01,  8.5296e-03],\n",
      "         [-1.2112e-01,  3.6192e-02, -3.4934e-02, -1.7391e-02,  1.4800e-01]],\n",
      "\n",
      "        [[ 1.9358e-01,  1.4915e-01, -3.4168e-01,  1.6182e-01,  6.3513e-04],\n",
      "         [ 3.7965e-01,  5.1070e-01, -6.9279e-01,  4.6040e-01,  1.8739e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0067 cost = 0.547647\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3176, -0.6594, -0.6871,  0.1169,  0.0835],\n",
      "         [ 0.2504, -0.3261, -0.4741,  0.1665,  0.0901]],\n",
      "\n",
      "        [[-0.4092, -0.0669,  0.4276,  0.3302, -0.6724],\n",
      "         [-0.2947,  0.0769,  0.3265,  0.3775, -0.4913]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0067 cost = 0.374206\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2525, -0.3279, -0.4754,  0.1688,  0.0900],\n",
      "         [ 0.0371,  0.0226, -0.0935,  0.0786,  0.1546]],\n",
      "\n",
      "        [[-0.2968,  0.0756,  0.3293,  0.3771, -0.4937],\n",
      "         [ 0.2053,  0.2907, -0.4066,  0.2770,  0.0354]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0068 cost = 0.456576\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1691,  0.0667, -0.0060,  0.1359,  0.0696],\n",
      "         [ 0.3839,  0.0470, -0.0508,  0.4458,  0.0068]],\n",
      "\n",
      "        [[ 0.3223,  0.3909, -0.5978,  0.3306,  0.2244],\n",
      "         [ 0.1956,  0.1507, -0.3408,  0.1633,  0.0014]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0068 cost = 0.583614\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1247,  0.0379, -0.0334, -0.0232,  0.1497],\n",
      "         [ 0.3223, -0.6653, -0.6910,  0.1215,  0.0830]],\n",
      "\n",
      "        [[ 0.3935,  0.5228, -0.7007,  0.4686,  0.1983],\n",
      "         [-0.4159, -0.0725,  0.4340,  0.3281, -0.6779]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0068 cost = 0.313681\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 3.9658e-02,  2.3074e-02, -9.2336e-02,  8.0639e-02,  1.5546e-01],\n",
      "         [ 3.9389e-01,  4.6267e-02, -5.1149e-02,  4.5275e-01,  5.5371e-03]],\n",
      "\n",
      "        [[ 2.1440e-01,  2.9916e-01, -4.1578e-01,  2.8237e-01,  4.5872e-02],\n",
      "         [ 1.9607e-01,  1.5061e-01, -3.3870e-01,  1.6382e-01,  3.4221e-04]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0069 cost = 0.628236\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2611, -0.3356, -0.4810,  0.1781,  0.0896],\n",
      "         [ 0.3257, -0.6688, -0.6936,  0.1251,  0.0826]],\n",
      "\n",
      "        [[-0.3051,  0.0696,  0.3403,  0.3754, -0.5038],\n",
      "         [-0.4203, -0.0762,  0.4382,  0.3267, -0.6813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0069 cost = 0.393575\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1258,  0.0385, -0.0325, -0.0258,  0.1507],\n",
      "         [-0.1652,  0.0669, -0.0053,  0.1370,  0.0692]],\n",
      "\n",
      "        [[ 0.4023,  0.5301, -0.7053,  0.4738,  0.2049],\n",
      "         [ 0.3295,  0.3984, -0.6024,  0.3363,  0.2315]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0069 cost = 0.334556\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1650,  0.0670, -0.0051,  0.1372,  0.0693],\n",
      "         [ 0.0439,  0.0233, -0.0913,  0.0846,  0.1560]],\n",
      "\n",
      "        [[ 0.3316,  0.4008, -0.6039,  0.3381,  0.2339],\n",
      "         [ 0.2224,  0.3065, -0.4234,  0.2873,  0.0547]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0070 cost = 0.408479\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1286,  0.0399, -0.0317, -0.0291,  0.1519],\n",
      "         [ 0.2668, -0.3412, -0.4849,  0.1847,  0.0894]],\n",
      "\n",
      "        [[ 0.4096,  0.5366, -0.7093,  0.4783,  0.2111],\n",
      "         [-0.3109,  0.0654,  0.3478,  0.3740, -0.5108]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0070 cost = 0.362423\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 3.3091e-01, -6.7569e-01, -6.9829e-01,  1.3134e-01,  8.2012e-02],\n",
      "         [ 4.1485e-01,  4.4806e-02, -5.2043e-02,  4.6815e-01,  2.9104e-03]],\n",
      "\n",
      "        [[-4.2959e-01, -8.3481e-02,  4.4656e-01,  3.2367e-01, -6.8852e-01],\n",
      "         [ 1.9853e-01,  1.5216e-01, -3.3536e-01,  1.6600e-01,  2.3601e-04]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0070 cost = 0.534593\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1316,  0.0415, -0.0309, -0.0326,  0.1530],\n",
      "         [ 0.0439,  0.0247, -0.0902,  0.0847,  0.1574]],\n",
      "\n",
      "        [[ 0.4166,  0.5426, -0.7131,  0.4825,  0.2170],\n",
      "         [ 0.2331,  0.3167, -0.4342,  0.2938,  0.0678]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0071 cost = 0.374031\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2720, -0.3461, -0.4888,  0.1909,  0.0892],\n",
      "         [ 0.3337, -0.6791, -0.7007,  0.1344,  0.0817]],\n",
      "\n",
      "        [[-0.3163,  0.0620,  0.3548,  0.3732, -0.5172],\n",
      "         [-0.4339, -0.0868,  0.4505,  0.3225, -0.6918]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0071 cost = 0.379656\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 4.2571e-01,  4.4036e-02, -5.2599e-02,  4.7732e-01,  9.4780e-04],\n",
      "         [-1.6953e-01,  6.8384e-02, -4.1570e-03,  1.3242e-01,  7.1130e-02]],\n",
      "\n",
      "        [[ 2.0007e-01,  1.5321e-01, -3.3345e-01,  1.6730e-01,  5.4334e-04],\n",
      "         [ 3.4402e-01,  4.1524e-01, -6.1317e-01,  3.4820e-01,  2.4840e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0071 cost = 0.557098\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 4.3077e-01,  4.3637e-02, -5.2866e-02,  4.8155e-01, -1.4733e-04],\n",
      "         [ 3.3652e-01, -6.8297e-01, -7.0316e-01,  1.3794e-01,  8.1317e-02]],\n",
      "\n",
      "        [[ 2.0005e-01,  1.5296e-01, -3.3203e-01,  1.6745e-01, -2.8236e-04],\n",
      "         [-4.3873e-01, -9.0776e-02,  4.5485e-01,  3.2078e-01, -6.9551e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0072 cost = 0.534675\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1658,  0.0683, -0.0039,  0.1366,  0.0700],\n",
      "         [-0.1351,  0.0435, -0.0296, -0.0357,  0.1544]],\n",
      "\n",
      "        [[ 0.3469,  0.4183, -0.6147,  0.3506,  0.2514],\n",
      "         [ 0.4286,  0.5530, -0.7193,  0.4899,  0.2271]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0072 cost = 0.311721\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0516,  0.0257, -0.0888,  0.0934,  0.1581],\n",
      "         [ 0.2811, -0.3533, -0.4943,  0.2031,  0.0884]],\n",
      "\n",
      "        [[ 0.2469,  0.3298, -0.4472,  0.3024,  0.0840],\n",
      "         [-0.3244,  0.0551,  0.3647,  0.3707, -0.5273]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0072 cost = 0.416756\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0526,  0.0259, -0.0885,  0.0946,  0.1583],\n",
      "         [ 0.4483,  0.0423, -0.0538,  0.4955, -0.0040]],\n",
      "\n",
      "        [[ 0.2499,  0.3326, -0.4500,  0.3042,  0.0875],\n",
      "         [ 0.1988,  0.1508, -0.3260,  0.1673, -0.0048]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0073 cost = 0.606543\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3421, -0.6902, -0.7078,  0.1450,  0.0805],\n",
      "         [ 0.2851, -0.3567, -0.4968,  0.2084,  0.0881]],\n",
      "\n",
      "        [[-0.4477, -0.0986,  0.4628,  0.3175, -0.7019],\n",
      "         [-0.3284,  0.0517,  0.3696,  0.3696, -0.5322]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0073 cost = 0.330123\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1371,  0.0451, -0.0286, -0.0387,  0.1557],\n",
      "         [-0.1612,  0.0686, -0.0033,  0.1403,  0.0690]],\n",
      "\n",
      "        [[ 0.4394,  0.5626, -0.7248,  0.4968,  0.2363],\n",
      "         [ 0.3534,  0.4255, -0.6186,  0.3562,  0.2587]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0073 cost = 0.310694\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0570,  0.0263, -0.0876,  0.0991,  0.1589],\n",
      "         [-0.1380,  0.0458, -0.0283, -0.0397,  0.1562]],\n",
      "\n",
      "        [[ 0.2578,  0.3405, -0.4574,  0.3095,  0.0968],\n",
      "         [ 0.4424,  0.5653, -0.7263,  0.4988,  0.2390]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0074 cost = 0.340742\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2910, -0.3620, -0.5006,  0.2155,  0.0878],\n",
      "         [ 0.4665,  0.0410, -0.0548,  0.5095, -0.0076]],\n",
      "\n",
      "        [[-0.3346,  0.0469,  0.3769,  0.3681, -0.5395],\n",
      "         [ 0.1985,  0.1497, -0.3196,  0.1680, -0.0084]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0074 cost = 0.589895\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1616,  0.0693, -0.0028,  0.1406,  0.0691],\n",
      "         [ 0.3474, -0.6973, -0.7122,  0.1506,  0.0799]],\n",
      "\n",
      "        [[ 0.3597,  0.4328, -0.6227,  0.3617,  0.2663],\n",
      "         [-0.4557, -0.1058,  0.4699,  0.3147, -0.7082]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0074 cost = 0.304777\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.2948, -0.3660, -0.5034,  0.2204,  0.0875],\n",
      "         [-0.1421,  0.0479, -0.0272, -0.0439,  0.1576]],\n",
      "\n",
      "        [[-0.3387,  0.0434,  0.3819,  0.3670, -0.5447],\n",
      "         [ 0.4516,  0.5737, -0.7311,  0.5050,  0.2475]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0075 cost = 0.318067\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1635,  0.0696, -0.0025,  0.1393,  0.0691],\n",
      "         [ 0.3503, -0.7009, -0.7147,  0.1537,  0.0796]],\n",
      "\n",
      "        [[ 0.3636,  0.4372, -0.6252,  0.3651,  0.2710],\n",
      "         [-0.4598, -0.1096,  0.4734,  0.3132, -0.7113]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0075 cost = 0.300978\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0567,  0.0273, -0.0864,  0.1004,  0.1606],\n",
      "         [ 0.4787,  0.0395, -0.0558,  0.5208, -0.0113]],\n",
      "\n",
      "        [[ 0.2731,  0.3556, -0.4717,  0.3197,  0.1156],\n",
      "         [ 0.1984,  0.1489, -0.3137,  0.1688, -0.0115]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0075 cost = 0.590239\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3533, -0.7044, -0.7172,  0.1569,  0.0793],\n",
      "         [ 0.4819,  0.0392, -0.0561,  0.5234, -0.0122]],\n",
      "\n",
      "        [[-0.4639, -0.1134,  0.4770,  0.3115, -0.7144],\n",
      "         [ 0.1982,  0.1484, -0.3117,  0.1690, -0.0128]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0076 cost = 0.503068\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0578,  0.0271, -0.0860,  0.1019,  0.1609],\n",
      "         [-0.1659,  0.0698, -0.0020,  0.1365,  0.0689]],\n",
      "\n",
      "        [[ 0.2776,  0.3600, -0.4754,  0.3229,  0.1207],\n",
      "         [ 0.3686,  0.4426, -0.6281,  0.3696,  0.2767]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0076 cost = 0.362222\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3038, -0.3766, -0.5107,  0.2310,  0.0868],\n",
      "         [-0.1490,  0.0496, -0.0261, -0.0523,  0.1596]],\n",
      "\n",
      "        [[-0.3488,  0.0347,  0.3936,  0.3640, -0.5569],\n",
      "         [ 0.4652,  0.5855, -0.7379,  0.5143,  0.2597]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0076 cost = 0.305443\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4919,  0.0379, -0.0571,  0.5321, -0.0158],\n",
      "         [ 0.0560,  0.0273, -0.0857,  0.1009,  0.1615]],\n",
      "\n",
      "        [[ 0.1961,  0.1454, -0.3039,  0.1686, -0.0186],\n",
      "         [ 0.2835,  0.3660, -0.4808,  0.3271,  0.1280]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0077 cost = 0.573290\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3593, -0.7117, -0.7222,  0.1633,  0.0786],\n",
      "         [ 0.3071, -0.3809, -0.5135,  0.2351,  0.0864]],\n",
      "\n",
      "        [[-0.4713, -0.1201,  0.4833,  0.3089, -0.7195],\n",
      "         [-0.3526,  0.0314,  0.3980,  0.3629, -0.5614]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0077 cost = 0.299920\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1694,  0.0701, -0.0014,  0.1337,  0.0679],\n",
      "         [-0.1527,  0.0503, -0.0254, -0.0567,  0.1606]],\n",
      "\n",
      "        [[ 0.3753,  0.4500, -0.6319,  0.3758,  0.2845],\n",
      "         [ 0.4729,  0.5925, -0.7418,  0.5198,  0.2669]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0077 cost = 0.283788\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1704,  0.0703, -0.0013,  0.1320,  0.0681],\n",
      "         [ 0.3625, -0.7155, -0.7247,  0.1665,  0.0783]],\n",
      "\n",
      "        [[ 0.3773,  0.4523, -0.6331,  0.3776,  0.2870],\n",
      "         [-0.4749, -0.1236,  0.4864,  0.3075, -0.7223]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0078 cost = 0.287110\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0554,  0.0274, -0.0848,  0.1001,  0.1628],\n",
      "         [-0.1556,  0.0514, -0.0248, -0.0615,  0.1618]],\n",
      "\n",
      "        [[ 0.2950,  0.3778, -0.4914,  0.3355,  0.1423],\n",
      "         [ 0.4788,  0.5978, -0.7448,  0.5240,  0.2726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0078 cost = 0.313209\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5064,  0.0362, -0.0584,  0.5429, -0.0199],\n",
      "         [ 0.3136, -0.3888, -0.5187,  0.2416,  0.0862]],\n",
      "\n",
      "        [[ 0.1956,  0.1444, -0.2955,  0.1700, -0.0226],\n",
      "         [-0.3599,  0.0256,  0.4063,  0.3609, -0.5701]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0078 cost = 0.542829\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1591,  0.0530, -0.0242, -0.0670,  0.1631],\n",
      "         [ 0.3665, -0.7202, -0.7280,  0.1704,  0.0779]],\n",
      "\n",
      "        [[ 0.4851,  0.6035, -0.7480,  0.5285,  0.2788],\n",
      "         [-0.4804, -0.1283,  0.4910,  0.3055, -0.7263]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0079 cost = 0.249037\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0528,  0.0282, -0.0840,  0.0966,  0.1644],\n",
      "         [ 0.3172, -0.3925, -0.5213,  0.2457,  0.0860]],\n",
      "\n",
      "        [[ 0.3049,  0.3879, -0.5008,  0.3426,  0.1551],\n",
      "         [-0.3638,  0.0223,  0.4106,  0.3597, -0.5747]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0079 cost = 0.366826\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-1.7872e-01,  7.1460e-02, -3.5681e-04,  1.1897e-01,  7.0131e-02],\n",
      "         [ 5.1542e-01,  3.5192e-02, -5.9095e-02,  5.4975e-01, -2.2494e-02]],\n",
      "\n",
      "        [[ 3.8828e-01,  4.6484e-01, -6.4062e-01,  3.8760e-01,  3.0067e-01],\n",
      "         [ 1.9528e-01,  1.4348e-01, -2.9058e-01,  1.7056e-01, -2.5245e-02]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0079 cost = 0.521218\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-1.7912e-01,  7.1634e-02, -1.6922e-04,  1.1735e-01,  7.0324e-02],\n",
      "         [ 5.1938e-01,  3.4809e-02, -5.9415e-02,  5.5257e-01, -2.3434e-02]],\n",
      "\n",
      "        [[ 3.8995e-01,  4.6653e-01, -6.4153e-01,  3.8915e-01,  3.0261e-01],\n",
      "         [ 1.9456e-01,  1.4248e-01, -2.8789e-01,  1.7047e-01, -2.7208e-02]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0080 cost = 0.518888\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3724, -0.7260, -0.7326,  0.1769,  0.0774],\n",
      "         [ 0.0531,  0.0286, -0.0833,  0.0956,  0.1657]],\n",
      "\n",
      "        [[-0.4874, -0.1352,  0.4969,  0.3025, -0.7313],\n",
      "         [ 0.3130,  0.3960, -0.5080,  0.3486,  0.1650]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0080 cost = 0.291716\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1648,  0.0557, -0.0227, -0.0782,  0.1661],\n",
      "         [ 0.3249, -0.3995, -0.5264,  0.2544,  0.0857]],\n",
      "\n",
      "        [[ 0.4982,  0.6146, -0.7544,  0.5380,  0.2911],\n",
      "         [-0.3717,  0.0151,  0.4193,  0.3571, -0.5840]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0080 cost = 0.286514\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3270, -0.4012, -0.5278,  0.2568,  0.0856],\n",
      "         [ 0.0533,  0.0290, -0.0828,  0.0946,  0.1667]],\n",
      "\n",
      "        [[-0.3737,  0.0132,  0.4214,  0.3564, -0.5863],\n",
      "         [ 0.3186,  0.4016, -0.5128,  0.3529,  0.1717]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0081 cost = 0.357913\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1675,  0.0569, -0.0220, -0.0831,  0.1673],\n",
      "         [ 0.3766, -0.7306, -0.7360,  0.1812,  0.0770]],\n",
      "\n",
      "        [[ 0.5035,  0.6191, -0.7570,  0.5419,  0.2962],\n",
      "         [-0.4923, -0.1399,  0.5010,  0.3007, -0.7347]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0081 cost = 0.235952\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5385,  0.0330, -0.0610,  0.5663, -0.0276],\n",
      "         [-0.1818,  0.0727,  0.0008,  0.1079,  0.0719]],\n",
      "\n",
      "        [[ 0.1911,  0.1378, -0.2745,  0.1703, -0.0366],\n",
      "         [ 0.3986,  0.4753, -0.6462,  0.3973,  0.3126]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0081 cost = 0.493634\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3326, -0.4070, -0.5321,  0.2635,  0.0853],\n",
      "         [ 0.0515,  0.0294, -0.0820,  0.0925,  0.1681]],\n",
      "\n",
      "        [[-0.3796,  0.0080,  0.4278,  0.3545, -0.5931],\n",
      "         [ 0.3279,  0.4109, -0.5215,  0.3599,  0.1835]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0082 cost = 0.349848\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1718,  0.0586, -0.0210, -0.0901,  0.1691],\n",
      "         [ 0.3809, -0.7349, -0.7395,  0.1860,  0.0766]],\n",
      "\n",
      "        [[ 0.5117,  0.6259, -0.7611,  0.5479,  0.3041],\n",
      "         [-0.4973, -0.1447,  0.5051,  0.2986, -0.7382]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0082 cost = 0.230300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5485,  0.0319, -0.0619,  0.5748, -0.0308],\n",
      "         [-0.1851,  0.0731,  0.0014,  0.1033,  0.0723]],\n",
      "\n",
      "        [[ 0.1900,  0.1360, -0.2680,  0.1707, -0.0405],\n",
      "         [ 0.4043,  0.4813, -0.6497,  0.4027,  0.3196]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0082 cost = 0.486758\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3840, -0.7376, -0.7419,  0.1896,  0.0763],\n",
      "         [ 0.5529,  0.0314, -0.0623,  0.5785, -0.0324]],\n",
      "\n",
      "        [[-0.5006, -0.1480,  0.5078,  0.2971, -0.7404],\n",
      "         [ 0.1892,  0.1349, -0.2651,  0.1706, -0.0426]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0083 cost = 0.458133\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-1.8404e-01,  7.3159e-02,  1.8211e-03,  1.0399e-01,  7.1220e-02],\n",
      "         [ 3.4090e-01, -4.1469e-01, -5.3795e-01,  2.7461e-01,  8.4574e-02]],\n",
      "\n",
      "        [[ 4.0684e-01,  4.8336e-01, -6.5065e-01,  4.0513e-01,  3.2240e-01],\n",
      "         [-3.8739e-01,  6.1026e-04,  4.3598e-01,  3.5145e-01, -6.0207e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0083 cost = 0.302232\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0532,  0.0291, -0.0810,  0.0954,  0.1690],\n",
      "         [-0.1759,  0.0598, -0.0198, -0.0968,  0.1708]],\n",
      "\n",
      "        [[ 0.3408,  0.4237, -0.5327,  0.3700,  0.1992],\n",
      "         [ 0.5207,  0.6331, -0.7654,  0.5546,  0.3128]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0083 cost = 0.280499\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0525,  0.0292, -0.0807,  0.0949,  0.1693],\n",
      "         [-0.1851,  0.0733,  0.0022,  0.1018,  0.0707]],\n",
      "\n",
      "        [[ 0.3437,  0.4266, -0.5352,  0.3722,  0.2029],\n",
      "         [ 0.4097,  0.4859, -0.6520,  0.4077,  0.3257]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0084 cost = 0.317851\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3463, -0.4198, -0.5421,  0.2816,  0.0841],\n",
      "         [-0.1801,  0.0615, -0.0191, -0.1027,  0.1720]],\n",
      "\n",
      "        [[-0.3928, -0.0044,  0.4417,  0.3495, -0.6083],\n",
      "         [ 0.5264,  0.6379, -0.7682,  0.5589,  0.3187]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0084 cost = 0.250395\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3909, -0.7444, -0.7476,  0.1974,  0.0756],\n",
      "         [ 0.5693,  0.0293, -0.0641,  0.5929, -0.0398]],\n",
      "\n",
      "        [[-0.5078, -0.1551,  0.5138,  0.2943, -0.7454],\n",
      "         [ 0.1848,  0.1293, -0.2502,  0.1700, -0.0530]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0084 cost = 0.446048\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3923, -0.7459, -0.7487,  0.1989,  0.0755],\n",
      "         [ 0.3492, -0.4237, -0.5449,  0.2853,  0.0839]],\n",
      "\n",
      "        [[-0.5092, -0.1565,  0.5149,  0.2938, -0.7463],\n",
      "         [-0.3962, -0.0074,  0.4453,  0.3484, -0.6123]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0085 cost = 0.247393\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0463,  0.0298, -0.0800,  0.0897,  0.1710],\n",
      "         [-0.1855,  0.0631, -0.0182, -0.1097,  0.1735]],\n",
      "\n",
      "        [[ 0.3561,  0.4396, -0.5466,  0.3819,  0.2194],\n",
      "         [ 0.5338,  0.6443, -0.7718,  0.5645,  0.3266]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0085 cost = 0.269540\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1961,  0.0744,  0.0032,  0.0894,  0.0710],\n",
      "         [ 0.5772,  0.0280, -0.0652,  0.6003, -0.0442]],\n",
      "\n",
      "        [[ 0.4194,  0.4965, -0.6578,  0.4168,  0.3382],\n",
      "         [ 0.1815,  0.1254, -0.2405,  0.1692, -0.0600]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0085 cost = 0.478231\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3969, -0.7506, -0.7522,  0.2036,  0.0751],\n",
      "         [ 0.5806,  0.0276, -0.0656,  0.6030, -0.0456]],\n",
      "\n",
      "        [[-0.5134, -0.1610,  0.5184,  0.2920, -0.7494],\n",
      "         [ 0.1799,  0.1236, -0.2363,  0.1688, -0.0631]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0086 cost = 0.435575\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3571, -0.4317, -0.5505,  0.2951,  0.0831],\n",
      "         [-0.1894,  0.0644, -0.0175, -0.1157,  0.1749]],\n",
      "\n",
      "        [[-0.4040, -0.0155,  0.4531,  0.3451, -0.6213],\n",
      "         [ 0.5401,  0.6498, -0.7746,  0.5695,  0.3333]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0086 cost = 0.237120\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1959,  0.0745,  0.0036,  0.0882,  0.0695],\n",
      "         [ 0.0458,  0.0294, -0.0795,  0.0897,  0.1719]],\n",
      "\n",
      "        [[ 0.4220,  0.4984, -0.6578,  0.4194,  0.3412],\n",
      "         [ 0.3647,  0.4487, -0.5530,  0.3890,  0.2303]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0086 cost = 0.312904\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1921,  0.0652, -0.0170, -0.1199,  0.1758],\n",
      "         [ 0.3610, -0.4359, -0.5534,  0.3003,  0.0827]],\n",
      "\n",
      "        [[ 0.5441,  0.6532, -0.7763,  0.5726,  0.3375],\n",
      "         [-0.4077, -0.0195,  0.4569,  0.3434, -0.6257]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0087 cost = 0.244050\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5949,  0.0255, -0.0677,  0.6152, -0.0527],\n",
      "         [ 0.0440,  0.0296, -0.0792,  0.0879,  0.1727]],\n",
      "\n",
      "        [[ 0.1715,  0.1139, -0.2167,  0.1657, -0.0788],\n",
      "         [ 0.3701,  0.4544, -0.5575,  0.3934,  0.2374]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0087 cost = 0.488138\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4039, -0.7583, -0.7577,  0.2110,  0.0743],\n",
      "         [-0.1991,  0.0751,  0.0042,  0.0825,  0.0693]],\n",
      "\n",
      "        [[-0.5193, -0.1679,  0.5234,  0.2897, -0.7536],\n",
      "         [ 0.4265,  0.5029, -0.6597,  0.4237,  0.3470]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0087 cost = 0.214071\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6022,  0.0246, -0.0686,  0.6212, -0.0561],\n",
      "         [ 0.0434,  0.0298, -0.0788,  0.0869,  0.1735]],\n",
      "\n",
      "        [[ 0.1683,  0.1103, -0.2084,  0.1649, -0.0849],\n",
      "         [ 0.3756,  0.4603, -0.5622,  0.3981,  0.2447]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0088 cost = 0.481507\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1974,  0.0678, -0.0156, -0.1303,  0.1781],\n",
      "         [-0.1992,  0.0754,  0.0047,  0.0790,  0.0692]],\n",
      "\n",
      "        [[ 0.5531,  0.6611, -0.7806,  0.5800,  0.3475],\n",
      "         [ 0.4295,  0.5061, -0.6611,  0.4268,  0.3509]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0088 cost = 0.235279\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3703, -0.4442, -0.5598,  0.3124,  0.0819],\n",
      "         [ 0.4070, -0.7621, -0.7604,  0.2144,  0.0740]],\n",
      "\n",
      "        [[-0.4160, -0.0277,  0.4654,  0.3403, -0.6350],\n",
      "         [-0.5225, -0.1710,  0.5260,  0.2888, -0.7557]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0088 cost = 0.263619\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4080, -0.7635, -0.7613,  0.2157,  0.0738],\n",
      "         [-0.1998,  0.0694, -0.0148, -0.1350,  0.1795]],\n",
      "\n",
      "        [[-0.5238, -0.1724,  0.5270,  0.2882, -0.7566],\n",
      "         [ 0.5580,  0.6656, -0.7829,  0.5840,  0.3532]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0089 cost = 0.164863\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3741, -0.4477, -0.5622,  0.3177,  0.0816],\n",
      "         [-0.1991,  0.0761,  0.0053,  0.0759,  0.0693]],\n",
      "\n",
      "        [[-0.4192, -0.0308,  0.4685,  0.3389, -0.6387],\n",
      "         [ 0.4349,  0.5122, -0.6641,  0.4323,  0.3583]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0089 cost = 0.259361\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6201,  0.0224, -0.0708,  0.6357, -0.0641],\n",
      "         [ 0.0434,  0.0302, -0.0778,  0.0862,  0.1754]],\n",
      "\n",
      "        [[ 0.1631,  0.1047, -0.1924,  0.1646, -0.0954],\n",
      "         [ 0.3905,  0.4767, -0.5756,  0.4109,  0.2651]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0089 cost = 0.466056\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1996,  0.0764,  0.0057,  0.0732,  0.0695],\n",
      "         [ 0.6239,  0.0218, -0.0713,  0.6389, -0.0660]],\n",
      "\n",
      "        [[ 0.4387,  0.5166, -0.6664,  0.4362,  0.3635],\n",
      "         [ 0.1619,  0.1034, -0.1890,  0.1644, -0.0977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0090 cost = 0.442096\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2033,  0.0712, -0.0136, -0.1437,  0.1820],\n",
      "         [ 0.3799, -0.4536, -0.5661,  0.3259,  0.0810]],\n",
      "\n",
      "        [[ 0.5672,  0.6739, -0.7872,  0.5918,  0.3638],\n",
      "         [-0.4241, -0.0356,  0.4733,  0.3368, -0.6443]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0090 cost = 0.223801\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4135, -0.7705, -0.7659,  0.2224,  0.0731],\n",
      "         [ 0.0462,  0.0296, -0.0773,  0.0885,  0.1761]],\n",
      "\n",
      "        [[-0.5298, -0.1788,  0.5318,  0.2853, -0.7608],\n",
      "         [ 0.3981,  0.4851, -0.5820,  0.4179,  0.2753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0090 cost = 0.231824\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3842, -0.4572, -0.5688,  0.3318,  0.0806],\n",
      "         [ 0.6368,  0.0202, -0.0731,  0.6493, -0.0716]],\n",
      "\n",
      "        [[-0.4274, -0.0391,  0.4765,  0.3353, -0.6481],\n",
      "         [ 0.1564,  0.0974, -0.1756,  0.1630, -0.1079]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0091 cost = 0.448139\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1962,  0.0767,  0.0065,  0.0718,  0.0685],\n",
      "         [-0.2055,  0.0721, -0.0127, -0.1498,  0.1837]],\n",
      "\n",
      "        [[ 0.4438,  0.5216, -0.6682,  0.4416,  0.3699],\n",
      "         [ 0.5730,  0.6789, -0.7898,  0.5968,  0.3704]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0091 cost = 0.219591\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0477,  0.0290, -0.0771,  0.0898,  0.1767],\n",
      "         [ 0.4172, -0.7747, -0.7688,  0.2266,  0.0726]],\n",
      "\n",
      "        [[ 0.4052,  0.4930, -0.5879,  0.4244,  0.2848],\n",
      "         [-0.5330, -0.1827,  0.5344,  0.2837, -0.7630]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0091 cost = 0.250314\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1974,  0.0772,  0.0068,  0.0676,  0.0688],\n",
      "         [-0.2077,  0.0735, -0.0122, -0.1549,  0.1851]],\n",
      "\n",
      "        [[ 0.4468,  0.5247, -0.6696,  0.4447,  0.3740],\n",
      "         [ 0.5772,  0.6825, -0.7918,  0.6003,  0.3753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0092 cost = 0.216885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4191, -0.7769, -0.7705,  0.2286,  0.0724],\n",
      "         [ 0.6489,  0.0182, -0.0753,  0.6595, -0.0786]],\n",
      "\n",
      "        [[-0.5352, -0.1848,  0.5361,  0.2827, -0.7645],\n",
      "         [ 0.1495,  0.0900, -0.1587,  0.1613, -0.1202]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0092 cost = 0.382363\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0433,  0.0296, -0.0769,  0.0839,  0.1786],\n",
      "         [ 0.3931, -0.4662, -0.5754,  0.3436,  0.0797]],\n",
      "\n",
      "        [[ 0.4136,  0.5021, -0.5951,  0.4319,  0.2963],\n",
      "         [-0.4352, -0.0475,  0.4841,  0.3316, -0.6570]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0092 cost = 0.274963\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2115,  0.0766, -0.0114, -0.1636,  0.1875],\n",
      "         [ 0.0414,  0.0297, -0.0769,  0.0815,  0.1792]],\n",
      "\n",
      "        [[ 0.5835,  0.6880, -0.7947,  0.6057,  0.3830],\n",
      "         [ 0.4162,  0.5050, -0.5975,  0.4344,  0.2999]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0093 cost = 0.239921\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2044,  0.0788,  0.0076,  0.0534,  0.0707],\n",
      "         [ 0.4224, -0.7803, -0.7732,  0.2321,  0.0721]],\n",
      "\n",
      "        [[ 0.4537,  0.5319, -0.6733,  0.4516,  0.3834],\n",
      "         [-0.5384, -0.1880,  0.5387,  0.2813, -0.7665]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0093 cost = 0.217589\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.3967, -0.4705, -0.5788,  0.3477,  0.0796],\n",
      "         [ 0.6560,  0.0170, -0.0770,  0.6653, -0.0838]],\n",
      "\n",
      "        [[-0.4393, -0.0513,  0.4881,  0.3300, -0.6614],\n",
      "         [ 0.1447,  0.0851, -0.1452,  0.1608, -0.1288]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0093 cost = 0.425353\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2166,  0.0802, -0.0106, -0.1730,  0.1900],\n",
      "         [ 0.6581,  0.0165, -0.0775,  0.6673, -0.0857]],\n",
      "\n",
      "        [[ 0.5904,  0.6939, -0.7981,  0.6115,  0.3914],\n",
      "         [ 0.1426,  0.0829, -0.1406,  0.1602, -0.1323]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0094 cost = 0.368644\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4261, -0.7834, -0.7759,  0.2362,  0.0718],\n",
      "         [ 0.4004, -0.4739, -0.5815,  0.3527,  0.0791]],\n",
      "\n",
      "        [[-0.5415, -0.1915,  0.5413,  0.2797, -0.7685],\n",
      "         [-0.4424, -0.0551,  0.4912,  0.3282, -0.6649]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0094 cost = 0.193386\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2083,  0.0799,  0.0082,  0.0441,  0.0712],\n",
      "         [ 0.0349,  0.0306, -0.0765,  0.0737,  0.1820]],\n",
      "\n",
      "        [[ 0.4584,  0.5362, -0.6750,  0.4566,  0.3898],\n",
      "         [ 0.4289,  0.5187, -0.6085,  0.4461,  0.3174]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0094 cost = 0.269429\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2093,  0.0802,  0.0084,  0.0412,  0.0717],\n",
      "         [ 0.4292, -0.7858, -0.7778,  0.2394,  0.0715]],\n",
      "\n",
      "        [[ 0.4599,  0.5376, -0.6756,  0.4581,  0.3917],\n",
      "         [-0.5436, -0.1943,  0.5430,  0.2784, -0.7699]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0095 cost = 0.211392\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6694,  0.0146, -0.0800,  0.6770, -0.0928],\n",
      "         [-0.2205,  0.0831, -0.0096, -0.1829,  0.1930]],\n",
      "\n",
      "        [[ 0.1330,  0.0728, -0.1203,  0.1570, -0.1481],\n",
      "         [ 0.5972,  0.6994, -0.8011,  0.6175,  0.3996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0095 cost = 0.340636\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4084, -0.4797, -0.5864,  0.3626,  0.0786],\n",
      "         [ 0.0330,  0.0312, -0.0763,  0.0690,  0.1841]],\n",
      "\n",
      "        [[-0.4486, -0.0622,  0.4969,  0.3247, -0.6718],\n",
      "         [ 0.4364,  0.5266, -0.6149,  0.4531,  0.3276]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0095 cost = 0.253568\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2229,  0.0845, -0.0091, -0.1887,  0.1947],\n",
      "         [ 0.4102, -0.4814, -0.5877,  0.3648,  0.0785]],\n",
      "\n",
      "        [[ 0.6010,  0.7025, -0.8028,  0.6208,  0.4041],\n",
      "         [-0.4501, -0.0638,  0.4983,  0.3239, -0.6734]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0096 cost = 0.192750\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2137,  0.0813,  0.0092,  0.0282,  0.0737],\n",
      "         [ 0.0310,  0.0314, -0.0761,  0.0661,  0.1853]],\n",
      "\n",
      "        [[ 0.4657,  0.5433, -0.6786,  0.4643,  0.3997],\n",
      "         [ 0.4416,  0.5322, -0.6195,  0.4580,  0.3348]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0096 cost = 0.260420\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4354, -0.7906, -0.7820,  0.2459,  0.0711],\n",
      "         [ 0.6790,  0.0129, -0.0822,  0.6850, -0.0990]],\n",
      "\n",
      "        [[-0.5486, -0.2001,  0.5470,  0.2753, -0.7733],\n",
      "         [ 0.1271,  0.0668, -0.1058,  0.1559, -0.1581]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0096 cost = 0.349625\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2276,  0.0872, -0.0081, -0.1984,  0.1975],\n",
      "         [-0.2173,  0.0820,  0.0096,  0.0202,  0.0752]],\n",
      "\n",
      "        [[ 0.6072,  0.7076, -0.8058,  0.6262,  0.4118],\n",
      "         [ 0.4693,  0.5469, -0.6807,  0.4680,  0.4046]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0097 cost = 0.197724\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4377, -0.7924, -0.7836,  0.2483,  0.0709],\n",
      "         [ 0.6836,  0.0121, -0.0833,  0.6891, -0.1023]],\n",
      "\n",
      "        [[-0.5503, -0.2021,  0.5484,  0.2744, -0.7745],\n",
      "         [ 0.1238,  0.0635, -0.0979,  0.1553, -0.1635]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0097 cost = 0.344880\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0271,  0.0324, -0.0757,  0.0606,  0.1879],\n",
      "         [ 0.4190, -0.4882, -0.5936,  0.3763,  0.0779]],\n",
      "\n",
      "        [[ 0.4523,  0.5438, -0.6289,  0.4680,  0.3498],\n",
      "         [-0.4570, -0.0713,  0.5048,  0.3202, -0.6811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0097 cost = 0.243810\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2198,  0.0831,  0.0102,  0.0148,  0.0757],\n",
      "         [-0.2316,  0.0901, -0.0072, -0.2055,  0.1999]],\n",
      "\n",
      "        [[ 0.4733,  0.5508, -0.6824,  0.4721,  0.4102],\n",
      "         [ 0.6126,  0.7121, -0.8083,  0.6309,  0.4187]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0098 cost = 0.192922\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0242,  0.0331, -0.0756,  0.0572,  0.1893],\n",
      "         [ 0.6905,  0.0109, -0.0851,  0.6956, -0.1082]],\n",
      "\n",
      "        [[ 0.4574,  0.5493, -0.6333,  0.4728,  0.3569],\n",
      "         [ 0.1171,  0.0568, -0.0838,  0.1533, -0.1740]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0098 cost = 0.398258\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4423, -0.7960, -0.7868,  0.2532,  0.0706],\n",
      "         [ 0.4236, -0.4920, -0.5970,  0.3823,  0.0776]],\n",
      "\n",
      "        [[-0.5535, -0.2058,  0.5511,  0.2728, -0.7766],\n",
      "         [-0.4610, -0.0756,  0.5086,  0.3180, -0.6854]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0098 cost = 0.172335\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4255, -0.4933, -0.5982,  0.3846,  0.0775],\n",
      "         [ 0.4436, -0.7970, -0.7876,  0.2546,  0.0705]],\n",
      "\n",
      "        [[-0.4625, -0.0774,  0.5099,  0.3170, -0.6870],\n",
      "         [-0.5543, -0.2070,  0.5518,  0.2723, -0.7772]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0099 cost = 0.209700\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6975,  0.0095, -0.0871,  0.7015, -0.1131],\n",
      "         [ 0.0214,  0.0336, -0.0755,  0.0531,  0.1913]],\n",
      "\n",
      "        [[ 0.1095,  0.0492, -0.0688,  0.1509, -0.1855],\n",
      "         [ 0.4641,  0.5567, -0.6389,  0.4794,  0.3661]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0099 cost = 0.375091\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2365,  0.0948, -0.0062, -0.2163,  0.2039],\n",
      "         [-0.2234,  0.0847,  0.0109,  0.0041,  0.0775]],\n",
      "\n",
      "        [[ 0.6208,  0.7189, -0.8120,  0.6382,  0.4293],\n",
      "         [ 0.4791,  0.5561, -0.6845,  0.4783,  0.4184]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0099 cost = 0.188371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4317, -0.4980, -0.6020,  0.3930,  0.0769],\n",
      "         [ 0.4477, -0.8001, -0.7902,  0.2593,  0.0701]],\n",
      "\n",
      "        [[-0.4671, -0.0832,  0.5140,  0.3136, -0.6922],\n",
      "         [-0.5572, -0.2110,  0.5541,  0.2701, -0.7792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0100 cost = 0.204342\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0220,  0.0335, -0.0756,  0.0533,  0.1927],\n",
      "         [ 0.7058,  0.0078, -0.0894,  0.7088, -0.1193]],\n",
      "\n",
      "        [[ 0.4707,  0.5641, -0.6445,  0.4860,  0.3752],\n",
      "         [ 0.1019,  0.0416, -0.0541,  0.1484, -0.1970]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0100 cost = 0.379487\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2382,  0.0966, -0.0057, -0.2197,  0.2059],\n",
      "         [-0.2218,  0.0853,  0.0113,  0.0044,  0.0774]],\n",
      "\n",
      "        [[ 0.6254,  0.7227, -0.8141,  0.6425,  0.4354],\n",
      "         [ 0.4824,  0.5592, -0.6856,  0.4820,  0.4231]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0100 cost = 0.185320\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2215,  0.0856,  0.0114,  0.0048,  0.0776],\n",
      "         [ 0.4511, -0.8032, -0.7924,  0.2638,  0.0696]],\n",
      "\n",
      "        [[ 0.4837,  0.5605, -0.6862,  0.4835,  0.4251],\n",
      "         [-0.5601, -0.2150,  0.5565,  0.2676, -0.7812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0101 cost = 0.189951\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0217,  0.0335, -0.0756,  0.0531,  0.1945],\n",
      "         [-0.2401,  0.0986, -0.0052, -0.2225,  0.2076]],\n",
      "\n",
      "        [[ 0.4778,  0.5719, -0.6504,  0.4932,  0.3850],\n",
      "         [ 0.6290,  0.7257, -0.8157,  0.6458,  0.4401]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0101 cost = 0.189631\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4405, -0.5053, -0.6078,  0.4051,  0.0763],\n",
      "         [ 0.7146,  0.0060, -0.0922,  0.7165, -0.1255]],\n",
      "\n",
      "        [[-0.4739, -0.0911,  0.5200,  0.3089, -0.6997],\n",
      "         [ 0.0946,  0.0345, -0.0385,  0.1467, -0.2081]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0101 cost = 0.351737\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2250,  0.0868,  0.0119, -0.0012,  0.0797],\n",
      "         [-0.2428,  0.1009, -0.0047, -0.2264,  0.2094]],\n",
      "\n",
      "        [[ 0.4886,  0.5655, -0.6889,  0.4887,  0.4319],\n",
      "         [ 0.6327,  0.7288, -0.8174,  0.6491,  0.4451]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0102 cost = 0.180023\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0160,  0.0345, -0.0756,  0.0468,  0.1972],\n",
      "         [ 0.4434, -0.5082, -0.6100,  0.4089,  0.0761]],\n",
      "\n",
      "        [[ 0.4856,  0.5804, -0.6571,  0.5010,  0.3961],\n",
      "         [-0.4764, -0.0941,  0.5222,  0.3072, -0.7025]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0102 cost = 0.217409\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7191,  0.0048, -0.0941,  0.7207, -0.1298],\n",
      "         [ 0.4554, -0.8068, -0.7956,  0.2692,  0.0693]],\n",
      "\n",
      "        [[ 0.0892,  0.0295, -0.0271,  0.1455, -0.2160],\n",
      "         [-0.5643, -0.2201,  0.5599,  0.2642, -0.7841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0102 cost = 0.307294\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2467,  0.1051, -0.0040, -0.2328,  0.2122],\n",
      "         [-0.2297,  0.0883,  0.0124, -0.0096,  0.0819]],\n",
      "\n",
      "        [[ 0.6379,  0.7331, -0.8198,  0.6539,  0.4522],\n",
      "         [ 0.4931,  0.5700, -0.6913,  0.4935,  0.4384]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0103 cost = 0.175903\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4572, -0.8082, -0.7968,  0.2714,  0.0691],\n",
      "         [ 0.4475, -0.5121, -0.6133,  0.4145,  0.0758]],\n",
      "\n",
      "        [[-0.5660, -0.2222,  0.5613,  0.2627, -0.7852],\n",
      "         [-0.4801, -0.0982,  0.5255,  0.3047, -0.7065]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0103 cost = 0.151585\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0108,  0.0358, -0.0757,  0.0408,  0.2004],\n",
      "         [ 0.7250,  0.0035, -0.0962,  0.7260, -0.1354]],\n",
      "\n",
      "        [[ 0.4952,  0.5908, -0.6653,  0.5106,  0.4097],\n",
      "         [ 0.0836,  0.0241, -0.0158,  0.1441, -0.2242]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0103 cost = 0.352075\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0105,  0.0359, -0.0757,  0.0403,  0.2010],\n",
      "         [ 0.7274,  0.0030, -0.0970,  0.7281, -0.1373]],\n",
      "\n",
      "        [[ 0.4974,  0.5931, -0.6670,  0.5128,  0.4126],\n",
      "         [ 0.0809,  0.0215, -0.0108,  0.1432, -0.2281]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0104 cost = 0.349065\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4604, -0.8106, -0.7988,  0.2751,  0.0688],\n",
      "         [-0.2498,  0.1099, -0.0032, -0.2380,  0.2156]],\n",
      "\n",
      "        [[-0.5684, -0.2257,  0.5632,  0.2604, -0.7868],\n",
      "         [ 0.6441,  0.7382, -0.8226,  0.6596,  0.4605]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0104 cost = 0.107146\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2298,  0.0902,  0.0130, -0.0124,  0.0832],\n",
      "         [ 0.4546, -0.5172, -0.6177,  0.4240,  0.0752]],\n",
      "\n",
      "        [[ 0.4988,  0.5751, -0.6932,  0.4997,  0.4464],\n",
      "         [-0.4852, -0.1047,  0.5300,  0.3005, -0.7120]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0104 cost = 0.191661\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7347,  0.0014, -0.0997,  0.7343, -0.1432],\n",
      "         [-0.2303,  0.0906,  0.0132, -0.0142,  0.0837]],\n",
      "\n",
      "        [[ 0.0715,  0.0120,  0.0059,  0.1397, -0.2415],\n",
      "         [ 0.4999,  0.5760, -0.6936,  0.5009,  0.4479]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0105 cost = 0.310206\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4635, -0.8131, -0.8009,  0.2787,  0.0685],\n",
      "         [-0.2520,  0.1127, -0.0026, -0.2433,  0.2180]],\n",
      "\n",
      "        [[-0.5705, -0.2289,  0.5649,  0.2585, -0.7881],\n",
      "         [ 0.6483,  0.7414, -0.8243,  0.6635,  0.4660]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0105 cost = 0.104374\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4601, -0.5208, -0.6210,  0.4314,  0.0747],\n",
      "         [ 0.0088,  0.0363, -0.0759,  0.0371,  0.2040]],\n",
      "\n",
      "        [[-0.4889, -0.1094,  0.5333,  0.2976, -0.7159],\n",
      "         [ 0.5072,  0.6036, -0.6745,  0.5232,  0.4254]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0105 cost = 0.194109\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-2.5359e-01,  1.1438e-01, -2.1300e-03, -2.4741e-01,  2.1961e-01],\n",
      "         [ 7.4175e-01, -3.2175e-04, -1.0233e-01,  7.4042e-01, -1.4943e-01]],\n",
      "\n",
      "        [[ 6.5126e-01,  7.4370e-01, -8.2559e-01,  6.6621e-01,  4.6978e-01],\n",
      "         [ 6.3254e-02,  3.8881e-03,  2.0448e-02,  1.3692e-01, -2.5303e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0106 cost = 0.275138\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0078,  0.0361, -0.0760,  0.0357,  0.2051],\n",
      "         [-0.2302,  0.0916,  0.0138, -0.0194,  0.0846]],\n",
      "\n",
      "        [[ 0.5113,  0.6079, -0.6776,  0.5274,  0.4307],\n",
      "         [ 0.5039,  0.5792, -0.6945,  0.5052,  0.4531]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0106 cost = 0.209759\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4649, -0.5251, -0.6244,  0.4378,  0.0743],\n",
      "         [ 0.4671, -0.8163, -0.8034,  0.2829,  0.0682]],\n",
      "\n",
      "        [[-0.4924, -0.1135,  0.5363,  0.2950, -0.7196],\n",
      "         [-0.5729, -0.2325,  0.5668,  0.2563, -0.7897]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0106 cost = 0.175395\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.0044,  0.0363, -0.0761,  0.0318,  0.2067],\n",
      "         [ 0.4664, -0.5267, -0.6256,  0.4400,  0.0741]],\n",
      "\n",
      "        [[ 0.5160,  0.6130, -0.6816,  0.5323,  0.4372],\n",
      "         [-0.4936, -0.1150,  0.5373,  0.2940, -0.7209]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0107 cost = 0.194130\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2580,  0.1175, -0.0012, -0.2556,  0.2228],\n",
      "         [ 0.4688, -0.8179, -0.8046,  0.2849,  0.0680]],\n",
      "\n",
      "        [[ 0.6574,  0.7485, -0.8283,  0.6719,  0.4778],\n",
      "         [-0.5744, -0.2346,  0.5680,  0.2549, -0.7908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0107 cost = 0.129500\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2354,  0.0928,  0.0144, -0.0292,  0.0868],\n",
      "         [ 0.7504, -0.0029, -0.1064,  0.7482, -0.1576]],\n",
      "\n",
      "        [[ 0.5096,  0.5847, -0.6975,  0.5114,  0.4610],\n",
      "         [ 0.0526, -0.0064,  0.0400,  0.1337, -0.2676]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0107 cost = 0.303305\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4708, -0.5311, -0.6290,  0.4457,  0.0738],\n",
      "         [ 0.7521, -0.0035, -0.1073,  0.7497, -0.1591]],\n",
      "\n",
      "        [[-0.4971, -0.1192,  0.5404,  0.2911, -0.7246],\n",
      "         [ 0.0502, -0.0087,  0.0442,  0.1329, -0.2709]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0108 cost = 0.299045\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-2.6132e-01,  1.1911e-01, -5.5869e-04, -2.6125e-01,  2.2493e-01],\n",
      "         [-2.3647e-01,  9.3116e-02,  1.4709e-02, -3.1944e-02,  8.7159e-02]],\n",
      "\n",
      "        [[ 6.6150e-01,  7.5152e-01, -8.3006e-01,  6.7573e-01,  4.8299e-01],\n",
      "         [ 5.1120e-01,  5.8574e-01, -6.9773e-01,  5.1314e-01,  4.6298e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0108 cost = 0.160010\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4729, -0.8211, -0.8073,  0.2901,  0.0676],\n",
      "         [-0.0016,  0.0358, -0.0765,  0.0255,  0.2100]],\n",
      "\n",
      "        [[-0.5773, -0.2389,  0.5703,  0.2519, -0.7927],\n",
      "         [ 0.5263,  0.6237, -0.6898,  0.5428,  0.4505]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0108 cost = 0.146833\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4762, -0.5360, -0.6329,  0.4533,  0.0733],\n",
      "         [-0.2374,  0.0937,  0.0150, -0.0334,  0.0878]],\n",
      "\n",
      "        [[-0.5008, -0.1239,  0.5435,  0.2878, -0.7285],\n",
      "         [ 0.5135,  0.5877, -0.6986,  0.5155,  0.4660]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0109 cost = 0.163368\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-2.6456e-01,  1.2141e-01,  2.5268e-04, -2.6608e-01,  2.2743e-01],\n",
      "         [-4.2839e-03,  3.5747e-02, -7.6639e-02,  2.2852e-02,  2.1149e-01]],\n",
      "\n",
      "        [[ 6.6588e-01,  7.5488e-01, -8.3200e-01,  6.7978e-01,  4.8881e-01],\n",
      "         [ 5.3073e-01,  6.2834e-01, -6.9345e-01,  5.4742e-01,  4.5652e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0109 cost = 0.160165\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7606, -0.0064, -0.1118,  0.7581, -0.1677],\n",
      "         [ 0.4755, -0.8232, -0.8090,  0.2932,  0.0674]],\n",
      "\n",
      "        [[ 0.0386, -0.0197,  0.0644,  0.1292, -0.2864],\n",
      "         [-0.5791, -0.2413,  0.5718,  0.2501, -0.7940]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0109 cost = 0.259302\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2406,  0.0947,  0.0156, -0.0384,  0.0897],\n",
      "         [ 0.7625, -0.0070, -0.1127,  0.7599, -0.1699]],\n",
      "\n",
      "        [[ 0.5183,  0.5925, -0.7014,  0.5207,  0.4727],\n",
      "         [ 0.0365, -0.0216,  0.0680,  0.1286, -0.2891]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0110 cost = 0.287068\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0066,  0.0359, -0.0767,  0.0209,  0.2135],\n",
      "         [ 0.4773, -0.8247, -0.8102,  0.2955,  0.0673]],\n",
      "\n",
      "        [[ 0.5376,  0.6355, -0.6991,  0.5545,  0.4657],\n",
      "         [-0.5804, -0.2432,  0.5729,  0.2487, -0.7948]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0110 cost = 0.167372\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2690,  0.1248,  0.0015, -0.2731,  0.2309],\n",
      "         [ 0.4837, -0.5429, -0.6383,  0.4636,  0.0726]],\n",
      "\n",
      "        [[ 0.6719,  0.7596, -0.8347,  0.6855,  0.4970],\n",
      "         [-0.5060, -0.1299,  0.5480,  0.2836, -0.7339]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0110 cost = 0.130197\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2422,  0.0955,  0.0161, -0.0423,  0.0907],\n",
      "         [ 0.4792, -0.8261, -0.8114,  0.2978,  0.0671]],\n",
      "\n",
      "        [[ 0.5218,  0.5956, -0.7029,  0.5245,  0.4774],\n",
      "         [-0.5818, -0.2450,  0.5740,  0.2472, -0.7956]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0111 cost = 0.159375\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4868, -0.5452, -0.6404,  0.4675,  0.0725],\n",
      "         [ 0.7694, -0.0092, -0.1163,  0.7663, -0.1772]],\n",
      "\n",
      "        [[-0.5080, -0.1322,  0.5497,  0.2819, -0.7359],\n",
      "         [ 0.0278, -0.0300,  0.0831,  0.1258, -0.3007]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0111 cost = 0.273818\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2724,  0.1268,  0.0023, -0.2790,  0.2333],\n",
      "         [-0.0106,  0.0358, -0.0769,  0.0158,  0.2162]],\n",
      "\n",
      "        [[ 0.6760,  0.7624, -0.8365,  0.6893,  0.5022],\n",
      "         [ 0.5456,  0.6435, -0.7056,  0.5629,  0.4760]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0111 cost = 0.150736\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7728, -0.0105, -0.1183,  0.7696, -0.1814],\n",
      "         [ 0.4903, -0.5478, -0.6428,  0.4723,  0.0721]],\n",
      "\n",
      "        [[ 0.0225, -0.0350,  0.0915,  0.1238, -0.3075],\n",
      "         [-0.5102, -0.1350,  0.5515,  0.2798, -0.7382]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0112 cost = 0.254398\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2449,  0.0965,  0.0168, -0.0484,  0.0920],\n",
      "         [-0.0107,  0.0357, -0.0769,  0.0159,  0.2171]],\n",
      "\n",
      "        [[ 0.5259,  0.5987, -0.7046,  0.5290,  0.4828],\n",
      "         [ 0.5492,  0.6471, -0.7084,  0.5667,  0.4806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0112 cost = 0.188790\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2756,  0.1293,  0.0033, -0.2845,  0.2359],\n",
      "         [ 0.4850, -0.8292, -0.8146,  0.3047,  0.0668]],\n",
      "\n",
      "        [[ 0.6799,  0.7652, -0.8383,  0.6929,  0.5075],\n",
      "         [-0.5850, -0.2500,  0.5766,  0.2432, -0.7976]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0112 cost = 0.114666\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4957, -0.5512, -0.6461,  0.4797,  0.0716],\n",
      "         [ 0.7784, -0.0123, -0.1212,  0.7749, -0.1883]],\n",
      "\n",
      "        [[-0.5134, -0.1391,  0.5542,  0.2767, -0.7414],\n",
      "         [ 0.0151, -0.0421,  0.1032,  0.1211, -0.3170]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0113 cost = 0.260831\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2462,  0.0974,  0.0174, -0.0524,  0.0930],\n",
      "         [-0.2776,  0.1310,  0.0039, -0.2879,  0.2376]],\n",
      "\n",
      "        [[ 0.5292,  0.6013, -0.7061,  0.5325,  0.4873],\n",
      "         [ 0.6823,  0.7669, -0.8394,  0.6952,  0.5109]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0113 cost = 0.148990\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4886, -0.8308, -0.8164,  0.3089,  0.0666],\n",
      "         [-0.0114,  0.0362, -0.0770,  0.0141,  0.2195]],\n",
      "\n",
      "        [[-0.5867, -0.2528,  0.5781,  0.2409, -0.7987],\n",
      "         [ 0.5567,  0.6545, -0.7143,  0.5745,  0.4905]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0113 cost = 0.128936\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4896, -0.8313, -0.8170,  0.3102,  0.0666],\n",
      "         [-0.2794,  0.1338,  0.0044, -0.2919,  0.2398]],\n",
      "\n",
      "        [[-0.5872, -0.2535,  0.5785,  0.2403, -0.7990],\n",
      "         [ 0.6850,  0.7688, -0.8406,  0.6977,  0.5146]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0114 cost = 0.082749\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2490,  0.0988,  0.0179, -0.0601,  0.0957],\n",
      "         [ 0.7843, -0.0145, -0.1249,  0.7805, -0.1962]],\n",
      "\n",
      "        [[ 0.5329,  0.6046, -0.7080,  0.5364,  0.4925],\n",
      "         [ 0.0059, -0.0507,  0.1178,  0.1178, -0.3284]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0114 cost = 0.258324\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0145,  0.0372, -0.0771,  0.0087,  0.2222],\n",
      "         [ 0.5038, -0.5568, -0.6513,  0.4900,  0.0710]],\n",
      "\n",
      "        [[ 0.5624,  0.6603, -0.7189,  0.5805,  0.4982],\n",
      "         [-0.5182, -0.1451,  0.5583,  0.2721, -0.7463]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0114 cost = 0.160696\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2820,  0.1377,  0.0052, -0.2983,  0.2429],\n",
      "         [ 0.4929, -0.8330, -0.8187,  0.3138,  0.0664]],\n",
      "\n",
      "        [[ 0.6889,  0.7717, -0.8423,  0.7013,  0.5200],\n",
      "         [-0.5888, -0.2558,  0.5798,  0.2385, -0.8000]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0115 cost = 0.108739\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2528,  0.1002,  0.0185, -0.0698,  0.0988],\n",
      "         [-0.0180,  0.0379, -0.0772,  0.0035,  0.2241]],\n",
      "\n",
      "        [[ 0.5364,  0.6077, -0.7098,  0.5401,  0.4973],\n",
      "         [ 0.5664,  0.6642, -0.7221,  0.5846,  0.5032]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0115 cost = 0.177402\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7886, -0.0162, -0.1280,  0.7841, -0.2015],\n",
      "         [ 0.5076, -0.5594, -0.6539,  0.4943,  0.0710]],\n",
      "\n",
      "        [[-0.0010, -0.0568,  0.1292,  0.1159, -0.3368],\n",
      "         [-0.5207, -0.1479,  0.5605,  0.2700, -0.7488]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0115 cost = 0.231989\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0218,  0.0390, -0.0771, -0.0024,  0.2264],\n",
      "         [-0.2854,  0.1418,  0.0062, -0.3053,  0.2461]],\n",
      "\n",
      "        [[ 0.5708,  0.6685, -0.7257,  0.5892,  0.5091],\n",
      "         [ 0.6931,  0.7747, -0.8443,  0.7052,  0.5258]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0116 cost = 0.134920\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.4967, -0.8347, -0.8207,  0.3181,  0.0663],\n",
      "         [ 0.5102, -0.5610, -0.6556,  0.4973,  0.0709]],\n",
      "\n",
      "        [[-0.5908, -0.2585,  0.5816,  0.2361, -0.8013],\n",
      "         [-0.5224, -0.1499,  0.5619,  0.2685, -0.7505]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0116 cost = 0.107625\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7916, -0.0174, -0.1303,  0.7866, -0.2058],\n",
      "         [-0.2597,  0.1025,  0.0193, -0.0856,  0.1040]],\n",
      "\n",
      "        [[-0.0055, -0.0608,  0.1368,  0.1147, -0.3422],\n",
      "         [ 0.5424,  0.6133, -0.7135,  0.5463,  0.5057]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0116 cost = 0.235786\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0276,  0.0407, -0.0771, -0.0114,  0.2297],\n",
      "         [ 0.7929, -0.0179, -0.1313,  0.7876, -0.2078]],\n",
      "\n",
      "        [[ 0.5771,  0.6748, -0.7308,  0.5956,  0.5173],\n",
      "         [-0.0075, -0.0626,  0.1399,  0.1140, -0.3446]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0117 cost = 0.251583\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2614,  0.1034,  0.0197, -0.0911,  0.1054],\n",
      "         [ 0.5146, -0.5638, -0.6583,  0.5023,  0.0707]],\n",
      "\n",
      "        [[ 0.5445,  0.6150, -0.7144,  0.5485,  0.5085],\n",
      "         [-0.5251, -0.1533,  0.5642,  0.2657, -0.7532]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0117 cost = 0.145529\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5005, -0.8366, -0.8227,  0.3223,  0.0662],\n",
      "         [-0.2903,  0.1485,  0.0076, -0.3161,  0.2512]],\n",
      "\n",
      "        [[-0.5927, -0.2615,  0.5831,  0.2337, -0.8025],\n",
      "         [ 0.6995,  0.7793, -0.8472,  0.7111,  0.5347]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0117 cost = 0.075156\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5014, -0.8371, -0.8232,  0.3233,  0.0661],\n",
      "         [-0.0322,  0.0417, -0.0773, -0.0191,  0.2325]],\n",
      "\n",
      "        [[-0.5931, -0.2622,  0.5835,  0.2332, -0.8028],\n",
      "         [ 0.5825,  0.6802, -0.7350,  0.6013,  0.5241]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0118 cost = 0.114354\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5188, -0.5664, -0.6610,  0.5070,  0.0705],\n",
      "         [ 0.7978, -0.0199, -0.1351,  0.7915, -0.2145]],\n",
      "\n",
      "        [[-0.5277, -0.1565,  0.5664,  0.2632, -0.7558],\n",
      "         [-0.0162, -0.0706,  0.1528,  0.1109, -0.3549]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0118 cost = 0.229081\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2926,  0.1515,  0.0083, -0.3223,  0.2539],\n",
      "         [-0.2662,  0.1050,  0.0204, -0.1048,  0.1090]],\n",
      "\n",
      "        [[ 0.7028,  0.7816, -0.8487,  0.7142,  0.5394],\n",
      "         [ 0.5488,  0.6185, -0.7164,  0.5529,  0.5142]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0118 cost = 0.131255\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2933,  0.1526,  0.0085, -0.3236,  0.2549],\n",
      "         [-0.2667,  0.1054,  0.0206, -0.1062,  0.1098]],\n",
      "\n",
      "        [[ 0.7040,  0.7825, -0.8492,  0.7153,  0.5411],\n",
      "         [ 0.5499,  0.6195, -0.7170,  0.5540,  0.5158]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0119 cost = 0.130438\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5053, -0.8392, -0.8252,  0.3276,  0.0660],\n",
      "         [ 0.5232, -0.5694, -0.6638,  0.5126,  0.0701]],\n",
      "\n",
      "        [[-0.5947, -0.2648,  0.5849,  0.2311, -0.8039],\n",
      "         [-0.5303, -0.1597,  0.5686,  0.2607, -0.7583]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0119 cost = 0.099997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0367,  0.0432, -0.0774, -0.0262,  0.2369],\n",
      "         [ 0.8026, -0.0220, -0.1387,  0.7959, -0.2211]],\n",
      "\n",
      "        [[ 0.5916,  0.6891, -0.7421,  0.6106,  0.5358],\n",
      "         [-0.0238, -0.0774,  0.1640,  0.1083, -0.3638]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0119 cost = 0.235512\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2951,  0.1566,  0.0094, -0.3264,  0.2580],\n",
      "         [-0.0371,  0.0435, -0.0774, -0.0268,  0.2377]],\n",
      "\n",
      "        [[ 0.7078,  0.7853, -0.8509,  0.7187,  0.5465],\n",
      "         [ 0.5934,  0.6908, -0.7434,  0.6124,  0.5379]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0120 cost = 0.121688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8051, -0.0230, -0.1406,  0.7982, -0.2240],\n",
      "         [-0.2674,  0.1075,  0.0213, -0.1081,  0.1136]],\n",
      "\n",
      "        [[-0.0277, -0.0809,  0.1697,  0.1070, -0.3682],\n",
      "         [ 0.5553,  0.6244, -0.7200,  0.5594,  0.5231]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0120 cost = 0.216759\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5087, -0.8409, -0.8269,  0.3315,  0.0658],\n",
      "         [ 0.5290, -0.5725, -0.6672,  0.5198,  0.0698]],\n",
      "\n",
      "        [[-0.5961, -0.2671,  0.5861,  0.2292, -0.8049],\n",
      "         [-0.5335, -0.1634,  0.5713,  0.2576, -0.7615]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0120 cost = 0.096936\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5307, -0.5733, -0.6681,  0.5219,  0.0697],\n",
      "         [-0.2970,  0.1607,  0.0104, -0.3294,  0.2610]],\n",
      "\n",
      "        [[-0.5343, -0.1645,  0.5720,  0.2567, -0.7624],\n",
      "         [ 0.7115,  0.7880, -0.8525,  0.7222,  0.5519]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0121 cost = 0.088838\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2666,  0.1087,  0.0218, -0.1080,  0.1152],\n",
      "         [ 0.5107, -0.8419, -0.8278,  0.3337,  0.0657]],\n",
      "\n",
      "        [[ 0.5586,  0.6273, -0.7217,  0.5627,  0.5277],\n",
      "         [-0.5969, -0.2685,  0.5867,  0.2280, -0.8055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0121 cost = 0.132050\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0377,  0.0451, -0.0773, -0.0281,  0.2416],\n",
      "         [ 0.8103, -0.0252, -0.1445,  0.8029, -0.2308]],\n",
      "\n",
      "        [[ 0.6023,  0.6994, -0.7501,  0.6217,  0.5493],\n",
      "         [-0.0352, -0.0876,  0.1804,  0.1044, -0.3766]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0121 cost = 0.224462\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8115, -0.0258, -0.1456,  0.8040, -0.2323],\n",
      "         [-0.0384,  0.0453, -0.0774, -0.0291,  0.2424]],\n",
      "\n",
      "        [[-0.0374, -0.0896,  0.1834,  0.1035, -0.3792],\n",
      "         [ 0.6039,  0.7010, -0.7513,  0.6234,  0.5513]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0122 cost = 0.209995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2999,  0.1646,  0.0115, -0.3342,  0.2646],\n",
      "         [ 0.5137, -0.8434, -0.8292,  0.3372,  0.0656]],\n",
      "\n",
      "        [[ 0.7158,  0.7910, -0.8544,  0.7262,  0.5581],\n",
      "         [-0.5980, -0.2705,  0.5877,  0.2261, -0.8063]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0122 cost = 0.092680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2682,  0.1100,  0.0225, -0.1126,  0.1179],\n",
      "         [ 0.5383, -0.5783, -0.6727,  0.5313,  0.0691]],\n",
      "\n",
      "        [[ 0.5626,  0.6304, -0.7234,  0.5669,  0.5329],\n",
      "         [-0.5384, -0.1698,  0.5753,  0.2522, -0.7664]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0122 cost = 0.130172\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5399, -0.5791, -0.6736,  0.5332,  0.0690],\n",
      "         [-0.2692,  0.1103,  0.0227, -0.1147,  0.1188]],\n",
      "\n",
      "        [[-0.5392, -0.1708,  0.5760,  0.2513, -0.7671],\n",
      "         [ 0.5637,  0.6312, -0.7240,  0.5680,  0.5343]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0123 cost = 0.117127\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0417,  0.0456, -0.0776, -0.0340,  0.2456],\n",
      "         [ 0.5168, -0.8447, -0.8307,  0.3409,  0.0655]],\n",
      "\n",
      "        [[ 0.6105,  0.7071, -0.7562,  0.6303,  0.5592],\n",
      "         [-0.5991, -0.2725,  0.5887,  0.2242, -0.8071]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0123 cost = 0.126060\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8168, -0.0288, -0.1508,  0.8088, -0.2405],\n",
      "         [-0.3037,  0.1680,  0.0128, -0.3406,  0.2683]],\n",
      "\n",
      "        [[-0.0466, -0.0981,  0.1963,  0.1001, -0.3896],\n",
      "         [ 0.7203,  0.7940, -0.8564,  0.7304,  0.5642]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0123 cost = 0.167184\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5440, -0.5821, -0.6764,  0.5379,  0.0688],\n",
      "         [-0.2737,  0.1114,  0.0233, -0.1236,  0.1220]],\n",
      "\n",
      "        [[-0.5415, -0.1735,  0.5779,  0.2488, -0.7694],\n",
      "         [ 0.5677,  0.6346, -0.7264,  0.5720,  0.5393]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0124 cost = 0.114224\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3058,  0.1694,  0.0134, -0.3441,  0.2701],\n",
      "         [ 0.5197, -0.8460, -0.8321,  0.3444,  0.0655]],\n",
      "\n",
      "        [[ 0.7225,  0.7954, -0.8574,  0.7325,  0.5672],\n",
      "         [-0.6003, -0.2744,  0.5897,  0.2221, -0.8079]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0124 cost = 0.088723\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8195, -0.0306, -0.1538,  0.8112, -0.2454],\n",
      "         [-0.0481,  0.0459, -0.0778, -0.0428,  0.2491]],\n",
      "\n",
      "        [[-0.0508, -0.1017,  0.2023,  0.0987, -0.3943],\n",
      "         [ 0.6174,  0.7133, -0.7614,  0.6375,  0.5674]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0124 cost = 0.196831\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5481, -0.5854, -0.6791,  0.5428,  0.0685],\n",
      "         [ 0.5218, -0.8468, -0.8330,  0.3470,  0.0654]],\n",
      "\n",
      "        [[-0.5437, -0.1761,  0.5797,  0.2463, -0.7715],\n",
      "         [-0.6011, -0.2757,  0.5904,  0.2206, -0.8084]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0125 cost = 0.117711\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3084,  0.1709,  0.0143, -0.3485,  0.2725],\n",
      "         [ 0.8216, -0.0320, -0.1560,  0.8132, -0.2495]],\n",
      "\n",
      "        [[ 0.7256,  0.7974, -0.8588,  0.7354,  0.5714],\n",
      "         [-0.0542, -0.1049,  0.2069,  0.0974, -0.3982]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0125 cost = 0.170026\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0492,  0.0450, -0.0782, -0.0446,  0.2505],\n",
      "         [-0.2779,  0.1124,  0.0243, -0.1329,  0.1246]],\n",
      "\n",
      "        [[ 0.6216,  0.7170, -0.7644,  0.6420,  0.5723],\n",
      "         [ 0.5730,  0.6386, -0.7292,  0.5774,  0.5459]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0125 cost = 0.142773\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3099,  0.1718,  0.0148, -0.3509,  0.2739],\n",
      "         [ 0.5527, -0.5892, -0.6823,  0.5487,  0.0679]],\n",
      "\n",
      "        [[ 0.7275,  0.7986, -0.8596,  0.7371,  0.5739],\n",
      "         [-0.5461, -0.1793,  0.5817,  0.2432, -0.7738]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0126 cost = 0.087953\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8248, -0.0344, -0.1596,  0.8163, -0.2553],\n",
      "         [-0.2798,  0.1129,  0.0246, -0.1362,  0.1258]],\n",
      "\n",
      "        [[-0.0600, -0.1104,  0.2145,  0.0948, -0.4047],\n",
      "         [ 0.5751,  0.6404, -0.7304,  0.5796,  0.5487]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0126 cost = 0.189215\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0524,  0.0446, -0.0785, -0.0482,  0.2525],\n",
      "         [ 0.5266, -0.8492, -0.8354,  0.3532,  0.0651]],\n",
      "\n",
      "        [[ 0.6261,  0.7211, -0.7678,  0.6468,  0.5777],\n",
      "         [-0.6032, -0.2795,  0.5922,  0.2166, -0.8099]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0126 cost = 0.117560\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2817,  0.1134,  0.0249, -0.1397,  0.1266],\n",
      "         [-0.3129,  0.1736,  0.0156, -0.3550,  0.2762]],\n",
      "\n",
      "        [[ 0.5772,  0.6420, -0.7315,  0.5817,  0.5513],\n",
      "         [ 0.7305,  0.8005, -0.8609,  0.7399,  0.5781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0127 cost = 0.117455\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5575, -0.5933, -0.6860,  0.5546,  0.0673],\n",
      "         [-0.0568,  0.0451, -0.0787, -0.0538,  0.2544]],\n",
      "\n",
      "        [[-0.5488, -0.1826,  0.5840,  0.2399, -0.7765],\n",
      "         [ 0.6295,  0.7242, -0.7703,  0.6503,  0.5818]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0127 cost = 0.107692\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8276, -0.0368, -0.1637,  0.8188, -0.2617],\n",
      "         [ 0.5286, -0.8501, -0.8366,  0.3557,  0.0651]],\n",
      "\n",
      "        [[-0.0651, -0.1149,  0.2218,  0.0932, -0.4102],\n",
      "         [-0.6042, -0.2811,  0.5932,  0.2146, -0.8106]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0127 cost = 0.174465\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2868,  0.1151,  0.0255, -0.1494,  0.1304],\n",
      "         [ 0.5294, -0.8506, -0.8370,  0.3568,  0.0650]],\n",
      "\n",
      "        [[ 0.5813,  0.6457, -0.7341,  0.5859,  0.5567],\n",
      "         [-0.6046, -0.2817,  0.5936,  0.2138, -0.8109]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0128 cost = 0.116887\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0635,  0.0459, -0.0791, -0.0626,  0.2575],\n",
      "         [ 0.5607, -0.5963, -0.6885,  0.5581,  0.0671]],\n",
      "\n",
      "        [[ 0.6345,  0.7288, -0.7741,  0.6556,  0.5879],\n",
      "         [-0.5508, -0.1849,  0.5856,  0.2376, -0.7784]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0128 cost = 0.115063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8294, -0.0386, -0.1666,  0.8203, -0.2664],\n",
      "         [-0.3169,  0.1795,  0.0168, -0.3624,  0.2809]],\n",
      "\n",
      "        [[-0.0683, -0.1176,  0.2264,  0.0923, -0.4136],\n",
      "         [ 0.7359,  0.8042, -0.8634,  0.7450,  0.5858]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0128 cost = 0.149296\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5629, -0.5982, -0.6903,  0.5606,  0.0669],\n",
      "         [-0.2919,  0.1165,  0.0260, -0.1596,  0.1337]],\n",
      "\n",
      "        [[-0.5521, -0.1865,  0.5867,  0.2360, -0.7796],\n",
      "         [ 0.5850,  0.6489, -0.7365,  0.5898,  0.5614]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0129 cost = 0.101414\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5330, -0.8520, -0.8388,  0.3611,  0.0649],\n",
      "         [-0.3185,  0.1812,  0.0173, -0.3654,  0.2826]],\n",
      "\n",
      "        [[-0.6063, -0.2843,  0.5950,  0.2106, -0.8120],\n",
      "         [ 0.7378,  0.8054, -0.8642,  0.7468,  0.5885]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0129 cost = 0.056455\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8314, -0.0406, -0.1697,  0.8222, -0.2717],\n",
      "         [-0.0718,  0.0465, -0.0797, -0.0737,  0.2612]],\n",
      "\n",
      "        [[-0.0718, -0.1207,  0.2312,  0.0911, -0.4174],\n",
      "         [ 0.6407,  0.7342, -0.7787,  0.6621,  0.5952]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0129 cost = 0.175097\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0734,  0.0464, -0.0799, -0.0758,  0.2619],\n",
      "         [-0.3199,  0.1827,  0.0177, -0.3683,  0.2842]],\n",
      "\n",
      "        [[ 0.6421,  0.7355, -0.7798,  0.6636,  0.5968],\n",
      "         [ 0.7397,  0.8066, -0.8650,  0.7485,  0.5911]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0130 cost = 0.098018\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5671, -0.6025, -0.6938,  0.5655,  0.0664],\n",
      "         [ 0.5352, -0.8531, -0.8400,  0.3638,  0.0648]],\n",
      "\n",
      "        [[-0.5545, -0.1894,  0.5887,  0.2330, -0.7820],\n",
      "         [-0.6073, -0.2859,  0.5959,  0.2087, -0.8127]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0130 cost = 0.105659\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2989,  0.1185,  0.0268, -0.1743,  0.1378],\n",
      "         [ 0.8332, -0.0426, -0.1729,  0.8238, -0.2772]],\n",
      "\n",
      "        [[ 0.5906,  0.6536, -0.7399,  0.5955,  0.5687],\n",
      "         [-0.0756, -0.1242,  0.2363,  0.0897, -0.4215]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0130 cost = 0.180235\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3000,  0.1188,  0.0269, -0.1768,  0.1386],\n",
      "         [ 0.8340, -0.0434, -0.1741,  0.8245, -0.2790]],\n",
      "\n",
      "        [[ 0.5914,  0.6542, -0.7403,  0.5963,  0.5698],\n",
      "         [-0.0774, -0.1259,  0.2385,  0.0889, -0.4235]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0131 cost = 0.178842\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5374, -0.8545, -0.8412,  0.3666,  0.0647],\n",
      "         [-0.0819,  0.0468, -0.0807, -0.0864,  0.2654]],\n",
      "\n",
      "        [[-0.6084, -0.2879,  0.5969,  0.2065, -0.8134],\n",
      "         [ 0.6475,  0.7404, -0.7837,  0.6694,  0.6034]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0131 cost = 0.081663\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3233,  0.1871,  0.0185, -0.3753,  0.2882],\n",
      "         [ 0.5715, -0.6073, -0.6975,  0.5707,  0.0659]],\n",
      "\n",
      "        [[ 0.7438,  0.8093, -0.8668,  0.7524,  0.5971],\n",
      "         [-0.5572, -0.1931,  0.5909,  0.2293, -0.7845]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0131 cost = 0.076967\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8366, -0.0459, -0.1781,  0.8268, -0.2842],\n",
      "         [ 0.5727, -0.6084, -0.6985,  0.5721,  0.0658]],\n",
      "\n",
      "        [[-0.0838, -0.1320,  0.2461,  0.0858, -0.4305],\n",
      "         [-0.5579, -0.1940,  0.5914,  0.2285, -0.7851]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0132 cost = 0.158845\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5397, -0.8559, -0.8425,  0.3695,  0.0645],\n",
      "         [-0.3034,  0.1200,  0.0275, -0.1854,  0.1414]],\n",
      "\n",
      "        [[-0.6095, -0.2899,  0.5978,  0.2045, -0.8141],\n",
      "         [ 0.5940,  0.6558, -0.7411,  0.5988,  0.5730]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0132 cost = 0.081319\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3248,  0.1888,  0.0191, -0.3789,  0.2903],\n",
      "         [-0.0861,  0.0463, -0.0815, -0.0927,  0.2681]],\n",
      "\n",
      "        [[ 0.7459,  0.8105, -0.8676,  0.7543,  0.6000],\n",
      "         [ 0.6520,  0.7444, -0.7868,  0.6741,  0.6085]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0132 cost = 0.090934\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5414, -0.8568, -0.8433,  0.3715,  0.0644],\n",
      "         [-0.3253,  0.1898,  0.0194, -0.3801,  0.2912]],\n",
      "\n",
      "        [[-0.6101, -0.2911,  0.5984,  0.2033, -0.8145],\n",
      "         [ 0.7467,  0.8111, -0.8680,  0.7551,  0.6012]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0133 cost = 0.052045\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0870,  0.0466, -0.0817, -0.0947,  0.2695],\n",
      "         [ 0.5781, -0.6126, -0.7022,  0.5786,  0.0650]],\n",
      "\n",
      "        [[ 0.6545,  0.7466, -0.7885,  0.6767,  0.6115],\n",
      "         [-0.5605, -0.1976,  0.5935,  0.2249, -0.7876]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0133 cost = 0.102725\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8411, -0.0497, -0.1842,  0.8309, -0.2929],\n",
      "         [-0.3049,  0.1214,  0.0282, -0.1921,  0.1443]],\n",
      "\n",
      "        [[-0.0922, -0.1399,  0.2562,  0.0818, -0.4394],\n",
      "         [ 0.5975,  0.6585, -0.7429,  0.6023,  0.5775]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0133 cost = 0.161497\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5804, -0.6143, -0.7038,  0.5813,  0.0647],\n",
      "         [-0.0892,  0.0470, -0.0818, -0.0980,  0.2709]],\n",
      "\n",
      "        [[-0.5616, -0.1990,  0.5945,  0.2235, -0.7887],\n",
      "         [ 0.6572,  0.7490, -0.7904,  0.6794,  0.6146]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0134 cost = 0.090921\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3273,  0.1932,  0.0204, -0.3849,  0.2942],\n",
      "         [ 0.8426, -0.0512, -0.1864,  0.8322, -0.2963]],\n",
      "\n",
      "        [[ 0.7501,  0.8133, -0.8695,  0.7582,  0.6062],\n",
      "         [-0.0950, -0.1424,  0.2595,  0.0806, -0.4423]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0134 cost = 0.138078\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5449, -0.8586, -0.8452,  0.3758,  0.0642],\n",
      "         [-0.3064,  0.1222,  0.0287, -0.1970,  0.1459]],\n",
      "\n",
      "        [[-0.6115, -0.2936,  0.5997,  0.2008, -0.8154],\n",
      "         [ 0.6001,  0.6605, -0.7441,  0.6048,  0.5808]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0134 cost = 0.077858\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5839, -0.6175, -0.7064,  0.5855,  0.0642],\n",
      "         [ 0.5456, -0.8590, -0.8455,  0.3766,  0.0641]],\n",
      "\n",
      "        [[-0.5634, -0.2013,  0.5959,  0.2212, -0.7904],\n",
      "         [-0.6118, -0.2940,  0.5999,  0.2003, -0.8156]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0135 cost = 0.096251\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3285,  0.1949,  0.0211, -0.3882,  0.2964],\n",
      "         [-0.0923,  0.0467, -0.0823, -0.1028,  0.2734]],\n",
      "\n",
      "        [[ 0.7523,  0.8147, -0.8704,  0.7602,  0.6094],\n",
      "         [ 0.6620,  0.7532, -0.7937,  0.6845,  0.6202]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0135 cost = 0.086160\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3068,  0.1229,  0.0292, -0.2008,  0.1479],\n",
      "         [ 0.8459, -0.0543, -0.1913,  0.8353, -0.3027]],\n",
      "\n",
      "        [[ 0.6027,  0.6626, -0.7455,  0.6074,  0.5842],\n",
      "         [-0.1012, -0.1481,  0.2668,  0.0777, -0.4487]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0135 cost = 0.161042\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8468, -0.0551, -0.1926,  0.8362, -0.3042],\n",
      "         [-0.3068,  0.1232,  0.0293, -0.2018,  0.1487]],\n",
      "\n",
      "        [[-0.1029, -0.1497,  0.2687,  0.0769, -0.4504],\n",
      "         [ 0.6035,  0.6632, -0.7459,  0.6082,  0.5852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0136 cost = 0.153618\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3298,  0.1971,  0.0220, -0.3912,  0.2987],\n",
      "         [ 0.5487, -0.8607, -0.8470,  0.3804,  0.0639]],\n",
      "\n",
      "        [[ 0.7546,  0.8162, -0.8714,  0.7624,  0.6128],\n",
      "         [-0.6130, -0.2963,  0.6011,  0.1979, -0.8164]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0136 cost = 0.069810\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0919,  0.0466, -0.0828, -0.1036,  0.2756],\n",
      "         [ 0.5905, -0.6226, -0.7108,  0.5935,  0.0633]],\n",
      "\n",
      "        [[ 0.6665,  0.7571, -0.7967,  0.6892,  0.6255],\n",
      "         [-0.5664, -0.2053,  0.5984,  0.2171, -0.7932]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0136 cost = 0.095803\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3071,  0.1240,  0.0299, -0.2046,  0.1504],\n",
      "         [-0.3309,  0.1984,  0.0226, -0.3933,  0.3002]],\n",
      "\n",
      "        [[ 0.6059,  0.6650, -0.7470,  0.6105,  0.5881],\n",
      "         [ 0.7561,  0.8171, -0.8720,  0.7638,  0.6150]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0137 cost = 0.100612\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0938,  0.0472, -0.0829, -0.1065,  0.2772],\n",
      "         [ 0.5927, -0.6242, -0.7123,  0.5960,  0.0631]],\n",
      "\n",
      "        [[ 0.6690,  0.7592, -0.7984,  0.6917,  0.6284],\n",
      "         [-0.5675, -0.2066,  0.5992,  0.2158, -0.7942]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0137 cost = 0.094428\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5515, -0.8619, -0.8483,  0.3836,  0.0638],\n",
      "         [ 0.8506, -0.0588, -0.1986,  0.8396, -0.3113]],\n",
      "\n",
      "        [[-0.6141, -0.2981,  0.6021,  0.1956, -0.8172],\n",
      "         [-0.1096, -0.1557,  0.2768,  0.0740, -0.4573]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0137 cost = 0.129304\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.0969,  0.0482, -0.0831, -0.1110,  0.2793],\n",
      "         [ 0.5523, -0.8622, -0.8486,  0.3846,  0.0637]],\n",
      "\n",
      "        [[ 0.6715,  0.7614, -0.8001,  0.6943,  0.6313],\n",
      "         [-0.6144, -0.2986,  0.6024,  0.1950, -0.8174]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0138 cost = 0.093647\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3326,  0.2040,  0.0236, -0.3971,  0.3039],\n",
      "         [ 0.5957, -0.6263, -0.7145,  0.5992,  0.0629]],\n",
      "\n",
      "        [[ 0.7594,  0.8192, -0.8734,  0.7668,  0.6197],\n",
      "         [-0.5692, -0.2086,  0.6006,  0.2136, -0.7957]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0138 cost = 0.066204\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8521, -0.0607, -0.2016,  0.8409, -0.3147],\n",
      "         [-0.3124,  0.1270,  0.0308, -0.2161,  0.1571]],\n",
      "\n",
      "        [[-0.1139, -0.1593,  0.2817,  0.0721, -0.4614],\n",
      "         [ 0.6109,  0.6691, -0.7499,  0.6154,  0.5944]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0138 cost = 0.145382\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8528, -0.0615, -0.2028,  0.8415, -0.3162],\n",
      "         [-0.3132,  0.1274,  0.0310, -0.2179,  0.1579]],\n",
      "\n",
      "        [[-0.1154, -0.1607,  0.2834,  0.0714, -0.4629],\n",
      "         [ 0.6117,  0.6697, -0.7504,  0.6162,  0.5954]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0139 cost = 0.144357\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1025,  0.0494, -0.0834, -0.1187,  0.2826],\n",
      "         [ 0.5991, -0.6286, -0.7169,  0.6031,  0.0625]],\n",
      "\n",
      "        [[ 0.6761,  0.7652, -0.8033,  0.6991,  0.6364],\n",
      "         [-0.5708, -0.2108,  0.6019,  0.2111, -0.7972]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0139 cost = 0.090366\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3346,  0.2081,  0.0247, -0.4009,  0.3068],\n",
      "         [ 0.5562, -0.8637, -0.8504,  0.3891,  0.0636]],\n",
      "\n",
      "        [[ 0.7623,  0.8209, -0.8747,  0.7695,  0.6238],\n",
      "         [-0.6157, -0.3009,  0.6036,  0.1920, -0.8182]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0139 cost = 0.065528\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5571, -0.8640, -0.8508,  0.3902,  0.0636],\n",
      "         [-0.3352,  0.2090,  0.0250, -0.4018,  0.3075]],\n",
      "\n",
      "        [[-0.6160, -0.3013,  0.6039,  0.1914, -0.8184],\n",
      "         [ 0.7630,  0.8214, -0.8750,  0.7701,  0.6248]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0140 cost = 0.044817\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3165,  0.1291,  0.0317, -0.2251,  0.1606],\n",
      "         [ 0.8553, -0.0647, -0.2075,  0.8438, -0.3228]],\n",
      "\n",
      "        [[ 0.6150,  0.6720, -0.7521,  0.6194,  0.5994],\n",
      "         [-0.1213, -0.1659,  0.2899,  0.0685, -0.4687]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0140 cost = 0.145753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1072,  0.0501, -0.0837, -0.1251,  0.2853],\n",
      "         [ 0.6036, -0.6315, -0.7199,  0.6080,  0.0619]],\n",
      "\n",
      "        [[ 0.6804,  0.7687, -0.8062,  0.7036,  0.6412],\n",
      "         [-0.5729, -0.2134,  0.6037,  0.2081, -0.7991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0140 cost = 0.087896\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3370,  0.2116,  0.0258, -0.4048,  0.3095],\n",
      "         [ 0.6046, -0.6322, -0.7206,  0.6092,  0.0618]],\n",
      "\n",
      "        [[ 0.7651,  0.8226, -0.8759,  0.7720,  0.6277],\n",
      "         [-0.5735, -0.2141,  0.6041,  0.2074, -0.7995]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0141 cost = 0.062458\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5605, -0.8651, -0.8522,  0.3941,  0.0635],\n",
      "         [ 0.8570, -0.0669, -0.2109,  0.8454, -0.3269]],\n",
      "\n",
      "        [[-0.6169, -0.3031,  0.6048,  0.1891, -0.8190],\n",
      "         [-0.1256, -0.1698,  0.2947,  0.0663, -0.4730]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0141 cost = 0.118914\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1105,  0.0504, -0.0838, -0.1295,  0.2872],\n",
      "         [-0.3200,  0.1306,  0.0324, -0.2319,  0.1632]],\n",
      "\n",
      "        [[ 0.6833,  0.7710, -0.8081,  0.7066,  0.6443],\n",
      "         [ 0.6177,  0.6736, -0.7532,  0.6218,  0.6024]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0141 cost = 0.107643\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5622, -0.8657, -0.8530,  0.3961,  0.0634],\n",
      "         [-0.1124,  0.0508, -0.0839, -0.1317,  0.2881]],\n",
      "\n",
      "        [[-0.6174, -0.3040,  0.6052,  0.1880, -0.8193],\n",
      "         [ 0.6845,  0.7719, -0.8089,  0.7077,  0.6455]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0142 cost = 0.064925\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6089, -0.6349, -0.7236,  0.6139,  0.0614],\n",
      "         [-0.3395,  0.2149,  0.0269, -0.4084,  0.3121]],\n",
      "\n",
      "        [[-0.5756, -0.2169,  0.6058,  0.2042, -0.8014],\n",
      "         [ 0.7677,  0.8241, -0.8770,  0.7744,  0.6314]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0142 cost = 0.052070\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8589, -0.0697, -0.2150,  0.8472, -0.3315],\n",
      "         [-0.3238,  0.1321,  0.0329, -0.2378,  0.1662]],\n",
      "\n",
      "        [[-0.1314, -0.1749,  0.3009,  0.0634, -0.4786],\n",
      "         [ 0.6203,  0.6756, -0.7547,  0.6243,  0.6056]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0142 cost = 0.133948\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6107, -0.6364, -0.7250,  0.6159,  0.0612],\n",
      "         [ 0.5643, -0.8665, -0.8539,  0.3985,  0.0633]],\n",
      "\n",
      "        [[-0.5765, -0.2181,  0.6066,  0.2029, -0.8023],\n",
      "         [-0.6180, -0.3050,  0.6058,  0.1866, -0.8197]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0143 cost = 0.083040\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3253,  0.1327,  0.0332, -0.2406,  0.1674],\n",
      "         [-0.1189,  0.0516, -0.0841, -0.1394,  0.2913]],\n",
      "\n",
      "        [[ 0.6218,  0.6767, -0.7554,  0.6257,  0.6074],\n",
      "         [ 0.6888,  0.7755, -0.8118,  0.7121,  0.6504]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0143 cost = 0.105376\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8604, -0.0719, -0.2182,  0.8486, -0.3351],\n",
      "         [-0.3417,  0.2181,  0.0279, -0.4116,  0.3146]],\n",
      "\n",
      "        [[-0.1348, -0.1779,  0.3047,  0.0618, -0.4819],\n",
      "         [ 0.7705,  0.8258, -0.8782,  0.7769,  0.6355]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0143 cost = 0.106349\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3264,  0.1336,  0.0335, -0.2432,  0.1695],\n",
      "         [ 0.8610, -0.0728, -0.2195,  0.8492, -0.3365]],\n",
      "\n",
      "        [[ 0.6237,  0.6783, -0.7566,  0.6275,  0.6098],\n",
      "         [-0.1361, -0.1790,  0.3061,  0.0612, -0.4831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0144 cost = 0.135311\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5672, -0.8678, -0.8552,  0.4019,  0.0632],\n",
      "         [ 0.6152, -0.6398, -0.7281,  0.6209,  0.0607]],\n",
      "\n",
      "        [[-0.6190, -0.3068,  0.6068,  0.1843, -0.8204],\n",
      "         [-0.5786, -0.2208,  0.6083,  0.1999, -0.8042]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0144 cost = 0.057292\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1204,  0.0523, -0.0846, -0.1421,  0.2941],\n",
      "         [-0.3430,  0.2205,  0.0285, -0.4138,  0.3168]],\n",
      "\n",
      "        [[ 0.6926,  0.7788, -0.8143,  0.7161,  0.6548],\n",
      "         [ 0.7722,  0.8268, -0.8789,  0.7785,  0.6381]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0144 cost = 0.074281\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3278,  0.1349,  0.0340, -0.2465,  0.1723],\n",
      "         [ 0.8630, -0.0754, -0.2235,  0.8510, -0.3407]],\n",
      "\n",
      "        [[ 0.6257,  0.6798, -0.7575,  0.6294,  0.6123],\n",
      "         [-0.1407, -0.1832,  0.3108,  0.0588, -0.4876]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0145 cost = 0.132381\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3440,  0.2229,  0.0289, -0.4154,  0.3184],\n",
      "         [ 0.5692, -0.8688, -0.8562,  0.4042,  0.0631]],\n",
      "\n",
      "        [[ 0.7734,  0.8276, -0.8794,  0.7795,  0.6398],\n",
      "         [-0.6197, -0.3082,  0.6075,  0.1827, -0.8209]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0145 cost = 0.059179\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6195, -0.6430, -0.7311,  0.6255,  0.0603],\n",
      "         [-0.1237,  0.0535, -0.0849, -0.1464,  0.2967]],\n",
      "\n",
      "        [[-0.5806, -0.2236,  0.6099,  0.1968, -0.8060],\n",
      "         [ 0.6953,  0.7810, -0.8160,  0.7188,  0.6579]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0145 cost = 0.069662\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3452,  0.2249,  0.0294, -0.4170,  0.3200],\n",
      "         [ 0.8647, -0.0777, -0.2272,  0.8525, -0.3440]],\n",
      "\n",
      "        [[ 0.7746,  0.8283, -0.8799,  0.7806,  0.6416],\n",
      "         [-0.1451, -0.1872,  0.3153,  0.0564, -0.4919]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0146 cost = 0.105997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1259,  0.0539, -0.0851, -0.1490,  0.2983],\n",
      "         [ 0.6215, -0.6447, -0.7326,  0.6277,  0.0602]],\n",
      "\n",
      "        [[ 0.6971,  0.7824, -0.8171,  0.7206,  0.6599],\n",
      "         [-0.5816, -0.2251,  0.6107,  0.1953, -0.8069]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0146 cost = 0.078455\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3319,  0.1371,  0.0348, -0.2534,  0.1776],\n",
      "         [ 0.5720, -0.8700, -0.8574,  0.4073,  0.0630]],\n",
      "\n",
      "        [[ 0.6286,  0.6817, -0.7586,  0.6320,  0.6160],\n",
      "         [-0.6206, -0.3100,  0.6083,  0.1807, -0.8215]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0146 cost = 0.086331\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8662, -0.0802, -0.2309,  0.8540, -0.3473],\n",
      "         [ 0.6235, -0.6463, -0.7340,  0.6297,  0.0601]],\n",
      "\n",
      "        [[-0.1495, -0.1913,  0.3198,  0.0539, -0.4963],\n",
      "         [-0.5826, -0.2264,  0.6115,  0.1938, -0.8078]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0147 cost = 0.113855\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3340,  0.1379,  0.0352, -0.2569,  0.1797],\n",
      "         [-0.1298,  0.0545, -0.0854, -0.1537,  0.3008]],\n",
      "\n",
      "        [[ 0.6299,  0.6825, -0.7592,  0.6332,  0.6174],\n",
      "         [ 0.6997,  0.7845, -0.8188,  0.7233,  0.6628]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0147 cost = 0.099515\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3483,  0.2295,  0.0307, -0.4210,  0.3238],\n",
      "         [ 0.5744, -0.8708, -0.8584,  0.4100,  0.0630]],\n",
      "\n",
      "        [[ 0.7774,  0.8299, -0.8811,  0.7831,  0.6457],\n",
      "         [-0.6212, -0.3113,  0.6089,  0.1789, -0.8219]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0147 cost = 0.056814\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8678, -0.0825, -0.2347,  0.8555, -0.3507],\n",
      "         [-0.3488,  0.2306,  0.0310, -0.4217,  0.3246]],\n",
      "\n",
      "        [[-0.1530, -0.1945,  0.3235,  0.0520, -0.4997],\n",
      "         [ 0.7780,  0.8302, -0.8814,  0.7837,  0.6466]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0148 cost = 0.096539\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3362,  0.1394,  0.0357, -0.2607,  0.1835],\n",
      "         [ 0.6280, -0.6491, -0.7369,  0.6345,  0.0597]],\n",
      "\n",
      "        [[ 0.6322,  0.6842, -0.7605,  0.6355,  0.6203],\n",
      "         [-0.5844, -0.2291,  0.6130,  0.1908, -0.8094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0148 cost = 0.078646\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5770, -0.8714, -0.8593,  0.4130,  0.0630],\n",
      "         [-0.1319,  0.0560, -0.0857, -0.1572,  0.3042]],\n",
      "\n",
      "        [[-0.6219, -0.3126,  0.6095,  0.1772, -0.8224],\n",
      "         [ 0.7033,  0.7873, -0.8211,  0.7270,  0.6668]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0148 cost = 0.056875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5778, -0.8716, -0.8596,  0.4139,  0.0630],\n",
      "         [-0.3380,  0.1404,  0.0361, -0.2639,  0.1861]],\n",
      "\n",
      "        [[-0.6221, -0.3129,  0.6097,  0.1767, -0.8225],\n",
      "         [ 0.6339,  0.6854, -0.7615,  0.6371,  0.6223]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0149 cost = 0.060020\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3509,  0.2344,  0.0321, -0.4249,  0.3279],\n",
      "         [-0.1336,  0.0569, -0.0857, -0.1603,  0.3061]],\n",
      "\n",
      "        [[ 0.7805,  0.8317, -0.8824,  0.7859,  0.6502],\n",
      "         [ 0.7052,  0.7889, -0.8223,  0.7289,  0.6690]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0149 cost = 0.066320\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8702, -0.0859, -0.2402,  0.8577, -0.3561],\n",
      "         [ 0.6321, -0.6509, -0.7393,  0.6387,  0.0596]],\n",
      "\n",
      "        [[-0.1569, -0.1979,  0.3280,  0.0504, -0.5033],\n",
      "         [-0.5860, -0.2311,  0.6143,  0.1886, -0.8108]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0149 cost = 0.108230\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1343,  0.0579, -0.0857, -0.1620,  0.3077],\n",
      "         [ 0.8708, -0.0866, -0.2414,  0.8582, -0.3574]],\n",
      "\n",
      "        [[ 0.7071,  0.7905, -0.8236,  0.7310,  0.6713],\n",
      "         [-0.1578, -0.1986,  0.3289,  0.0500, -0.5041]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0150 cost = 0.119429\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6343, -0.6521, -0.7407,  0.6411,  0.0593],\n",
      "         [ 0.5810, -0.8724, -0.8607,  0.4174,  0.0630]],\n",
      "\n",
      "        [[-0.5868, -0.2323,  0.6150,  0.1873, -0.8115],\n",
      "         [-0.6229, -0.3142,  0.6105,  0.1748, -0.8230]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0150 cost = 0.073034\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3400,  0.1427,  0.0371, -0.2697,  0.1916],\n",
      "         [-0.3523,  0.2382,  0.0332, -0.4274,  0.3310]],\n",
      "\n",
      "        [[ 0.6380,  0.6886, -0.7639,  0.6410,  0.6274],\n",
      "         [ 0.7829,  0.8331, -0.8834,  0.7882,  0.6539]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0150 cost = 0.082580\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6366, -0.6535, -0.7421,  0.6435,  0.0591],\n",
      "         [-0.3522,  0.2397,  0.0334, -0.4277,  0.3320]],\n",
      "\n",
      "        [[-0.5878, -0.2336,  0.6158,  0.1858, -0.8124],\n",
      "         [ 0.7835,  0.8334, -0.8837,  0.7887,  0.6548]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0151 cost = 0.042988\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8730, -0.0899, -0.2464,  0.8602, -0.3620],\n",
      "         [-0.3405,  0.1440,  0.0374, -0.2716,  0.1946]],\n",
      "\n",
      "        [[-0.1619, -0.2024,  0.3331,  0.0478, -0.5079],\n",
      "         [ 0.6395,  0.6899, -0.7648,  0.6426,  0.6294]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0151 cost = 0.113142\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1348,  0.0598, -0.0864, -0.1645,  0.3116],\n",
      "         [ 0.5840, -0.8734, -0.8619,  0.4208,  0.0629]],\n",
      "\n",
      "        [[ 0.7112,  0.7937, -0.8262,  0.7352,  0.6760],\n",
      "         [-0.6238, -0.3160,  0.6114,  0.1724, -0.8237]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0151 cost = 0.073447\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3412,  0.1452,  0.0377, -0.2738,  0.1971],\n",
      "         [-0.1357,  0.0602, -0.0865, -0.1659,  0.3124]],\n",
      "\n",
      "        [[ 0.6411,  0.6911, -0.7657,  0.6441,  0.6313],\n",
      "         [ 0.7121,  0.7944, -0.8268,  0.7361,  0.6770]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0152 cost = 0.092300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6405, -0.6561, -0.7448,  0.6475,  0.0588],\n",
      "         [-0.3526,  0.2453,  0.0343, -0.4294,  0.3355]],\n",
      "\n",
      "        [[-0.5894, -0.2357,  0.6171,  0.1834, -0.8139],\n",
      "         [ 0.7858,  0.8349, -0.8847,  0.7909,  0.6585]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0152 cost = 0.041784\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8745, -0.0928, -0.2507,  0.8615, -0.3657],\n",
      "         [ 0.5859, -0.8740, -0.8626,  0.4229,  0.0630]],\n",
      "\n",
      "        [[-0.1645, -0.2044,  0.3360,  0.0471, -0.5101],\n",
      "         [-0.6243, -0.3170,  0.6120,  0.1708, -0.8242]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0152 cost = 0.107770\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1375,  0.0624, -0.0867, -0.1692,  0.3154],\n",
      "         [-0.3529,  0.2481,  0.0347, -0.4303,  0.3373]],\n",
      "\n",
      "        [[ 0.7150,  0.7967, -0.8286,  0.7391,  0.6804],\n",
      "         [ 0.7870,  0.8357, -0.8852,  0.7920,  0.6604]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0153 cost = 0.064110\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5872, -0.8744, -0.8632,  0.4243,  0.0630],\n",
      "         [ 0.6431, -0.6579, -0.7466,  0.6501,  0.0587]],\n",
      "\n",
      "        [[-0.6247, -0.3177,  0.6125,  0.1696, -0.8245],\n",
      "         [-0.5905, -0.2371,  0.6180,  0.1817, -0.8149]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0153 cost = 0.048167\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3440,  0.1486,  0.0385, -0.2803,  0.2046],\n",
      "         [ 0.8757, -0.0949, -0.2540,  0.8625, -0.3688]],\n",
      "\n",
      "        [[ 0.6457,  0.6951, -0.7688,  0.6487,  0.6372],\n",
      "         [-0.1664, -0.2060,  0.3381,  0.0463, -0.5118]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0153 cost = 0.112205\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6449, -0.6591, -0.7478,  0.6519,  0.0586],\n",
      "         [-0.3532,  0.2524,  0.0352, -0.4314,  0.3397]],\n",
      "\n",
      "        [[-0.5913, -0.2382,  0.6186,  0.1804, -0.8156],\n",
      "         [ 0.7886,  0.8367, -0.8859,  0.7936,  0.6630]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0154 cost = 0.040377\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8765, -0.0965, -0.2564,  0.8632, -0.3708],\n",
      "         [-0.3450,  0.1497,  0.0388, -0.2825,  0.2071]],\n",
      "\n",
      "        [[-0.1682, -0.2076,  0.3400,  0.0454, -0.5135],\n",
      "         [ 0.6470,  0.6961, -0.7695,  0.6500,  0.6388]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0154 cost = 0.107420\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1410,  0.0652, -0.0873, -0.1745,  0.3197],\n",
      "         [ 0.5900, -0.8754, -0.8643,  0.4273,  0.0629]],\n",
      "\n",
      "        [[ 0.7189,  0.7999, -0.8311,  0.7432,  0.6850],\n",
      "         [-0.6256, -0.3194,  0.6133,  0.1673, -0.8252]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0154 cost = 0.069717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8774, -0.0984, -0.2590,  0.8641, -0.3732],\n",
      "         [ 0.5907, -0.8757, -0.8646,  0.4281,  0.0629]],\n",
      "\n",
      "        [[-0.1702, -0.2094,  0.3419,  0.0443, -0.5153],\n",
      "         [-0.6258, -0.3198,  0.6135,  0.1668, -0.8253]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0155 cost = 0.103686\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3542,  0.2567,  0.0359, -0.4332,  0.3425],\n",
      "         [-0.3464,  0.1511,  0.0392, -0.2853,  0.2097]],\n",
      "\n",
      "        [[ 0.7905,  0.8378, -0.8867,  0.7953,  0.6658],\n",
      "         [ 0.6488,  0.6974, -0.7703,  0.6516,  0.6409]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0155 cost = 0.072904\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1422,  0.0660, -0.0878, -0.1761,  0.3216],\n",
      "         [ 0.6496, -0.6629, -0.7513,  0.6568,  0.0579]],\n",
      "\n",
      "        [[ 0.7211,  0.8017, -0.8325,  0.7455,  0.6875],\n",
      "         [-0.5933, -0.2410,  0.6202,  0.1770, -0.8173]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0155 cost = 0.065800\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6505, -0.6636, -0.7519,  0.6577,  0.0578],\n",
      "         [-0.3544,  0.2590,  0.0363, -0.4334,  0.3440]],\n",
      "\n",
      "        [[-0.5937, -0.2415,  0.6205,  0.1764, -0.8177],\n",
      "         [ 0.7915,  0.8385, -0.8871,  0.7963,  0.6675]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0156 cost = 0.038740\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8792, -0.1021, -0.2641,  0.8658, -0.3784],\n",
      "         [-0.3467,  0.1528,  0.0395, -0.2853,  0.2125]],\n",
      "\n",
      "        [[-0.1736, -0.2126,  0.3454,  0.0425, -0.5185],\n",
      "         [ 0.6510,  0.6992, -0.7717,  0.6538,  0.6437]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0156 cost = 0.103537\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1436,  0.0669, -0.0882, -0.1775,  0.3237],\n",
      "         [ 0.5941, -0.8770, -0.8660,  0.4321,  0.0628]],\n",
      "\n",
      "        [[ 0.7236,  0.8036, -0.8340,  0.7480,  0.6902],\n",
      "         [-0.6268, -0.3218,  0.6146,  0.1636, -0.8261]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0156 cost = 0.067501\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3474,  0.1537,  0.0398, -0.2861,  0.2140],\n",
      "         [ 0.5947, -0.8772, -0.8663,  0.4329,  0.0628]],\n",
      "\n",
      "        [[ 0.6525,  0.7003, -0.7725,  0.6552,  0.6454],\n",
      "         [-0.6270, -0.3221,  0.6148,  0.1630, -0.8262]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0157 cost = 0.073030\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3557,  0.2628,  0.0372, -0.4346,  0.3467],\n",
      "         [-0.1460,  0.0677, -0.0883, -0.1802,  0.3253]],\n",
      "\n",
      "        [[ 0.7937,  0.8398, -0.8880,  0.7983,  0.6708],\n",
      "         [ 0.7253,  0.8049, -0.8351,  0.7498,  0.6922]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0157 cost = 0.057520\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6545, -0.6667, -0.7549,  0.6619,  0.0574],\n",
      "         [ 0.8804, -0.1054, -0.2684,  0.8670, -0.3826]],\n",
      "\n",
      "        [[-0.5955, -0.2437,  0.6220,  0.1736, -0.8192],\n",
      "         [-0.1758, -0.2144,  0.3479,  0.0417, -0.5204]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0157 cost = 0.094285\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.5970, -0.8778, -0.8671,  0.4356,  0.0629],\n",
      "         [-0.1474,  0.0686, -0.0883, -0.1818,  0.3269]],\n",
      "\n",
      "        [[-0.6275, -0.3231,  0.6154,  0.1612, -0.8266],\n",
      "         [ 0.7270,  0.8062, -0.8363,  0.7516,  0.6941]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0158 cost = 0.047631\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3502,  0.1558,  0.0405, -0.2894,  0.2183],\n",
      "         [ 0.6563, -0.6680, -0.7561,  0.6637,  0.0572]],\n",
      "\n",
      "        [[ 0.6558,  0.7029, -0.7747,  0.6585,  0.6494],\n",
      "         [-0.5962, -0.2447,  0.6226,  0.1723, -0.8198]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0158 cost = 0.065560\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3570,  0.2667,  0.0382, -0.4358,  0.3494],\n",
      "         [ 0.8814, -0.1077, -0.2714,  0.8680, -0.3855]],\n",
      "\n",
      "        [[ 0.7959,  0.8412, -0.8890,  0.8004,  0.6743],\n",
      "         [-0.1780, -0.2163,  0.3501,  0.0407, -0.5223]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0158 cost = 0.083371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3574,  0.2675,  0.0385, -0.4362,  0.3501],\n",
      "         [-0.1497,  0.0697, -0.0884, -0.1846,  0.3292]],\n",
      "\n",
      "        [[ 0.7964,  0.8414, -0.8892,  0.8008,  0.6750],\n",
      "         [ 0.7294,  0.8080, -0.8378,  0.7541,  0.6968]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0159 cost = 0.055838\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6000, -0.8786, -0.8681,  0.4389,  0.0629],\n",
      "         [-0.3518,  0.1573,  0.0410, -0.2914,  0.2214]],\n",
      "\n",
      "        [[-0.6282, -0.3244,  0.6161,  0.1590, -0.8271],\n",
      "         [ 0.6580,  0.7045, -0.7760,  0.6606,  0.6520]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0159 cost = 0.049842\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8826, -0.1099, -0.2745,  0.8691, -0.3881],\n",
      "         [ 0.6598, -0.6700, -0.7583,  0.6672,  0.0569]],\n",
      "\n",
      "        [[-0.1802, -0.2183,  0.3525,  0.0396, -0.5243],\n",
      "         [-0.5976, -0.2465,  0.6237,  0.1701, -0.8210]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0159 cost = 0.090482\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6014, -0.8789, -0.8686,  0.4404,  0.0629],\n",
      "         [-0.3521,  0.1582,  0.0413, -0.2921,  0.2233]],\n",
      "\n",
      "        [[-0.6285, -0.3250,  0.6164,  0.1581, -0.8273],\n",
      "         [ 0.6594,  0.7055, -0.7768,  0.6619,  0.6537]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0160 cost = 0.049290\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8835, -0.1117, -0.2769,  0.8700, -0.3904],\n",
      "         [-0.3585,  0.2713,  0.0396, -0.4376,  0.3529]],\n",
      "\n",
      "        [[-0.1819, -0.2197,  0.3540,  0.0387, -0.5258],\n",
      "         [ 0.7985,  0.8427, -0.8901,  0.8027,  0.6782]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0160 cost = 0.077186\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6627, -0.6718, -0.7601,  0.6704,  0.0564],\n",
      "         [-0.1493,  0.0713, -0.0886, -0.1851,  0.3322]],\n",
      "\n",
      "        [[-0.5986, -0.2479,  0.6246,  0.1684, -0.8219],\n",
      "         [ 0.7330,  0.8107, -0.8400,  0.7577,  0.7009]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0160 cost = 0.051494\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6637, -0.6725, -0.7608,  0.6713,  0.0562],\n",
      "         [-0.3588,  0.2727,  0.0401, -0.4384,  0.3541]],\n",
      "\n",
      "        [[-0.5990, -0.2484,  0.6249,  0.1678, -0.8222],\n",
      "         [ 0.7994,  0.8432, -0.8905,  0.8035,  0.6796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0161 cost = 0.035070\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3517,  0.1595,  0.0420, -0.2933,  0.2259],\n",
      "         [ 0.8850, -0.1148, -0.2807,  0.8714, -0.3943]],\n",
      "\n",
      "        [[ 0.6618,  0.7072, -0.7780,  0.6641,  0.6565],\n",
      "         [-0.1846, -0.2223,  0.3566,  0.0371, -0.5282]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0161 cost = 0.098065\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6047, -0.8801, -0.8699,  0.4443,  0.0628],\n",
      "         [-0.1492,  0.0713, -0.0889, -0.1853,  0.3337]],\n",
      "\n",
      "        [[-0.6292, -0.3266,  0.6172,  0.1558, -0.8278],\n",
      "         [ 0.7349,  0.8121, -0.8410,  0.7595,  0.7029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0161 cost = 0.044781\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8859, -0.1168, -0.2833,  0.8723, -0.3966],\n",
      "         [-0.3598,  0.2746,  0.0408, -0.4400,  0.3559]],\n",
      "\n",
      "        [[-0.1866, -0.2242,  0.3585,  0.0358, -0.5302],\n",
      "         [ 0.8006,  0.8439, -0.8910,  0.8046,  0.6815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0162 cost = 0.074599\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3522,  0.1604,  0.0424, -0.2948,  0.2278],\n",
      "         [ 0.6059, -0.8807, -0.8704,  0.4457,  0.0627]],\n",
      "\n",
      "        [[ 0.6631,  0.7080, -0.7785,  0.6652,  0.6581],\n",
      "         [-0.6294, -0.3272,  0.6175,  0.1551, -0.8280]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0162 cost = 0.067539\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1501,  0.0712, -0.0893, -0.1865,  0.3352],\n",
      "         [ 0.6682, -0.6766, -0.7641,  0.6761,  0.0553]],\n",
      "\n",
      "        [[ 0.7367,  0.8134, -0.8421,  0.7613,  0.7049],\n",
      "         [-0.6008, -0.2508,  0.6264,  0.1649, -0.8236]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0162 cost = 0.058355\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3536,  0.1611,  0.0428, -0.2972,  0.2295],\n",
      "         [ 0.8871, -0.1200, -0.2872,  0.8735, -0.4002]],\n",
      "\n",
      "        [[ 0.6642,  0.7087, -0.7791,  0.6662,  0.6594],\n",
      "         [-0.1896, -0.2271,  0.3612,  0.0339, -0.5329]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0163 cost = 0.095074\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6079, -0.8814, -0.8712,  0.4480,  0.0627],\n",
      "         [-0.1522,  0.0715, -0.0895, -0.1890,  0.3366]],\n",
      "\n",
      "        [[-0.6298, -0.3281,  0.6179,  0.1538, -0.8282],\n",
      "         [ 0.7379,  0.8144, -0.8428,  0.7626,  0.7062]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0163 cost = 0.043642\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6705, -0.6787, -0.7658,  0.6784,  0.0550],\n",
      "         [-0.3621,  0.2777,  0.0421, -0.4432,  0.3588]],\n",
      "\n",
      "        [[-0.6018, -0.2523,  0.6272,  0.1632, -0.8245],\n",
      "         [ 0.8026,  0.8450, -0.8918,  0.8065,  0.6845]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0163 cost = 0.033305\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8880, -0.1228, -0.2908,  0.8744, -0.4030],\n",
      "         [-0.3626,  0.2783,  0.0424, -0.4439,  0.3594]],\n",
      "\n",
      "        [[-0.1923, -0.2297,  0.3637,  0.0322, -0.5355],\n",
      "         [ 0.8030,  0.8452, -0.8920,  0.8068,  0.6851]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0164 cost = 0.071686\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3570,  0.1624,  0.0435, -0.3030,  0.2330],\n",
      "         [ 0.6096, -0.8821, -0.8719,  0.4501,  0.0626]],\n",
      "\n",
      "        [[ 0.6662,  0.7099, -0.7800,  0.6680,  0.6616],\n",
      "         [-0.6302, -0.3289,  0.6183,  0.1526, -0.8285]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0164 cost = 0.065747\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6729, -0.6810, -0.7677,  0.6808,  0.0546],\n",
      "         [-0.1564,  0.0718, -0.0899, -0.1940,  0.3392]],\n",
      "\n",
      "        [[-0.6028, -0.2536,  0.6280,  0.1616, -0.8253],\n",
      "         [ 0.7404,  0.8161, -0.8442,  0.7650,  0.7088]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0164 cost = 0.048031\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1577,  0.0719, -0.0899, -0.1956,  0.3399],\n",
      "         [ 0.6110, -0.8826, -0.8724,  0.4517,  0.0626]],\n",
      "\n",
      "        [[ 0.7410,  0.8166, -0.8446,  0.7657,  0.7095],\n",
      "         [-0.6305, -0.3295,  0.6186,  0.1518, -0.8287]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0165 cost = 0.059453\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8891, -0.1266, -0.2954,  0.8755, -0.4068],\n",
      "         [ 0.6742, -0.6824, -0.7687,  0.6820,  0.0545]],\n",
      "\n",
      "        [[-0.1955, -0.2327,  0.3667,  0.0302, -0.5385],\n",
      "         [-0.6034, -0.2545,  0.6286,  0.1606, -0.8258]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0165 cost = 0.081104\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3653,  0.2809,  0.0436, -0.4473,  0.3620],\n",
      "         [-0.3615,  0.1637,  0.0443, -0.3098,  0.2368]],\n",
      "\n",
      "        [[ 0.8050,  0.8462, -0.8928,  0.8086,  0.6880],\n",
      "         [ 0.6684,  0.7113, -0.7812,  0.6700,  0.6641]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0165 cost = 0.063768\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1616,  0.0723, -0.0901, -0.1999,  0.3420],\n",
      "         [-0.3656,  0.2817,  0.0438, -0.4476,  0.3627]],\n",
      "\n",
      "        [[ 0.7430,  0.8181, -0.8459,  0.7677,  0.7116],\n",
      "         [ 0.8055,  0.8465, -0.8930,  0.8091,  0.6887]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0166 cost = 0.052403\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6135, -0.8834, -0.8734,  0.4546,  0.0627],\n",
      "         [-0.3629,  0.1648,  0.0446, -0.3113,  0.2391]],\n",
      "\n",
      "        [[-0.6309, -0.3306,  0.6191,  0.1499, -0.8290],\n",
      "         [ 0.6699,  0.7123, -0.7820,  0.6713,  0.6658]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0166 cost = 0.044922\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6768, -0.6849, -0.7707,  0.6844,  0.0543],\n",
      "         [ 0.8899, -0.1300, -0.2993,  0.8763, -0.4100]],\n",
      "\n",
      "        [[-0.6046, -0.2560,  0.6296,  0.1587, -0.8268],\n",
      "         [-0.1977, -0.2347,  0.3690,  0.0291, -0.5404]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0166 cost = 0.079550\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3644,  0.1659,  0.0449, -0.3136,  0.2416],\n",
      "         [ 0.6775, -0.6856, -0.7712,  0.6851,  0.0542]],\n",
      "\n",
      "        [[ 0.6713,  0.7134, -0.7829,  0.6727,  0.6675],\n",
      "         [-0.6049, -0.2565,  0.6298,  0.1582, -0.8270]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0167 cost = 0.057124\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3667,  0.2856,  0.0448, -0.4488,  0.3655],\n",
      "         [-0.1675,  0.0744, -0.0900, -0.2064,  0.3456]],\n",
      "\n",
      "        [[ 0.8074,  0.8476, -0.8938,  0.8108,  0.6917],\n",
      "         [ 0.7459,  0.8203, -0.8477,  0.7706,  0.7149]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0167 cost = 0.049391\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6157, -0.8840, -0.8742,  0.4569,  0.0627],\n",
      "         [ 0.8906, -0.1323, -0.3020,  0.8769, -0.4121]],\n",
      "\n",
      "        [[-0.6314, -0.3315,  0.6197,  0.1484, -0.8294],\n",
      "         [-0.1994, -0.2361,  0.3707,  0.0284, -0.5419]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0167 cost = 0.071741\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3673,  0.2877,  0.0453, -0.4493,  0.3669],\n",
      "         [ 0.6798, -0.6871, -0.7727,  0.6872,  0.0541]],\n",
      "\n",
      "        [[ 0.8083,  0.8481, -0.8942,  0.8116,  0.6932],\n",
      "         [-0.6059, -0.2577,  0.6306,  0.1566, -0.8278]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0168 cost = 0.037131\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3661,  0.1679,  0.0456, -0.3160,  0.2462],\n",
      "         [-0.1689,  0.0758, -0.0900, -0.2082,  0.3478]],\n",
      "\n",
      "        [[ 0.6738,  0.7153, -0.7844,  0.6750,  0.6705],\n",
      "         [ 0.7478,  0.8217, -0.8488,  0.7725,  0.7171]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0168 cost = 0.073401\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6175, -0.8844, -0.8748,  0.4588,  0.0627],\n",
      "         [ 0.8916, -0.1346, -0.3049,  0.8778, -0.4142]],\n",
      "\n",
      "        [[-0.6318, -0.3322,  0.6201,  0.1472, -0.8297],\n",
      "         [-0.2015, -0.2379,  0.3727,  0.0273, -0.5436]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0168 cost = 0.070588\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6824, -0.6884, -0.7742,  0.6897,  0.0539],\n",
      "         [ 0.6182, -0.8846, -0.8750,  0.4596,  0.0627]],\n",
      "\n",
      "        [[-0.6068, -0.2590,  0.6314,  0.1551, -0.8286],\n",
      "         [-0.6319, -0.3325,  0.6203,  0.1468, -0.8297]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0169 cost = 0.054664\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8924, -0.1363, -0.3071,  0.8786, -0.4158],\n",
      "         [-0.3659,  0.1696,  0.0460, -0.3158,  0.2496]],\n",
      "\n",
      "        [[-0.2032, -0.2395,  0.3742,  0.0263, -0.5452],\n",
      "         [ 0.6756,  0.7166, -0.7854,  0.6767,  0.6726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0169 cost = 0.083242\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3676,  0.2924,  0.0464, -0.4497,  0.3702],\n",
      "         [-0.1671,  0.0776, -0.0902, -0.2071,  0.3504]],\n",
      "\n",
      "        [[ 0.8102,  0.8492, -0.8950,  0.8134,  0.6963],\n",
      "         [ 0.7500,  0.8233, -0.8501,  0.7747,  0.7196]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0169 cost = 0.047664\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3676,  0.2934,  0.0466, -0.4497,  0.3709],\n",
      "         [ 0.6205, -0.8852, -0.8758,  0.4622,  0.0627]],\n",
      "\n",
      "        [[ 0.8106,  0.8495, -0.8952,  0.8137,  0.6970],\n",
      "         [-0.6325, -0.3336,  0.6208,  0.1450, -0.8301]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0170 cost = 0.039335\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1652,  0.0784, -0.0903, -0.2053,  0.3514],\n",
      "         [ 0.6863, -0.6904, -0.7765,  0.6937,  0.0533]],\n",
      "\n",
      "        [[ 0.7511,  0.8241, -0.8507,  0.7758,  0.7209],\n",
      "         [-0.6082, -0.2611,  0.6325,  0.1525, -0.8298]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0170 cost = 0.051499\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3644,  0.1714,  0.0465, -0.3138,  0.2526],\n",
      "         [ 0.8941, -0.1399, -0.3115,  0.8801, -0.4193]],\n",
      "\n",
      "        [[ 0.6776,  0.7180, -0.7865,  0.6786,  0.6752],\n",
      "         [-0.2063, -0.2422,  0.3769,  0.0245, -0.5478]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0170 cost = 0.083756\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3681,  0.2959,  0.0473, -0.4499,  0.3727],\n",
      "         [-0.3645,  0.1718,  0.0467, -0.3139,  0.2535]],\n",
      "\n",
      "        [[ 0.8118,  0.8501, -0.8957,  0.8148,  0.6990],\n",
      "         [ 0.6782,  0.7183, -0.7867,  0.6791,  0.6758]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0171 cost = 0.059274\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6233, -0.8858, -0.8766,  0.4653,  0.0627],\n",
      "         [-0.1644,  0.0799, -0.0903, -0.2048,  0.3533]],\n",
      "\n",
      "        [[-0.6330, -0.3347,  0.6215,  0.1430, -0.8305],\n",
      "         [ 0.7529,  0.8254, -0.8517,  0.7776,  0.7230]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0171 cost = 0.038342\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6895, -0.6919, -0.7784,  0.6969,  0.0530],\n",
      "         [ 0.8951, -0.1421, -0.3144,  0.8810, -0.4212]],\n",
      "\n",
      "        [[-0.6094, -0.2627,  0.6335,  0.1505, -0.8308],\n",
      "         [-0.2081, -0.2438,  0.3787,  0.0237, -0.5494]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0171 cost = 0.072997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1640,  0.0808, -0.0904, -0.2044,  0.3546],\n",
      "         [ 0.6245, -0.8861, -0.8770,  0.4666,  0.0626]],\n",
      "\n",
      "        [[ 0.7541,  0.8263, -0.8525,  0.7788,  0.7244],\n",
      "         [-0.6333, -0.3352,  0.6218,  0.1421, -0.8307]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0172 cost = 0.053583\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8957, -0.1437, -0.3164,  0.8816, -0.4226],\n",
      "         [ 0.6912, -0.6928, -0.7795,  0.6984,  0.0528]],\n",
      "\n",
      "        [[-0.2097, -0.2452,  0.3801,  0.0227, -0.5508],\n",
      "         [-0.6100, -0.2636,  0.6340,  0.1494, -0.8313]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0172 cost = 0.072405\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3636,  0.1743,  0.0473, -0.3110,  0.2583],\n",
      "         [-0.3686,  0.3003,  0.0485, -0.4495,  0.3759]],\n",
      "\n",
      "        [[ 0.6810,  0.7204, -0.7883,  0.6817,  0.6792],\n",
      "         [ 0.8138,  0.8514, -0.8966,  0.8167,  0.7023]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0172 cost = 0.061194\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6265, -0.8866, -0.8777,  0.4689,  0.0626],\n",
      "         [-0.1643,  0.0821, -0.0905, -0.2047,  0.3567]],\n",
      "\n",
      "        [[-0.6336, -0.3361,  0.6223,  0.1406, -0.8310],\n",
      "         [ 0.7558,  0.8275, -0.8535,  0.7806,  0.7263]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0173 cost = 0.037347\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6934, -0.6943, -0.7810,  0.7006,  0.0525],\n",
      "         [-0.3637,  0.1758,  0.0475, -0.3113,  0.2610]],\n",
      "\n",
      "        [[-0.6109, -0.2650,  0.6348,  0.1476, -0.8321],\n",
      "         [ 0.6822,  0.7213, -0.7890,  0.6828,  0.6806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0173 cost = 0.043768\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8968, -0.1471, -0.3204,  0.8826, -0.4254],\n",
      "         [-0.3679,  0.3042,  0.0490, -0.4491,  0.3782]],\n",
      "\n",
      "        [[-0.2124, -0.2476,  0.3826,  0.0212, -0.5531],\n",
      "         [ 0.8150,  0.8521, -0.8971,  0.8178,  0.7043]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0173 cost = 0.061038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3640,  0.1771,  0.0478, -0.3122,  0.2635],\n",
      "         [ 0.6948, -0.6953, -0.7819,  0.7018,  0.0524]],\n",
      "\n",
      "        [[ 0.6834,  0.7222, -0.7897,  0.6840,  0.6821],\n",
      "         [-0.6115, -0.2657,  0.6353,  0.1466, -0.8325]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0174 cost = 0.051196\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3678,  0.3064,  0.0493, -0.4492,  0.3796],\n",
      "         [-0.1665,  0.0845, -0.0908, -0.2079,  0.3599]],\n",
      "\n",
      "        [[ 0.8158,  0.8525, -0.8974,  0.8185,  0.7055],\n",
      "         [ 0.7581,  0.8293, -0.8549,  0.7829,  0.7290]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0174 cost = 0.044311\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6292, -0.8873, -0.8787,  0.4716,  0.0627],\n",
      "         [ 0.8976, -0.1498, -0.3234,  0.8832, -0.4276]],\n",
      "\n",
      "        [[-0.6341, -0.3373,  0.6229,  0.1386, -0.8314],\n",
      "         [-0.2140, -0.2489,  0.3841,  0.0205, -0.5544]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0174 cost = 0.063814\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6970, -0.6967, -0.7834,  0.7038,  0.0522],\n",
      "         [ 0.8979, -0.1507, -0.3245,  0.8835, -0.4284]],\n",
      "\n",
      "        [[-0.6124, -0.2668,  0.6360,  0.1452, -0.8332],\n",
      "         [-0.2148, -0.2496,  0.3847,  0.0200, -0.5551]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0175 cost = 0.069025\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3678,  0.3096,  0.0499, -0.4492,  0.3816],\n",
      "         [-0.1661,  0.0859, -0.0910, -0.2079,  0.3618]],\n",
      "\n",
      "        [[ 0.8168,  0.8531, -0.8979,  0.8195,  0.7073],\n",
      "         [ 0.7597,  0.8304, -0.8557,  0.7844,  0.7307]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0175 cost = 0.043682\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6310, -0.8878, -0.8793,  0.4736,  0.0626],\n",
      "         [-0.3635,  0.1799,  0.0484, -0.3118,  0.2684]],\n",
      "\n",
      "        [[-0.6344, -0.3381,  0.6232,  0.1373, -0.8317],\n",
      "         [ 0.6859,  0.7240, -0.7910,  0.6862,  0.6851]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0175 cost = 0.039047\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6995, -0.6983, -0.7850,  0.7064,  0.0517],\n",
      "         [-0.3676,  0.3116,  0.0503, -0.4491,  0.3829]],\n",
      "\n",
      "        [[-0.6133, -0.2682,  0.6367,  0.1434, -0.8340],\n",
      "         [ 0.8175,  0.8535, -0.8981,  0.8201,  0.7084]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0176 cost = 0.026596\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3628,  0.1809,  0.0487, -0.3111,  0.2701],\n",
      "         [ 0.8994, -0.1546, -0.3289,  0.8849, -0.4318]],\n",
      "\n",
      "        [[ 0.6868,  0.7246, -0.7915,  0.6870,  0.6863],\n",
      "         [-0.2182, -0.2528,  0.3876,  0.0177, -0.5581]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0176 cost = 0.076498\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1641,  0.0875, -0.0913, -0.2061,  0.3639],\n",
      "         [ 0.6327, -0.8883, -0.8799,  0.4754,  0.0626]],\n",
      "\n",
      "        [[ 0.7615,  0.8317, -0.8568,  0.7862,  0.7329],\n",
      "         [-0.6347, -0.3388,  0.6236,  0.1361, -0.8319]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0176 cost = 0.050331\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3630,  0.1818,  0.0490, -0.3116,  0.2721],\n",
      "         [ 0.7018, -0.6999, -0.7865,  0.7086,  0.0514]],\n",
      "\n",
      "        [[ 0.6877,  0.7252, -0.7920,  0.6878,  0.6873],\n",
      "         [-0.6142, -0.2694,  0.6375,  0.1419, -0.8347]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0177 cost = 0.049019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3682,  0.3151,  0.0512, -0.4498,  0.3853],\n",
      "         [-0.1651,  0.0886, -0.0913, -0.2076,  0.3653]],\n",
      "\n",
      "        [[ 0.8188,  0.8543, -0.8987,  0.8213,  0.7106],\n",
      "         [ 0.7626,  0.8324, -0.8574,  0.7872,  0.7340]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0177 cost = 0.042480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6343, -0.8887, -0.8804,  0.4770,  0.0626],\n",
      "         [ 0.9004, -0.1576, -0.3327,  0.8859, -0.4341]],\n",
      "\n",
      "        [[-0.6351, -0.3395,  0.6240,  0.1350, -0.8321],\n",
      "         [-0.2207, -0.2550,  0.3898,  0.0163, -0.5603]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0177 cost = 0.060592\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7039, -0.7009, -0.7877,  0.7105,  0.0513],\n",
      "         [-0.3643,  0.1834,  0.0495, -0.3136,  0.2754]],\n",
      "\n",
      "        [[-0.6150, -0.2705,  0.6382,  0.1404, -0.8354],\n",
      "         [ 0.6894,  0.7265, -0.7930,  0.6894,  0.6893]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0178 cost = 0.040697\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1657,  0.0904, -0.0912, -0.2088,  0.3673],\n",
      "         [-0.3690,  0.3179,  0.0520, -0.4504,  0.3871]],\n",
      "\n",
      "        [[ 0.7641,  0.8335, -0.8582,  0.7887,  0.7358],\n",
      "         [ 0.8199,  0.8550, -0.8992,  0.8223,  0.7124]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0178 cost = 0.043887\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9011, -0.1596, -0.3352,  0.8865, -0.4355],\n",
      "         [ 0.6359, -0.8891, -0.8809,  0.4786,  0.0627]],\n",
      "\n",
      "        [[-0.2226, -0.2566,  0.3915,  0.0153, -0.5619],\n",
      "         [-0.6354, -0.3401,  0.6244,  0.1339, -0.8324]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0178 cost = 0.071912\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1673,  0.0918, -0.0911, -0.2110,  0.3688],\n",
      "         [ 0.9014, -0.1605, -0.3362,  0.8867, -0.4363]],\n",
      "\n",
      "        [[ 0.7652,  0.8343, -0.8589,  0.7898,  0.7370],\n",
      "         [-0.2232, -0.2572,  0.3921,  0.0149, -0.5625]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0179 cost = 0.072990\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7064, -0.7026, -0.7894,  0.7127,  0.0510],\n",
      "         [ 0.6369, -0.8894, -0.8813,  0.4798,  0.0626]],\n",
      "\n",
      "        [[-0.6162, -0.2720,  0.6391,  0.1384, -0.8363],\n",
      "         [-0.6357, -0.3406,  0.6247,  0.1330, -0.8325]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0179 cost = 0.047234\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3695,  0.3217,  0.0529, -0.4512,  0.3893],\n",
      "         [-0.3660,  0.1859,  0.0503, -0.3166,  0.2801]],\n",
      "\n",
      "        [[ 0.8212,  0.8557, -0.8997,  0.8234,  0.7144],\n",
      "         [ 0.6916,  0.7280, -0.7940,  0.6914,  0.6919]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0179 cost = 0.053049\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1685,  0.0931, -0.0913, -0.2123,  0.3706],\n",
      "         [-0.3695,  0.3226,  0.0530, -0.4510,  0.3899]],\n",
      "\n",
      "        [[ 0.7666,  0.8354, -0.8597,  0.7912,  0.7385],\n",
      "         [ 0.8215,  0.8559, -0.8999,  0.8238,  0.7150]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0180 cost = 0.042854\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6385, -0.8900, -0.8819,  0.4815,  0.0626],\n",
      "         [ 0.9023, -0.1642, -0.3403,  0.8876, -0.4390]],\n",
      "\n",
      "        [[-0.6361, -0.3416,  0.6252,  0.1313, -0.8329],\n",
      "         [-0.2261, -0.2601,  0.3945,  0.0129, -0.5650]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0180 cost = 0.058026\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3658,  0.1877,  0.0505, -0.3160,  0.2831],\n",
      "         [ 0.7088, -0.7046, -0.7912,  0.7149,  0.0507]],\n",
      "\n",
      "        [[ 0.6930,  0.7290, -0.7948,  0.6927,  0.6936],\n",
      "         [-0.6173, -0.2739,  0.6400,  0.1362, -0.8372]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0180 cost = 0.046463\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7094, -0.7050, -0.7916,  0.7154,  0.0506],\n",
      "         [ 0.6395, -0.8903, -0.8823,  0.4826,  0.0625]],\n",
      "\n",
      "        [[-0.6176, -0.2743,  0.6402,  0.1356, -0.8374],\n",
      "         [-0.6365, -0.3422,  0.6256,  0.1303, -0.8331]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0181 cost = 0.046146\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1711,  0.0957, -0.0914, -0.2153,  0.3736],\n",
      "         [-0.3695,  0.3267,  0.0537, -0.4506,  0.3923]],\n",
      "\n",
      "        [[ 0.7685,  0.8369, -0.8608,  0.7932,  0.7408],\n",
      "         [ 0.8228,  0.8566, -0.9004,  0.8250,  0.7172]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0181 cost = 0.041983\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3666,  0.1895,  0.0508, -0.3171,  0.2864],\n",
      "         [ 0.9031, -0.1673, -0.3437,  0.8882, -0.4410]],\n",
      "\n",
      "        [[ 0.6944,  0.7300, -0.7955,  0.6940,  0.6953],\n",
      "         [-0.2287, -0.2625,  0.3967,  0.0113, -0.5673]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0181 cost = 0.070495\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1735,  0.0973, -0.0913, -0.2183,  0.3753],\n",
      "         [ 0.7112, -0.7064, -0.7929,  0.7169,  0.0505]],\n",
      "\n",
      "        [[ 0.7695,  0.8376, -0.8614,  0.7942,  0.7420],\n",
      "         [-0.6185, -0.2758,  0.6409,  0.1339, -0.8381]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0182 cost = 0.043151\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9034, -0.1687, -0.3453,  0.8884, -0.4418],\n",
      "         [-0.3699,  0.3300,  0.0542, -0.4508,  0.3941]],\n",
      "\n",
      "        [[-0.2299, -0.2637,  0.3978,  0.0105, -0.5684],\n",
      "         [ 0.8237,  0.8572, -0.9008,  0.8258,  0.7188]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0182 cost = 0.053126\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3687,  0.1913,  0.0512, -0.3201,  0.2898],\n",
      "         [ 0.6419, -0.8911, -0.8832,  0.4850,  0.0625]],\n",
      "\n",
      "        [[ 0.6956,  0.7308, -0.7961,  0.6952,  0.6968],\n",
      "         [-0.6374, -0.3439,  0.6266,  0.1276, -0.8337]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0182 cost = 0.051121\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9037, -0.1705, -0.3472,  0.8887, -0.4430],\n",
      "         [-0.1781,  0.0994, -0.0913, -0.2235,  0.3777]],\n",
      "\n",
      "        [[-0.2313, -0.2650,  0.3989,  0.0096, -0.5695],\n",
      "         [ 0.7709,  0.8387, -0.8622,  0.7957,  0.7436]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0183 cost = 0.063651\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3707,  0.3327,  0.0548, -0.4515,  0.3956],\n",
      "         [ 0.6429, -0.8914, -0.8836,  0.4860,  0.0624]],\n",
      "\n",
      "        [[ 0.8246,  0.8576, -0.9011,  0.8266,  0.7202],\n",
      "         [-0.6377, -0.3445,  0.6269,  0.1266, -0.8339]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0183 cost = 0.032383\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3713,  0.1930,  0.0517, -0.3237,  0.2927],\n",
      "         [ 0.7138, -0.7088, -0.7950,  0.7191,  0.0502]],\n",
      "\n",
      "        [[ 0.6968,  0.7316, -0.7967,  0.6963,  0.6983],\n",
      "         [-0.6198, -0.2780,  0.6421,  0.1311, -0.8392]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0183 cost = 0.044459\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1821,  0.1009, -0.0914, -0.2280,  0.3799],\n",
      "         [ 0.7144, -0.7092, -0.7954,  0.7195,  0.0501]],\n",
      "\n",
      "        [[ 0.7723,  0.8397, -0.8630,  0.7971,  0.7451],\n",
      "         [-0.6201, -0.2784,  0.6423,  0.1306, -0.8394]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0184 cost = 0.041797\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3736,  0.1941,  0.0520, -0.3269,  0.2949],\n",
      "         [ 0.9043, -0.1741, -0.3510,  0.8892, -0.4457]],\n",
      "\n",
      "        [[ 0.6978,  0.7323, -0.7973,  0.6973,  0.6994],\n",
      "         [-0.2335, -0.2672,  0.4009,  0.0081, -0.5715]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0184 cost = 0.067657\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6451, -0.8920, -0.8843,  0.4883,  0.0625],\n",
      "         [-0.3725,  0.3358,  0.0558, -0.4530,  0.3974]],\n",
      "\n",
      "        [[-0.6383, -0.3456,  0.6275,  0.1247, -0.8343],\n",
      "         [ 0.8258,  0.8583, -0.9016,  0.8278,  0.7221]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0184 cost = 0.020589\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1871,  0.1026, -0.0913, -0.2338,  0.3821],\n",
      "         [ 0.7160, -0.7104, -0.7966,  0.7208,  0.0500]],\n",
      "\n",
      "        [[ 0.7737,  0.8407, -0.8638,  0.7986,  0.7466],\n",
      "         [-0.6209, -0.2797,  0.6430,  0.1291, -0.8400]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0185 cost = 0.041125\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9047, -0.1764, -0.3536,  0.8895, -0.4472],\n",
      "         [-0.3736,  0.3373,  0.0563, -0.4539,  0.3983]],\n",
      "\n",
      "        [[-0.2351, -0.2687,  0.4023,  0.0071, -0.5729],\n",
      "         [ 0.8264,  0.8586, -0.9019,  0.8283,  0.7230]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0185 cost = 0.050749\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6465, -0.8924, -0.8849,  0.4897,  0.0625],\n",
      "         [-0.3784,  0.1961,  0.0528, -0.3337,  0.2988]],\n",
      "\n",
      "        [[-0.6387, -0.3463,  0.6279,  0.1234, -0.8345],\n",
      "         [ 0.6995,  0.7334, -0.7983,  0.6989,  0.7014]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0185 cost = 0.033960\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.1965,  0.0529, -0.3354,  0.2997],\n",
      "         [ 0.7175, -0.7117, -0.7978,  0.7221,  0.0498]],\n",
      "\n",
      "        [[ 0.7000,  0.7337, -0.7985,  0.6993,  0.7018],\n",
      "         [-0.6216, -0.2808,  0.6436,  0.1277, -0.8405]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0186 cost = 0.042895\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1934,  0.1045, -0.0911, -0.2412,  0.3848],\n",
      "         [ 0.9051, -0.1790, -0.3563,  0.8898, -0.4491]],\n",
      "\n",
      "        [[ 0.7755,  0.8420, -0.8649,  0.8004,  0.7485],\n",
      "         [-0.2365, -0.2701,  0.4037,  0.0061, -0.5742]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0186 cost = 0.065043\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3755,  0.3399,  0.0573, -0.4560,  0.3998],\n",
      "         [ 0.6478, -0.8928, -0.8854,  0.4909,  0.0624]],\n",
      "\n",
      "        [[ 0.8276,  0.8592, -0.9024,  0.8294,  0.7248],\n",
      "         [-0.6391, -0.3469,  0.6283,  0.1223, -0.8347]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0186 cost = 0.030937\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3831,  0.1979,  0.0536, -0.3411,  0.3026],\n",
      "         [-0.1967,  0.1055, -0.0909, -0.2449,  0.3862]],\n",
      "\n",
      "        [[ 0.7014,  0.7347, -0.7993,  0.7006,  0.7033],\n",
      "         [ 0.7763,  0.8427, -0.8654,  0.8013,  0.7494]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0187 cost = 0.058093\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6488, -0.8930, -0.8857,  0.4919,  0.0625],\n",
      "         [-0.3764,  0.3415,  0.0579, -0.4569,  0.4007]],\n",
      "\n",
      "        [[-0.6393, -0.3473,  0.6286,  0.1216, -0.8349],\n",
      "         [ 0.8283,  0.8596, -0.9027,  0.8300,  0.7258]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0187 cost = 0.019828\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7201, -0.7132, -0.7995,  0.7240,  0.0498],\n",
      "         [ 0.9055, -0.1816, -0.3594,  0.8902, -0.4509]],\n",
      "\n",
      "        [[-0.6228, -0.2824,  0.6446,  0.1257, -0.8414],\n",
      "         [-0.2382, -0.2715,  0.4053,  0.0052, -0.5755]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0187 cost = 0.056440\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.1995,  0.1079, -0.0907, -0.2489,  0.3883],\n",
      "         [ 0.9057, -0.1824, -0.3603,  0.8904, -0.4514]],\n",
      "\n",
      "        [[ 0.7777,  0.8436, -0.8662,  0.8027,  0.7510],\n",
      "         [-0.2388, -0.2721,  0.4057,  0.0048, -0.5760]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0188 cost = 0.063534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6504, -0.8933, -0.8862,  0.4935,  0.0625],\n",
      "         [-0.3863,  0.2002,  0.0543, -0.3463,  0.3069]],\n",
      "\n",
      "        [[-0.6396, -0.3479,  0.6290,  0.1205, -0.8351],\n",
      "         [ 0.7034,  0.7361, -0.8004,  0.7024,  0.7055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0188 cost = 0.032702\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.3445,  0.0589, -0.4583,  0.4025],\n",
      "         [ 0.7218, -0.7143, -0.8006,  0.7255,  0.0496]],\n",
      "\n",
      "        [[ 0.8294,  0.8602, -0.9031,  0.8310,  0.7275],\n",
      "         [-0.6235, -0.2835,  0.6452,  0.1242, -0.8420]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0188 cost = 0.026608\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2020,  0.1094, -0.0906, -0.2522,  0.3901],\n",
      "         [-0.3875,  0.2010,  0.0547, -0.3486,  0.3088]],\n",
      "\n",
      "        [[ 0.7789,  0.8445, -0.8668,  0.8039,  0.7522],\n",
      "         [ 0.7042,  0.7367, -0.8008,  0.7031,  0.7064]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0189 cost = 0.056281\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9064, -0.1852, -0.3636,  0.8909, -0.4533],\n",
      "         [ 0.6517, -0.8937, -0.8866,  0.4949,  0.0625]],\n",
      "\n",
      "        [[-0.2410, -0.2741,  0.4077,  0.0033, -0.5779],\n",
      "         [-0.6400, -0.3485,  0.6294,  0.1194, -0.8353]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0189 cost = 0.062395\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7233, -0.7153, -0.8016,  0.7267,  0.0494],\n",
      "         [-0.3781,  0.3467,  0.0596, -0.4593,  0.4038]],\n",
      "\n",
      "        [[-0.6242, -0.2845,  0.6458,  0.1230, -0.8425],\n",
      "         [ 0.8303,  0.8607, -0.9035,  0.8318,  0.7289]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0189 cost = 0.021300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7238, -0.7157, -0.8020,  0.7271,  0.0493],\n",
      "         [ 0.6527, -0.8940, -0.8870,  0.4959,  0.0624]],\n",
      "\n",
      "        [[-0.6244, -0.2848,  0.6460,  0.1225, -0.8427],\n",
      "         [-0.6402, -0.3489,  0.6296,  0.1185, -0.8355]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0190 cost = 0.041261\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9069, -0.1879, -0.3663,  0.8914, -0.4551],\n",
      "         [-0.3784,  0.3479,  0.0600, -0.4597,  0.4046]],\n",
      "\n",
      "        [[-0.2425, -0.2756,  0.4089,  0.0023, -0.5792],\n",
      "         [ 0.8308,  0.8610, -0.9037,  0.8323,  0.7297]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0190 cost = 0.047294\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2052,  0.1115, -0.0906, -0.2564,  0.3929],\n",
      "         [-0.3893,  0.2032,  0.0553, -0.3523,  0.3127]],\n",
      "\n",
      "        [[ 0.7809,  0.8459, -0.8680,  0.8059,  0.7544],\n",
      "         [ 0.7062,  0.7381, -0.8020,  0.7050,  0.7087]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0190 cost = 0.055244\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2061,  0.1120, -0.0906, -0.2574,  0.3935],\n",
      "         [-0.3786,  0.3490,  0.0603, -0.4601,  0.4053]],\n",
      "\n",
      "        [[ 0.7813,  0.8463, -0.8682,  0.8064,  0.7549],\n",
      "         [ 0.8313,  0.8612, -0.9039,  0.8327,  0.7305]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0191 cost = 0.036330\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9074, -0.1907, -0.3690,  0.8918, -0.4569],\n",
      "         [-0.3904,  0.2044,  0.0555, -0.3540,  0.3147]],\n",
      "\n",
      "        [[-0.2439, -0.2770,  0.4101,  0.0014, -0.5804],\n",
      "         [ 0.7071,  0.7389, -0.8025,  0.7059,  0.7099]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0191 cost = 0.060210\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6548, -0.8948, -0.8878,  0.4982,  0.0623],\n",
      "         [ 0.7263, -0.7182, -0.8040,  0.7292,  0.0488]],\n",
      "\n",
      "        [[-0.6409, -0.3502,  0.6304,  0.1162, -0.8359],\n",
      "         [-0.6256, -0.2866,  0.6470,  0.1201, -0.8436]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0191 cost = 0.026242\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3914,  0.2055,  0.0557, -0.3557,  0.3165],\n",
      "         [ 0.6552, -0.8949, -0.8880,  0.4986,  0.0623]],\n",
      "\n",
      "        [[ 0.7080,  0.7396, -0.8030,  0.7067,  0.7109],\n",
      "         [-0.6410, -0.3505,  0.6306,  0.1158, -0.8360]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0192 cost = 0.045069\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.0770e-01, -1.9337e-01, -3.7141e-01,  8.9207e-01, -4.5863e-01],\n",
      "         [-2.1135e-01,  1.1477e-01, -9.0467e-02, -2.6333e-01,  3.9630e-01]],\n",
      "\n",
      "        [[-2.4513e-01, -2.7811e-01,  4.1122e-01,  7.2773e-04, -5.8135e-01],\n",
      "         [ 7.8315e-01,  8.4768e-01, -8.6926e-01,  8.0820e-01,  7.5700e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0192 cost = 0.055627\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.3532,  0.0611, -0.4611,  0.4076],\n",
      "         [ 0.7276, -0.7195, -0.8050,  0.7303,  0.0486]],\n",
      "\n",
      "        [[ 0.8327,  0.8621, -0.9045,  0.8341,  0.7330],\n",
      "         [-0.6263, -0.2876,  0.6476,  0.1189, -0.8441]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0192 cost = 0.025144\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.3539,  0.0613, -0.4613,  0.4080],\n",
      "         [-0.2135,  0.1157, -0.0905, -0.2657,  0.3975]],\n",
      "\n",
      "        [[ 0.8330,  0.8622, -0.9046,  0.8343,  0.7334],\n",
      "         [ 0.7840,  0.8483, -0.8697,  0.8090,  0.7579]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0193 cost = 0.034617\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6570, -0.8954, -0.8886,  0.5007,  0.0623],\n",
      "         [-0.3937,  0.2075,  0.0562, -0.3594,  0.3199]],\n",
      "\n",
      "        [[-0.6414, -0.3513,  0.6311,  0.1141, -0.8363],\n",
      "         [ 0.7099,  0.7410, -0.8041,  0.7085,  0.7131]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0193 cost = 0.030599\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.0821e-01, -1.9686e-01, -3.7469e-01,  8.9250e-01, -4.6110e-01],\n",
      "         [ 7.2894e-01, -7.2056e-01, -8.0601e-01,  7.3140e-01,  4.8283e-02]],\n",
      "\n",
      "        [[-2.4653e-01, -2.7941e-01,  4.1255e-01, -1.8268e-05, -5.8244e-01],\n",
      "         [-6.2693e-01, -2.8846e-01,  6.4813e-01,  1.1774e-01, -8.4454e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0193 cost = 0.052800\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2150,  0.1173, -0.0904, -0.2679,  0.3992],\n",
      "         [ 0.7295, -0.7210, -0.8064,  0.7319,  0.0481]],\n",
      "\n",
      "        [[ 0.7853,  0.8492, -0.8704,  0.8103,  0.7594],\n",
      "         [-0.6271, -0.2888,  0.6483,  0.1173, -0.8447]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0194 cost = 0.036038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6583, -0.8958, -0.8890,  0.5021,  0.0623],\n",
      "         [-0.3946,  0.2089,  0.0566, -0.3613,  0.3220]],\n",
      "\n",
      "        [[-0.6418, -0.3519,  0.6314,  0.1130, -0.8365],\n",
      "         [ 0.7113,  0.7420, -0.8050,  0.7098,  0.7147]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0194 cost = 0.030203\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.8037e-01,  3.5736e-01,  6.2372e-02, -4.6236e-01,  4.1017e-01],\n",
      "         [ 9.0869e-01, -1.9965e-01, -3.7727e-01,  8.9292e-01, -4.6319e-01]],\n",
      "\n",
      "        [[ 8.3443e-01,  8.6310e-01, -9.0522e-01,  8.3563e-01,  7.3588e-01],\n",
      "         [-2.4757e-01, -2.8037e-01,  4.1351e-01, -6.2871e-04, -5.8323e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0194 cost = 0.046354\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7310, -0.7222, -0.8075,  0.7332,  0.0477],\n",
      "         [-0.2168,  0.1186, -0.0903, -0.2704,  0.4006]],\n",
      "\n",
      "        [[-0.6278, -0.2897,  0.6489,  0.1160, -0.8452],\n",
      "         [ 0.7864,  0.8501, -0.8711,  0.8115,  0.7608]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0195 cost = 0.029407\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9090, -0.2016, -0.3790,  0.8932, -0.4646],\n",
      "         [ 0.6596, -0.8962, -0.8895,  0.5035,  0.0622]],\n",
      "\n",
      "        [[-0.2485, -0.2813,  0.4143, -0.0013, -0.5840],\n",
      "         [-0.6421, -0.3524,  0.6318,  0.1119, -0.8367]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0195 cost = 0.057878\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3954,  0.2103,  0.0571, -0.3638,  0.3242],\n",
      "         [-0.3808,  0.3589,  0.0630, -0.4631,  0.4113]],\n",
      "\n",
      "        [[ 0.7127,  0.7430, -0.8057,  0.7111,  0.7164],\n",
      "         [ 0.8351,  0.8635, -0.9055,  0.8363,  0.7370]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0195 cost = 0.045792\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7326, -0.7237, -0.8087,  0.7346,  0.0471],\n",
      "         [-0.3955,  0.2109,  0.0572, -0.3643,  0.3252]],\n",
      "\n",
      "        [[-0.6285, -0.2907,  0.6494,  0.1147, -0.8457],\n",
      "         [ 0.7131,  0.7433, -0.8060,  0.7115,  0.7169]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0196 cost = 0.031348\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3802,  0.3608,  0.0633, -0.4630,  0.4124],\n",
      "         [ 0.6610, -0.8966, -0.8900,  0.5049,  0.0621]],\n",
      "\n",
      "        [[ 0.8356,  0.8638, -0.9057,  0.8367,  0.7379],\n",
      "         [-0.6424, -0.3531,  0.6322,  0.1106, -0.8370]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0196 cost = 0.027300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9097, -0.2057, -0.3827,  0.8938, -0.4674],\n",
      "         [-0.2188,  0.1206, -0.0905, -0.2735,  0.4032]],\n",
      "\n",
      "        [[-0.2500, -0.2827,  0.4156, -0.0022, -0.5852],\n",
      "         [ 0.7882,  0.8514, -0.8721,  0.8133,  0.7628]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0196 cost = 0.052444\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3959,  0.2125,  0.0576, -0.3661,  0.3278],\n",
      "         [ 0.6620, -0.8969, -0.8903,  0.5060,  0.0621]],\n",
      "\n",
      "        [[ 0.7145,  0.7444, -0.8068,  0.7128,  0.7184],\n",
      "         [-0.6426, -0.3535,  0.6324,  0.1098, -0.8371]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0197 cost = 0.042346\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9100, -0.2078, -0.3845,  0.8941, -0.4688],\n",
      "         [-0.2197,  0.1213, -0.0906, -0.2749,  0.4042]],\n",
      "\n",
      "        [[-0.2507, -0.2833,  0.4162, -0.0026, -0.5857],\n",
      "         [ 0.7889,  0.8519, -0.8725,  0.8140,  0.7636]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0197 cost = 0.051976\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7350, -0.7260, -0.8105,  0.7366,  0.0464],\n",
      "         [-0.3797,  0.3639,  0.0640, -0.4632,  0.4143]],\n",
      "\n",
      "        [[-0.6295, -0.2920,  0.6503,  0.1127, -0.8465],\n",
      "         [ 0.8366,  0.8644, -0.9061,  0.8377,  0.7395]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0197 cost = 0.018913\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7355, -0.7265, -0.8109,  0.7370,  0.0462],\n",
      "         [ 0.6635, -0.8973, -0.8909,  0.5079,  0.0621]],\n",
      "\n",
      "        [[-0.6297, -0.2923,  0.6505,  0.1123, -0.8466],\n",
      "         [-0.6429, -0.3541,  0.6328,  0.1085, -0.8373]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0198 cost = 0.037483\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3797,  0.3650,  0.0643, -0.4634,  0.4151],\n",
      "         [-0.3972,  0.2143,  0.0581, -0.3688,  0.3307]],\n",
      "\n",
      "        [[ 0.8370,  0.8646, -0.9063,  0.8381,  0.7402],\n",
      "         [ 0.7160,  0.7456, -0.8077,  0.7143,  0.7202]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0198 cost = 0.043300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2207,  0.1222, -0.0909, -0.2764,  0.4061],\n",
      "         [ 0.9106, -0.2124, -0.3883,  0.8947, -0.4718]],\n",
      "\n",
      "        [[ 0.7903,  0.8528, -0.8732,  0.8154,  0.7651],\n",
      "         [-0.2521, -0.2847,  0.4175, -0.0034, -0.5869]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0198 cost = 0.054973\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9108, -0.2135, -0.3892,  0.8949, -0.4725],\n",
      "         [ 0.7371, -0.7281, -0.8121,  0.7385,  0.0457]],\n",
      "\n",
      "        [[-0.2525, -0.2851,  0.4179, -0.0037, -0.5872],\n",
      "         [-0.6304, -0.2933,  0.6510,  0.1110, -0.8471]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0199 cost = 0.048978\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3966,  0.2156,  0.0584, -0.3678,  0.3328],\n",
      "         [-0.2208,  0.1224, -0.0912, -0.2763,  0.4070]],\n",
      "\n",
      "        [[ 0.7171,  0.7463, -0.8083,  0.7153,  0.7215],\n",
      "         [ 0.7910,  0.8532, -0.8736,  0.8161,  0.7659]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0199 cost = 0.050619\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6661, -0.8981, -0.8918,  0.5108,  0.0620],\n",
      "         [-0.3792,  0.3678,  0.0648, -0.4628,  0.4171]],\n",
      "\n",
      "        [[-0.6434, -0.3553,  0.6334,  0.1060, -0.8378],\n",
      "         [ 0.8380,  0.8652, -0.9067,  0.8390,  0.7419]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0199 cost = 0.016766\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2203,  0.1236, -0.0914, -0.2761,  0.4082],\n",
      "         [ 0.6666, -0.8982, -0.8919,  0.5114,  0.0620]],\n",
      "\n",
      "        [[ 0.7917,  0.8537, -0.8740,  0.8168,  0.7667],\n",
      "         [-0.6435, -0.3555,  0.6336,  0.1056, -0.8379]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0200 cost = 0.036586\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3789,  0.3695,  0.0651, -0.4625,  0.4181],\n",
      "         [-0.3961,  0.2174,  0.0587, -0.3673,  0.3354]],\n",
      "\n",
      "        [[ 0.8385,  0.8655, -0.9069,  0.8395,  0.7428],\n",
      "         [ 0.7184,  0.7473, -0.8090,  0.7166,  0.7230]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0200 cost = 0.042327\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9118, -0.2186, -0.3940,  0.8958, -0.4756],\n",
      "         [ 0.7397, -0.7302, -0.8140,  0.7409,  0.0451]],\n",
      "\n",
      "        [[-0.2543, -0.2867,  0.4194, -0.0047, -0.5886],\n",
      "         [-0.6314, -0.2948,  0.6519,  0.1090, -0.8479]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0200 cost = 0.047861\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9120, -0.2196, -0.3950,  0.8960, -0.4763],\n",
      "         [ 0.7403, -0.7306, -0.8144,  0.7414,  0.0449]],\n",
      "\n",
      "        [[-0.2546, -0.2870,  0.4197, -0.0049, -0.5889],\n",
      "         [-0.6316, -0.2950,  0.6521,  0.1086, -0.8481]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0201 cost = 0.047637\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2195,  0.1257, -0.0918, -0.2754,  0.4105],\n",
      "         [-0.3948,  0.2193,  0.0589, -0.3649,  0.3380]],\n",
      "\n",
      "        [[ 0.7932,  0.8548, -0.8748,  0.8183,  0.7685],\n",
      "         [ 0.7198,  0.7483, -0.8098,  0.7178,  0.7246]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0201 cost = 0.048926\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3782,  0.3730,  0.0656, -0.4615,  0.4204],\n",
      "         [ 0.6688, -0.8988, -0.8927,  0.5139,  0.0620]],\n",
      "\n",
      "        [[ 0.8396,  0.8662, -0.9074,  0.8405,  0.7447],\n",
      "         [-0.6438, -0.3565,  0.6340,  0.1036, -0.8383]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0201 cost = 0.025466\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3782,  0.3738,  0.0657, -0.4613,  0.4209],\n",
      "         [ 0.9126, -0.2231, -0.3981,  0.8965, -0.4785]],\n",
      "\n",
      "        [[ 0.8399,  0.8663, -0.9075,  0.8408,  0.7452],\n",
      "         [-0.2558, -0.2881,  0.4207, -0.0056, -0.5897]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0202 cost = 0.041889\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3941,  0.2211,  0.0591, -0.3632,  0.3405],\n",
      "         [ 0.7424, -0.7323, -0.8161,  0.7433,  0.0443]],\n",
      "\n",
      "        [[ 0.7210,  0.7493, -0.8106,  0.7189,  0.7261],\n",
      "         [-0.6324, -0.2963,  0.6527,  0.1070, -0.8487]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0202 cost = 0.034370\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6701, -0.8991, -0.8932,  0.5154,  0.0620],\n",
      "         [-0.2197,  0.1277, -0.0920, -0.2754,  0.4129]],\n",
      "\n",
      "        [[-0.6440, -0.3571,  0.6343,  0.1024, -0.8384],\n",
      "         [ 0.7947,  0.8558, -0.8757,  0.8198,  0.7702]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0202 cost = 0.024561\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7434, -0.7329, -0.8168,  0.7442,  0.0441],\n",
      "         [-0.2200,  0.1283, -0.0920, -0.2758,  0.4135]],\n",
      "\n",
      "        [[-0.6328, -0.2968,  0.6531,  0.1062, -0.8489],\n",
      "         [ 0.7951,  0.8561, -0.8759,  0.8202,  0.7707]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0203 cost = 0.026399\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9133, -0.2270, -0.4017,  0.8972, -0.4809],\n",
      "         [-0.3944,  0.2227,  0.0594, -0.3633,  0.3431]],\n",
      "\n",
      "        [[-0.2572, -0.2895,  0.4220, -0.0065, -0.5909],\n",
      "         [ 0.7223,  0.7502, -0.8113,  0.7201,  0.7275]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0203 cost = 0.051330\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6713, -0.8994, -0.8936,  0.5167,  0.0620],\n",
      "         [-0.3786,  0.3773,  0.0666, -0.4611,  0.4232]],\n",
      "\n",
      "        [[-0.6442, -0.3576,  0.6346,  0.1015, -0.8386],\n",
      "         [ 0.8412,  0.8671, -0.9080,  0.8419,  0.7474]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0203 cost = 0.015907\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3948,  0.2236,  0.0596, -0.3637,  0.3445],\n",
      "         [ 0.7447, -0.7341, -0.8178,  0.7453,  0.0437]],\n",
      "\n",
      "        [[ 0.7231,  0.7508, -0.8118,  0.7208,  0.7284],\n",
      "         [-0.6333, -0.2976,  0.6536,  0.1052, -0.8494]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0204 cost = 0.033648\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9137, -0.2299, -0.4042,  0.8976, -0.4827],\n",
      "         [ 0.6721, -0.8996, -0.8939,  0.5175,  0.0620]],\n",
      "\n",
      "        [[-0.2582, -0.2904,  0.4228, -0.0070, -0.5916],\n",
      "         [-0.6443, -0.3579,  0.6347,  0.1009, -0.8387]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0204 cost = 0.051532\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3790,  0.3789,  0.0671, -0.4615,  0.4244],\n",
      "         [-0.2221,  0.1301, -0.0922, -0.2781,  0.4160]],\n",
      "\n",
      "        [[ 0.8419,  0.8675, -0.9083,  0.8426,  0.7486],\n",
      "         [ 0.7968,  0.8573, -0.8768,  0.8219,  0.7726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0204 cost = 0.030033\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9141, -0.2321, -0.4061,  0.8979, -0.4841],\n",
      "         [ 0.6731, -0.8999, -0.8942,  0.5186,  0.0620]],\n",
      "\n",
      "        [[-0.2589, -0.2911,  0.4234, -0.0075, -0.5922],\n",
      "         [-0.6445, -0.3583,  0.6350,  0.1001, -0.8388]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0205 cost = 0.051074\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2214,  0.1307, -0.0924, -0.2776,  0.4167],\n",
      "         [-0.3791,  0.3800,  0.0674, -0.4615,  0.4252]],\n",
      "\n",
      "        [[ 0.7974,  0.8577, -0.8771,  0.8225,  0.7733],\n",
      "         [ 0.8423,  0.8677, -0.9085,  0.8430,  0.7493]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0205 cost = 0.030307\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7472, -0.7360, -0.8197,  0.7476,  0.0428],\n",
      "         [-0.3949,  0.2258,  0.0601, -0.3639,  0.3472]],\n",
      "\n",
      "        [[-0.6342, -0.2991,  0.6544,  0.1033, -0.8500],\n",
      "         [ 0.7248,  0.7519, -0.8127,  0.7223,  0.7304]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0205 cost = 0.027602\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.3815,  0.0677, -0.4615,  0.4260],\n",
      "         [-0.3951,  0.2263,  0.0602, -0.3642,  0.3478]],\n",
      "\n",
      "        [[ 0.8428,  0.8680, -0.9087,  0.8434,  0.7501],\n",
      "         [ 0.7252,  0.7522, -0.8129,  0.7227,  0.7308]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0206 cost = 0.039693\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6748, -0.9004, -0.8948,  0.5205,  0.0619],\n",
      "         [ 0.7480, -0.7366, -0.8203,  0.7483,  0.0426]],\n",
      "\n",
      "        [[-0.6449, -0.3591,  0.6354,  0.0985, -0.8391],\n",
      "         [-0.6346, -0.2995,  0.6547,  0.1026, -0.8503]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0206 cost = 0.021735\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2220,  0.1328, -0.0925, -0.2785,  0.4188],\n",
      "         [ 0.9149, -0.2371, -0.4105,  0.8987, -0.4874]],\n",
      "\n",
      "        [[ 0.7988,  0.8587, -0.8779,  0.8238,  0.7750],\n",
      "         [-0.2603, -0.2925,  0.4247, -0.0083, -0.5933]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0206 cost = 0.049713\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2223,  0.1334, -0.0925, -0.2788,  0.4194],\n",
      "         [-0.3947,  0.2280,  0.0604, -0.3634,  0.3504]],\n",
      "\n",
      "        [[ 0.7991,  0.8589, -0.8781,  0.8242,  0.7754],\n",
      "         [ 0.7265,  0.7533, -0.8138,  0.7240,  0.7324]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0207 cost = 0.046022\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.3847,  0.0683, -0.4610,  0.4281],\n",
      "         [ 0.7492, -0.7375, -0.8212,  0.7492,  0.0423]],\n",
      "\n",
      "        [[ 0.8439,  0.8686, -0.9091,  0.8444,  0.7521],\n",
      "         [-0.6351, -0.3003,  0.6551,  0.1017, -0.8507]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0207 cost = 0.020553\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9152, -0.2392, -0.4125,  0.8989, -0.4884],\n",
      "         [ 0.6760, -0.9007, -0.8953,  0.5216,  0.0619]],\n",
      "\n",
      "        [[-0.2610, -0.2932,  0.4255, -0.0085, -0.5938],\n",
      "         [-0.6452, -0.3598,  0.6358,  0.0972, -0.8394]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0207 cost = 0.049568\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9153, -0.2402, -0.4134,  0.8990, -0.4890],\n",
      "         [-0.3793,  0.3862,  0.0686, -0.4609,  0.4290]],\n",
      "\n",
      "        [[-0.2614, -0.2935,  0.4258, -0.0087, -0.5940],\n",
      "         [ 0.8444,  0.8690, -0.9094,  0.8449,  0.7530]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0208 cost = 0.037128\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2246,  0.1360, -0.0925, -0.2812,  0.4218],\n",
      "         [-0.3950,  0.2303,  0.0607, -0.3633,  0.3537]],\n",
      "\n",
      "        [[ 0.8006,  0.8600, -0.8789,  0.8256,  0.7771],\n",
      "         [ 0.7281,  0.7545, -0.8147,  0.7255,  0.7344]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0208 cost = 0.045250\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6769, -0.9011, -0.8957,  0.5227,  0.0619],\n",
      "         [ 0.7506, -0.7388, -0.8225,  0.7504,  0.0419]],\n",
      "\n",
      "        [[-0.6454, -0.3603,  0.6361,  0.0960, -0.8395],\n",
      "         [-0.6358, -0.3013,  0.6557,  0.1003, -0.8512]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0208 cost = 0.021180\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9157, -0.2434, -0.4160,  0.8994, -0.4908],\n",
      "         [-0.3952,  0.2315,  0.0608, -0.3634,  0.3554]],\n",
      "\n",
      "        [[-0.2624, -0.2947,  0.4267, -0.0095, -0.5949],\n",
      "         [ 0.7289,  0.7552, -0.8152,  0.7262,  0.7353]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0209 cost = 0.047839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2266,  0.1377, -0.0925, -0.2830,  0.4236],\n",
      "         [-0.3796,  0.3890,  0.0691, -0.4608,  0.4307]],\n",
      "\n",
      "        [[ 0.8016,  0.8608, -0.8795,  0.8267,  0.7784],\n",
      "         [ 0.8453,  0.8695, -0.9097,  0.8457,  0.7547]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0209 cost = 0.028716\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6777, -0.9014, -0.8961,  0.5235,  0.0618],\n",
      "         [ 0.7516, -0.7398, -0.8234,  0.7512,  0.0416]],\n",
      "\n",
      "        [[-0.6456, -0.3609,  0.6364,  0.0950, -0.8397],\n",
      "         [-0.6363, -0.3021,  0.6562,  0.0992, -0.8515]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0209 cost = 0.020950\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6780, -0.9015, -0.8962,  0.5238,  0.0618],\n",
      "         [ 0.7519, -0.7401, -0.8236,  0.7515,  0.0415]],\n",
      "\n",
      "        [[-0.6457, -0.3610,  0.6365,  0.0947, -0.8397],\n",
      "         [-0.6365, -0.3024,  0.6563,  0.0988, -0.8516]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0210 cost = 0.020874\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9161, -0.2471, -0.4190,  0.8997, -0.4929],\n",
      "         [-0.3796,  0.3913,  0.0695, -0.4608,  0.4320]],\n",
      "\n",
      "        [[-0.2638, -0.2960,  0.4278, -0.0103, -0.5959],\n",
      "         [ 0.8460,  0.8699, -0.9100,  0.8463,  0.7559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0210 cost = 0.036002\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2296,  0.1402, -0.0924, -0.2862,  0.4258],\n",
      "         [-0.3960,  0.2342,  0.0611, -0.3645,  0.3589]],\n",
      "\n",
      "        [[ 0.8029,  0.8618, -0.8802,  0.8279,  0.7799],\n",
      "         [ 0.7306,  0.7564, -0.8161,  0.7276,  0.7373]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0210 cost = 0.044043\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2307,  0.1410, -0.0924, -0.2873,  0.4264],\n",
      "         [ 0.9163, -0.2490, -0.4205,  0.8999, -0.4940]],\n",
      "\n",
      "        [[ 0.8033,  0.8620, -0.8804,  0.8283,  0.7804],\n",
      "         [-0.2645, -0.2968,  0.4284, -0.0109, -0.5965]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0211 cost = 0.047105\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3967,  0.2354,  0.0612, -0.3655,  0.3606],\n",
      "         [ 0.7532, -0.7415, -0.8247,  0.7525,  0.0410]],\n",
      "\n",
      "        [[ 0.7312,  0.7569, -0.8165,  0.7282,  0.7381],\n",
      "         [-0.6372, -0.3036,  0.6569,  0.0972, -0.8522]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0211 cost = 0.030814\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.3943,  0.0700, -0.4611,  0.4336],\n",
      "         [ 0.6794, -0.9020, -0.8968,  0.5252,  0.0618]],\n",
      "\n",
      "        [[ 0.8468,  0.8704, -0.9103,  0.8470,  0.7573],\n",
      "         [-0.6461, -0.3620,  0.6369,  0.0931, -0.8399]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0211 cost = 0.022630\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9165, -0.2515, -0.4226,  0.9000, -0.4953],\n",
      "         [ 0.7538, -0.7420, -0.8252,  0.7529,  0.0409]],\n",
      "\n",
      "        [[-0.2656, -0.2979,  0.4293, -0.0117, -0.5974],\n",
      "         [-0.6376, -0.3041,  0.6572,  0.0965, -0.8524]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0212 cost = 0.041334\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3805,  0.3956,  0.0703, -0.4613,  0.4343],\n",
      "         [ 0.6801, -0.9022, -0.8970,  0.5259,  0.0618]],\n",
      "\n",
      "        [[ 0.8472,  0.8706, -0.9105,  0.8474,  0.7580],\n",
      "         [-0.6462, -0.3623,  0.6371,  0.0924, -0.8400]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0212 cost = 0.022462\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2359,  0.1442, -0.0921, -0.2925,  0.4292],\n",
      "         [-0.3985,  0.2373,  0.0616, -0.3678,  0.3634]],\n",
      "\n",
      "        [[ 0.8049,  0.8632, -0.8812,  0.8298,  0.7822],\n",
      "         [ 0.7325,  0.7577, -0.8171,  0.7293,  0.7395]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0212 cost = 0.043046\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3991,  0.2379,  0.0617, -0.3685,  0.3643],\n",
      "         [ 0.6809, -0.9024, -0.8973,  0.5268,  0.0618]],\n",
      "\n",
      "        [[ 0.7329,  0.7580, -0.8174,  0.7296,  0.7400],\n",
      "         [-0.6464, -0.3627,  0.6373,  0.0917, -0.8401]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0213 cost = 0.035224\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3812,  0.3974,  0.0708, -0.4617,  0.4354],\n",
      "         [ 0.9169, -0.2548, -0.4254,  0.9003, -0.4971]],\n",
      "\n",
      "        [[ 0.8479,  0.8710, -0.9108,  0.8480,  0.7592],\n",
      "         [-0.2669, -0.2991,  0.4303, -0.0125, -0.5983]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0213 cost = 0.036396\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7556, -0.7433, -0.8265,  0.7541,  0.0405],\n",
      "         [-0.2393,  0.1465, -0.0917, -0.2961,  0.4311]],\n",
      "\n",
      "        [[-0.6384, -0.3056,  0.6580,  0.0946, -0.8530],\n",
      "         [ 0.8059,  0.8640, -0.8818,  0.8309,  0.7834]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0213 cost = 0.022936\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4013,  0.2395,  0.0621, -0.3711,  0.3670],\n",
      "         [ 0.6821, -0.9027, -0.8977,  0.5281,  0.0618]],\n",
      "\n",
      "        [[ 0.7341,  0.7589, -0.8181,  0.7307,  0.7414],\n",
      "         [-0.6466, -0.3632,  0.6376,  0.0906, -0.8403]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0214 cost = 0.034770\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7562, -0.7438, -0.8269,  0.7546,  0.0404],\n",
      "         [-0.2416,  0.1478, -0.0914, -0.2985,  0.4323]],\n",
      "\n",
      "        [[-0.6388, -0.3061,  0.6583,  0.0939, -0.8533],\n",
      "         [ 0.8066,  0.8644, -0.8822,  0.8315,  0.7841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0214 cost = 0.022732\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3825,  0.3997,  0.0717, -0.4624,  0.4367],\n",
      "         [ 0.9170, -0.2574, -0.4277,  0.9004, -0.4984]],\n",
      "\n",
      "        [[ 0.8488,  0.8715, -0.9112,  0.8488,  0.7607],\n",
      "         [-0.2679, -0.3001,  0.4313, -0.0130, -0.5991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0214 cost = 0.035897\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6834, -0.9029, -0.8981,  0.5296,  0.0618],\n",
      "         [ 0.7569, -0.7444, -0.8274,  0.7550,  0.0404]],\n",
      "\n",
      "        [[-0.6468, -0.3636,  0.6379,  0.0895, -0.8404],\n",
      "         [-0.6391, -0.3067,  0.6586,  0.0932, -0.8535]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0215 cost = 0.019730\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4046,  0.2410,  0.0627, -0.3752,  0.3703],\n",
      "         [-0.2448,  0.1494, -0.0910, -0.3018,  0.4339]],\n",
      "\n",
      "        [[ 0.7356,  0.7599, -0.8190,  0.7321,  0.7430],\n",
      "         [ 0.8075,  0.8651, -0.8827,  0.8324,  0.7851]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0215 cost = 0.042477\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9173, -0.2593, -0.4294,  0.9006, -0.4993],\n",
      "         [-0.3832,  0.4012,  0.0723, -0.4628,  0.4377]],\n",
      "\n",
      "        [[-0.2688, -0.3010,  0.4321, -0.0136, -0.5998],\n",
      "         [ 0.8494,  0.8718, -0.9115,  0.8494,  0.7617]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0215 cost = 0.033842\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9174, -0.2600, -0.4301,  0.9007, -0.4997],\n",
      "         [-0.3834,  0.4019,  0.0725, -0.4629,  0.4381]],\n",
      "\n",
      "        [[-0.2691, -0.3013,  0.4324, -0.0138, -0.6000],\n",
      "         [ 0.8496,  0.8719, -0.9115,  0.8496,  0.7620]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0216 cost = 0.033706\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6852, -0.9033, -0.8986,  0.5315,  0.0619],\n",
      "         [-0.2459,  0.1516, -0.0909, -0.3034,  0.4354]],\n",
      "\n",
      "        [[-0.6470, -0.3643,  0.6382,  0.0881, -0.8407],\n",
      "         [ 0.8083,  0.8656, -0.8831,  0.8333,  0.7860]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0216 cost = 0.020721\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7589, -0.7457, -0.8287,  0.7566,  0.0399],\n",
      "         [-0.4059,  0.2429,  0.0631, -0.3771,  0.3732]],\n",
      "\n",
      "        [[-0.6399, -0.3081,  0.6594,  0.0915, -0.8541],\n",
      "         [ 0.7369,  0.7608, -0.8197,  0.7333,  0.7444]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0216 cost = 0.024115\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6860, -0.9035, -0.8988,  0.5324,  0.0619],\n",
      "         [ 0.9178, -0.2628, -0.4326,  0.9011, -0.5014]],\n",
      "\n",
      "        [[-0.6471, -0.3646,  0.6384,  0.0875, -0.8408],\n",
      "         [-0.2702, -0.3025,  0.4333, -0.0147, -0.6010]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0217 cost = 0.036076\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4063,  0.2437,  0.0634, -0.3779,  0.3743],\n",
      "         [-0.3839,  0.4042,  0.0731, -0.4634,  0.4394]],\n",
      "\n",
      "        [[ 0.7374,  0.7611, -0.8200,  0.7337,  0.7449],\n",
      "         [ 0.8503,  0.8723, -0.9118,  0.8502,  0.7632]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0217 cost = 0.035688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7600, -0.7466, -0.8295,  0.7575,  0.0396],\n",
      "         [-0.2471,  0.1538, -0.0909, -0.3052,  0.4372]],\n",
      "\n",
      "        [[-0.6405, -0.3088,  0.6598,  0.0904, -0.8545],\n",
      "         [ 0.8093,  0.8663, -0.8837,  0.8342,  0.7871]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0217 cost = 0.021839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3833,  0.4059,  0.0734, -0.4631,  0.4404],\n",
      "         [ 0.7603, -0.7469, -0.8297,  0.7577,  0.0395]],\n",
      "\n",
      "        [[ 0.8506,  0.8725, -0.9120,  0.8505,  0.7638],\n",
      "         [-0.6406, -0.3091,  0.6600,  0.0901, -0.8546]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0218 cost = 0.018053\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9182, -0.2661, -0.4353,  0.9015, -0.5031],\n",
      "         [-0.2482,  0.1553, -0.0908, -0.3065,  0.4383]],\n",
      "\n",
      "        [[-0.2716, -0.3038,  0.4344, -0.0156, -0.6021],\n",
      "         [ 0.8098,  0.8666, -0.8839,  0.8347,  0.7877]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0218 cost = 0.039730\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6878, -0.9039, -0.8994,  0.5342,  0.0618],\n",
      "         [-0.4071,  0.2460,  0.0638, -0.3799,  0.3776]],\n",
      "\n",
      "        [[-0.6474, -0.3653,  0.6387,  0.0862, -0.8410],\n",
      "         [ 0.7387,  0.7621, -0.8207,  0.7349,  0.7464]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0218 cost = 0.022626\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9184, -0.2679, -0.4367,  0.9017, -0.5041],\n",
      "         [-0.4073,  0.2465,  0.0640, -0.3805,  0.3783]],\n",
      "\n",
      "        [[-0.2723, -0.3044,  0.4349, -0.0161, -0.6027],\n",
      "         [ 0.7390,  0.7623, -0.8209,  0.7352,  0.7467]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0219 cost = 0.042597\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6884, -0.9041, -0.8996,  0.5349,  0.0618],\n",
      "         [-0.3828,  0.4087,  0.0740, -0.4631,  0.4419]],\n",
      "\n",
      "        [[-0.6475, -0.3656,  0.6389,  0.0857, -0.8411],\n",
      "         [ 0.8513,  0.8729, -0.9122,  0.8511,  0.7650]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0219 cost = 0.013192\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2499,  0.1572, -0.0908, -0.3088,  0.4399],\n",
      "         [ 0.7620, -0.7486, -0.8310,  0.7592,  0.0388]],\n",
      "\n",
      "        [[ 0.8108,  0.8673, -0.8844,  0.8357,  0.7888],\n",
      "         [-0.6414, -0.3101,  0.6607,  0.0886, -0.8552]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0219 cost = 0.026107\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2506,  0.1577, -0.0908, -0.3096,  0.4404],\n",
      "         [ 0.9187, -0.2708, -0.4389,  0.9020, -0.5057]],\n",
      "\n",
      "        [[ 0.8111,  0.8675, -0.8846,  0.8359,  0.7891],\n",
      "         [-0.2733, -0.3053,  0.4357, -0.0168, -0.6034]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0220 cost = 0.042314\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3827,  0.4105,  0.0745, -0.4634,  0.4429],\n",
      "         [ 0.6893, -0.9044, -0.8999,  0.5358,  0.0618]],\n",
      "\n",
      "        [[ 0.8518,  0.8732, -0.9124,  0.8516,  0.7658],\n",
      "         [-0.6476, -0.3659,  0.6391,  0.0850, -0.8412]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0220 cost = 0.020551\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4087,  0.2486,  0.0646, -0.3840,  0.3812],\n",
      "         [ 0.7629, -0.7496, -0.8318,  0.7599,  0.0384]],\n",
      "\n",
      "        [[ 0.7403,  0.7633, -0.8216,  0.7363,  0.7482],\n",
      "         [-0.6419, -0.3107,  0.6611,  0.0876, -0.8555]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0220 cost = 0.027590\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6900, -0.9046, -0.9002,  0.5365,  0.0617],\n",
      "         [-0.4093,  0.2490,  0.0648, -0.3850,  0.3819]],\n",
      "\n",
      "        [[-0.6477, -0.3661,  0.6392,  0.0844, -0.8413],\n",
      "         [ 0.7407,  0.7635, -0.8217,  0.7366,  0.7485]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0221 cost = 0.022092\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9190, -0.2739, -0.4413,  0.9022, -0.5074],\n",
      "         [-0.2540,  0.1597, -0.0904, -0.3135,  0.4420]],\n",
      "\n",
      "        [[-0.2747, -0.3065,  0.4367, -0.0178, -0.6045],\n",
      "         [ 0.8121,  0.8682, -0.8851,  0.8370,  0.7902]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0221 cost = 0.038337\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3832,  0.4124,  0.0753, -0.4642,  0.4441],\n",
      "         [ 0.7639, -0.7504, -0.8324,  0.7606,  0.0381]],\n",
      "\n",
      "        [[ 0.8525,  0.8736, -0.9127,  0.8522,  0.7669],\n",
      "         [-0.6424, -0.3113,  0.6615,  0.0867, -0.8558]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0221 cost = 0.017324\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3833,  0.4128,  0.0755, -0.4645,  0.4443],\n",
      "         [-0.4109,  0.2500,  0.0652, -0.3880,  0.3836]],\n",
      "\n",
      "        [[ 0.8527,  0.8737, -0.9128,  0.8524,  0.7672],\n",
      "         [ 0.7416,  0.7641, -0.8222,  0.7374,  0.7494]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0222 cost = 0.033900\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6912, -0.9049, -0.9006,  0.5378,  0.0617],\n",
      "         [ 0.9193, -0.2764, -0.4432,  0.9025, -0.5087]],\n",
      "\n",
      "        [[-0.6479, -0.3665,  0.6396,  0.0835, -0.8415],\n",
      "         [-0.2755, -0.3073,  0.4374, -0.0184, -0.6051]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0222 cost = 0.033980\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7649, -0.7512, -0.8331,  0.7614,  0.0377],\n",
      "         [-0.2561,  0.1613, -0.0901, -0.3160,  0.4436]],\n",
      "\n",
      "        [[-0.6428, -0.3119,  0.6619,  0.0858, -0.8561],\n",
      "         [ 0.8131,  0.8689, -0.8857,  0.8379,  0.7913]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0222 cost = 0.020615\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4111,  0.2513,  0.0656, -0.3884,  0.3855],\n",
      "         [ 0.9195, -0.2781, -0.4445,  0.9027, -0.5096]],\n",
      "\n",
      "        [[ 0.7425,  0.7648, -0.8228,  0.7383,  0.7505],\n",
      "         [-0.2762, -0.3079,  0.4380, -0.0188, -0.6056]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0223 cost = 0.041560\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3836,  0.4147,  0.0762, -0.4646,  0.4457],\n",
      "         [ 0.6922, -0.9052, -0.9009,  0.5388,  0.0617]],\n",
      "\n",
      "        [[ 0.8534,  0.8741, -0.9131,  0.8530,  0.7684],\n",
      "         [-0.6481, -0.3669,  0.6398,  0.0827, -0.8416]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0223 cost = 0.019922\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2573,  0.1622, -0.0900, -0.3172,  0.4447],\n",
      "         [ 0.7660, -0.7522, -0.8339,  0.7624,  0.0373]],\n",
      "\n",
      "        [[ 0.8139,  0.8693, -0.8860,  0.8386,  0.7921],\n",
      "         [-0.6433, -0.3126,  0.6623,  0.0849, -0.8565]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0223 cost = 0.024965\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9199, -0.2805, -0.4465,  0.9030, -0.5108],\n",
      "         [-0.4120,  0.2523,  0.0659, -0.3895,  0.3873]],\n",
      "\n",
      "        [[-0.2773, -0.3090,  0.4388, -0.0197, -0.6065],\n",
      "         [ 0.7433,  0.7654, -0.8232,  0.7390,  0.7513]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0224 cost = 0.040264\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2585,  0.1629, -0.0898, -0.3185,  0.4455],\n",
      "         [ 0.7667, -0.7528, -0.8344,  0.7629,  0.0371]],\n",
      "\n",
      "        [[ 0.8144,  0.8697, -0.8863,  0.8391,  0.7926],\n",
      "         [-0.6436, -0.3130,  0.6626,  0.0842, -0.8567]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0224 cost = 0.024779\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.6935, -0.9055, -0.9013,  0.5402,  0.0616],\n",
      "         [-0.3845,  0.4162,  0.0770, -0.4651,  0.4468]],\n",
      "\n",
      "        [[-0.6483, -0.3674,  0.6401,  0.0816, -0.8417],\n",
      "         [ 0.8542,  0.8745, -0.9134,  0.8536,  0.7695]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0224 cost = 0.012457\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3847,  0.4166,  0.0772, -0.4653,  0.4471],\n",
      "         [-0.4133,  0.2532,  0.0663, -0.3914,  0.3889]],\n",
      "\n",
      "        [[ 0.8543,  0.8746, -0.9135,  0.8538,  0.7698],\n",
      "         [ 0.7442,  0.7659, -0.8236,  0.7396,  0.7522]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0225 cost = 0.033036\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7676, -0.7536, -0.8350,  0.7637,  0.0368],\n",
      "         [ 0.6940, -0.9056, -0.9015,  0.5407,  0.0616]],\n",
      "\n",
      "        [[-0.6441, -0.3136,  0.6630,  0.0833, -0.8570],\n",
      "         [-0.6484, -0.3676,  0.6402,  0.0812, -0.8418]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0225 cost = 0.028306\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9204, -0.2843, -0.4495,  0.9035, -0.5126],\n",
      "         [-0.2609,  0.1646, -0.0894, -0.3209,  0.4471]],\n",
      "\n",
      "        [[-0.2787, -0.3103,  0.4400, -0.0207, -0.6076],\n",
      "         [ 0.8154,  0.8704, -0.8869,  0.8401,  0.7938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0225 cost = 0.036504\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4134,  0.2544,  0.0666, -0.3914,  0.3907],\n",
      "         [ 0.7684, -0.7542, -0.8356,  0.7643,  0.0365]],\n",
      "\n",
      "        [[ 0.7452,  0.7666, -0.8242,  0.7406,  0.7532],\n",
      "         [-0.6444, -0.3141,  0.6633,  0.0826, -0.8572]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0226 cost = 0.026003\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2615,  0.1651, -0.0894, -0.3215,  0.4479],\n",
      "         [ 0.6950, -0.9059, -0.9018,  0.5419,  0.0615]],\n",
      "\n",
      "        [[ 0.8159,  0.8707, -0.8871,  0.8406,  0.7943],\n",
      "         [-0.6486, -0.3681,  0.6405,  0.0801, -0.8420]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0226 cost = 0.026996\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9206, -0.2866, -0.4513,  0.9037, -0.5138],\n",
      "         [-0.3853,  0.4186,  0.0780, -0.4653,  0.4486]],\n",
      "\n",
      "        [[-0.2795, -0.3110,  0.4406, -0.0211, -0.6082],\n",
      "         [ 0.8553,  0.8751, -0.9139,  0.8547,  0.7714]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0226 cost = 0.029673\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3855,  0.4190,  0.0781, -0.4654,  0.4489],\n",
      "         [ 0.6958, -0.9061, -0.9021,  0.5426,  0.0615]],\n",
      "\n",
      "        [[ 0.8555,  0.8752, -0.9140,  0.8549,  0.7717],\n",
      "         [-0.6487, -0.3684,  0.6407,  0.0794, -0.8421]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0227 cost = 0.019159\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7698, -0.7554, -0.8365,  0.7655,  0.0360],\n",
      "         [ 0.9209, -0.2884, -0.4526,  0.9039, -0.5147]],\n",
      "\n",
      "        [[-0.6451, -0.3150,  0.6638,  0.0813, -0.8576],\n",
      "         [-0.2800, -0.3116,  0.4411, -0.0216, -0.6086]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0227 cost = 0.034496\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4146,  0.2559,  0.0671, -0.3927,  0.3931],\n",
      "         [-0.2632,  0.1662, -0.0892, -0.3233,  0.4493]],\n",
      "\n",
      "        [[ 0.7465,  0.7675, -0.8249,  0.7417,  0.7547],\n",
      "         [ 0.8169,  0.8713, -0.8876,  0.8416,  0.7953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0227 cost = 0.037945\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9212, -0.2903, -0.4540,  0.9042, -0.5158],\n",
      "         [ 0.6971, -0.9064, -0.9025,  0.5442,  0.0615]],\n",
      "\n",
      "        [[-0.2807, -0.3123,  0.4416, -0.0222, -0.6092],\n",
      "         [-0.6489, -0.3689,  0.6410,  0.0783, -0.8423]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0228 cost = 0.039866\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2628,  0.1671, -0.0893, -0.3232,  0.4500],\n",
      "         [-0.3857,  0.4206,  0.0787, -0.4654,  0.4499]],\n",
      "\n",
      "        [[ 0.8173,  0.8716, -0.8879,  0.8420,  0.7958],\n",
      "         [ 0.8561,  0.8755, -0.9142,  0.8554,  0.7727]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0228 cost = 0.022947\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7715, -0.7565, -0.8377,  0.7669,  0.0353],\n",
      "         [-0.4144,  0.2573,  0.0673, -0.3927,  0.3946]],\n",
      "\n",
      "        [[-0.6457, -0.3161,  0.6644,  0.0797, -0.8581],\n",
      "         [ 0.7472,  0.7680, -0.8253,  0.7424,  0.7555]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0228 cost = 0.021131\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2634,  0.1684, -0.0893, -0.3241,  0.4509],\n",
      "         [ 0.9215, -0.2930, -0.4562,  0.9046, -0.5173]],\n",
      "\n",
      "        [[ 0.8178,  0.8719, -0.8881,  0.8425,  0.7964],\n",
      "         [-0.2817, -0.3133,  0.4423, -0.0230, -0.6100]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0229 cost = 0.038243\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7722, -0.7571, -0.8382,  0.7674,  0.0350],\n",
      "         [-0.3857,  0.4225,  0.0791, -0.4654,  0.4509]],\n",
      "\n",
      "        [[-0.6460, -0.3166,  0.6646,  0.0790, -0.8583],\n",
      "         [ 0.8566,  0.8758, -0.9144,  0.8559,  0.7736]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0229 cost = 0.012543\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4150,  0.2586,  0.0676, -0.3938,  0.3963],\n",
      "         [ 0.6991, -0.9069, -0.9031,  0.5463,  0.0614]],\n",
      "\n",
      "        [[ 0.7480,  0.7685, -0.8257,  0.7431,  0.7563],\n",
      "         [-0.6493, -0.3698,  0.6414,  0.0764, -0.8426]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0229 cost = 0.029454\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4154,  0.2590,  0.0678, -0.3945,  0.3970],\n",
      "         [ 0.6995, -0.9070, -0.9032,  0.5467,  0.0614]],\n",
      "\n",
      "        [[ 0.7482,  0.7687, -0.8258,  0.7434,  0.7566],\n",
      "         [-0.6494, -0.3700,  0.6415,  0.0760, -0.8426]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0230 cost = 0.029340\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7732, -0.7578, -0.8388,  0.7682,  0.0348],\n",
      "         [ 0.9219, -0.2960, -0.4586,  0.9048, -0.5188]],\n",
      "\n",
      "        [[-0.6465, -0.3173,  0.6651,  0.0780, -0.8587],\n",
      "         [-0.2828, -0.3144,  0.4433, -0.0238, -0.6109]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0230 cost = 0.033293\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2662,  0.1715, -0.0888, -0.3275,  0.4530],\n",
      "         [-0.3861,  0.4245,  0.0799, -0.4656,  0.4520]],\n",
      "\n",
      "        [[ 0.8189,  0.8726, -0.8887,  0.8437,  0.7976],\n",
      "         [ 0.8572,  0.8761, -0.9147,  0.8565,  0.7746]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0230 cost = 0.022350\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2672,  0.1722, -0.0887, -0.3286,  0.4535],\n",
      "         [-0.3862,  0.4251,  0.0800, -0.4657,  0.4523]],\n",
      "\n",
      "        [[ 0.8192,  0.8728, -0.8888,  0.8439,  0.7978],\n",
      "         [ 0.8574,  0.8762, -0.9148,  0.8566,  0.7749]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0231 cost = 0.022252\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7012, -0.9073, -0.9037,  0.5486,  0.0614],\n",
      "         [-0.4181,  0.2609,  0.0684, -0.3983,  0.4000]],\n",
      "\n",
      "        [[-0.6497, -0.3706,  0.6419,  0.0744, -0.8429],\n",
      "         [ 0.7494,  0.7695, -0.8265,  0.7445,  0.7579]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0231 cost = 0.019843\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9220, -0.2982, -0.4603,  0.9049, -0.5199],\n",
      "         [ 0.7742, -0.7587, -0.8394,  0.7688,  0.0347]],\n",
      "\n",
      "        [[-0.2838, -0.3154,  0.4441, -0.0244, -0.6116],\n",
      "         [-0.6471, -0.3183,  0.6656,  0.0766, -0.8591]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0231 cost = 0.033019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3864,  0.4272,  0.0806, -0.4659,  0.4533],\n",
      "         [ 0.7019, -0.9075, -0.9039,  0.5492,  0.0614]],\n",
      "\n",
      "        [[ 0.8579,  0.8764, -0.9150,  0.8571,  0.7757],\n",
      "         [-0.6499, -0.3709,  0.6421,  0.0738, -0.8430]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0232 cost = 0.018122\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7747, -0.7591, -0.8398,  0.7691,  0.0345],\n",
      "         [-0.4199,  0.2623,  0.0688, -0.4014,  0.4023]],\n",
      "\n",
      "        [[-0.6474, -0.3188,  0.6659,  0.0760, -0.8593],\n",
      "         [ 0.7503,  0.7701, -0.8270,  0.7453,  0.7589]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0232 cost = 0.020277\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9221, -0.3002, -0.4617,  0.9050, -0.5209],\n",
      "         [-0.2725,  0.1763, -0.0876, -0.3345,  0.4560]],\n",
      "\n",
      "        [[-0.2845, -0.3161,  0.4448, -0.0249, -0.6122],\n",
      "         [ 0.8203,  0.8736, -0.8895,  0.8451,  0.7992]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0232 cost = 0.033795\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3866,  0.4287,  0.0812, -0.4662,  0.4543],\n",
      "         [ 0.7030, -0.9077, -0.9042,  0.5505,  0.0614]],\n",
      "\n",
      "        [[ 0.8583,  0.8767, -0.9152,  0.8575,  0.7765],\n",
      "         [-0.6501, -0.3713,  0.6424,  0.0727, -0.8432]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0233 cost = 0.017928\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2739,  0.1772, -0.0874, -0.3360,  0.4568],\n",
      "         [ 0.7756, -0.7600, -0.8403,  0.7697,  0.0341]],\n",
      "\n",
      "        [[ 0.8208,  0.8739, -0.8897,  0.8455,  0.7997],\n",
      "         [-0.6479, -0.3195,  0.6663,  0.0750, -0.8596]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0233 cost = 0.022364\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4219,  0.2636,  0.0694, -0.4048,  0.4045],\n",
      "         [ 0.9223, -0.3025, -0.4633,  0.9051, -0.5221]],\n",
      "\n",
      "        [[ 0.7513,  0.7708, -0.8276,  0.7463,  0.7600],\n",
      "         [-0.2853, -0.3169,  0.4454, -0.0255, -0.6127]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0233 cost = 0.037016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3871,  0.4299,  0.0818, -0.4667,  0.4550],\n",
      "         [ 0.9223, -0.3032, -0.4638,  0.9052, -0.5225]],\n",
      "\n",
      "        [[ 0.8588,  0.8769, -0.9153,  0.8579,  0.7772],\n",
      "         [-0.2856, -0.3172,  0.4456, -0.0258, -0.6130]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0234 cost = 0.028811\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7046, -0.9081, -0.9047,  0.5522,  0.0614],\n",
      "         [-0.4230,  0.2642,  0.0697, -0.4064,  0.4055]],\n",
      "\n",
      "        [[-0.6504, -0.3719,  0.6428,  0.0714, -0.8434],\n",
      "         [ 0.7517,  0.7710, -0.8278,  0.7466,  0.7604]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0234 cost = 0.019235\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2772,  0.1791, -0.0869, -0.3394,  0.4582],\n",
      "         [ 0.7767, -0.7611, -0.8411,  0.7706,  0.0337]],\n",
      "\n",
      "        [[ 0.8216,  0.8745, -0.8901,  0.8464,  0.8005],\n",
      "         [-0.6485, -0.3205,  0.6668,  0.0736, -0.8600]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0234 cost = 0.022037\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9226, -0.3055, -0.4657,  0.9055, -0.5237],\n",
      "         [-0.2781,  0.1796, -0.0867, -0.3405,  0.4586]],\n",
      "\n",
      "        [[-0.2866, -0.3183,  0.4464, -0.0268, -0.6139],\n",
      "         [ 0.8218,  0.8746, -0.8902,  0.8466,  0.8007]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0235 cost = 0.032918\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4246,  0.2650,  0.0701, -0.4091,  0.4069],\n",
      "         [ 0.7055, -0.9084, -0.9050,  0.5533,  0.0614]],\n",
      "\n",
      "        [[ 0.7523,  0.7713, -0.8280,  0.7471,  0.7610],\n",
      "         [-0.6507, -0.3723,  0.6430,  0.0705, -0.8435]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0235 cost = 0.027650\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3883,  0.4317,  0.0829, -0.4678,  0.4562],\n",
      "         [ 0.7775, -0.7619, -0.8417,  0.7712,  0.0333]],\n",
      "\n",
      "        [[ 0.8594,  0.8772, -0.9156,  0.8585,  0.7782],\n",
      "         [-0.6490, -0.3211,  0.6672,  0.0726, -0.8602]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0235 cost = 0.014793\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3886,  0.4320,  0.0831, -0.4680,  0.4564],\n",
      "         [-0.2810,  0.1809, -0.0862, -0.3434,  0.4596]],\n",
      "\n",
      "        [[ 0.8596,  0.8773, -0.9157,  0.8587,  0.7784],\n",
      "         [ 0.8224,  0.8750, -0.8906,  0.8472,  0.8014]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0236 cost = 0.021688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7781, -0.7623, -0.8421,  0.7716,  0.0331],\n",
      "         [ 0.9228, -0.3084, -0.4679,  0.9057, -0.5252]],\n",
      "\n",
      "        [[-0.6492, -0.3215,  0.6675,  0.0720, -0.8604],\n",
      "         [-0.2878, -0.3195,  0.4473, -0.0278, -0.6148]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0236 cost = 0.031265\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4268,  0.2661,  0.0708, -0.4124,  0.4088],\n",
      "         [ 0.7070, -0.9087, -0.9054,  0.5548,  0.0614]],\n",
      "\n",
      "        [[ 0.7532,  0.7718, -0.8285,  0.7479,  0.7619],\n",
      "         [-0.6509, -0.3728,  0.6433,  0.0692, -0.8437]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0236 cost = 0.027266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3894,  0.4331,  0.0837, -0.4684,  0.4571],\n",
      "         [-0.2824,  0.1825, -0.0858, -0.3452,  0.4607]],\n",
      "\n",
      "        [[ 0.8600,  0.8775, -0.9159,  0.8591,  0.7792],\n",
      "         [ 0.8230,  0.8754, -0.8909,  0.8478,  0.8020]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0237 cost = 0.021487\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7792, -0.7630, -0.8426,  0.7724,  0.0328],\n",
      "         [ 0.9231, -0.3104, -0.4695,  0.9060, -0.5264]],\n",
      "\n",
      "        [[-0.6497, -0.3223,  0.6679,  0.0710, -0.8607],\n",
      "         [-0.2886, -0.3203,  0.4479, -0.0285, -0.6155]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0237 cost = 0.030929\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7083, -0.9089, -0.9057,  0.5562,  0.0614],\n",
      "         [-0.4278,  0.2669,  0.0712, -0.4138,  0.4102]],\n",
      "\n",
      "        [[-0.6512, -0.3733,  0.6436,  0.0682, -0.8438],\n",
      "         [ 0.7539,  0.7723, -0.8288,  0.7485,  0.7627]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0237 cost = 0.018625\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7800, -0.7634, -0.8431,  0.7731,  0.0325],\n",
      "         [ 0.9234, -0.3117, -0.4707,  0.9062, -0.5271]],\n",
      "\n",
      "        [[-0.6500, -0.3228,  0.6681,  0.0702, -0.8609],\n",
      "         [-0.2892, -0.3210,  0.4484, -0.0290, -0.6160]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0238 cost = 0.030688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7092, -0.9090, -0.9060,  0.5572,  0.0613],\n",
      "         [-0.2827,  0.1844, -0.0855, -0.3461,  0.4619]],\n",
      "\n",
      "        [[-0.6513, -0.3736,  0.6438,  0.0675, -0.8439],\n",
      "         [ 0.8238,  0.8759, -0.8913,  0.8486,  0.8029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0238 cost = 0.016376\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3901,  0.4348,  0.0846, -0.4688,  0.4583],\n",
      "         [-0.4281,  0.2675,  0.0716, -0.4146,  0.4112]],\n",
      "\n",
      "        [[ 0.8607,  0.8778, -0.9161,  0.8597,  0.7804],\n",
      "         [ 0.7545,  0.7725, -0.8291,  0.7490,  0.7633]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0238 cost = 0.029661\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4279,  0.2679,  0.0716, -0.4145,  0.4118],\n",
      "         [ 0.7100, -0.9092, -0.9062,  0.5582,  0.0613]],\n",
      "\n",
      "        [[ 0.7547,  0.7727, -0.8292,  0.7493,  0.7636],\n",
      "         [-0.6515, -0.3739,  0.6440,  0.0669, -0.8440]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0239 cost = 0.026623\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3901,  0.4356,  0.0849, -0.4686,  0.4590],\n",
      "         [-0.2827,  0.1857, -0.0852, -0.3465,  0.4629]],\n",
      "\n",
      "        [[ 0.8611,  0.8780, -0.9163,  0.8600,  0.7810],\n",
      "         [ 0.8244,  0.8762, -0.8916,  0.8491,  0.8036]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0239 cost = 0.021016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7819, -0.7645, -0.8440,  0.7746,  0.0318],\n",
      "         [ 0.9239, -0.3147, -0.4731,  0.9067, -0.5288]],\n",
      "\n",
      "        [[-0.6508, -0.3240,  0.6688,  0.0686, -0.8614],\n",
      "         [-0.2906, -0.3224,  0.4495, -0.0300, -0.6171]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0239 cost = 0.030151\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3901,  0.4365,  0.0852, -0.4684,  0.4597],\n",
      "         [-0.4279,  0.2689,  0.0720, -0.4144,  0.4137]],\n",
      "\n",
      "        [[ 0.8614,  0.8782, -0.9165,  0.8604,  0.7816],\n",
      "         [ 0.7557,  0.7734, -0.8298,  0.7502,  0.7646]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0240 cost = 0.029266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2819,  0.1876, -0.0850, -0.3461,  0.4642],\n",
      "         [ 0.9242, -0.3158, -0.4741,  0.9069, -0.5293]],\n",
      "\n",
      "        [[ 0.8251,  0.8767, -0.8919,  0.8498,  0.8044],\n",
      "         [-0.2911, -0.3228,  0.4499, -0.0302, -0.6175]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0240 cost = 0.033832\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7120, -0.9094, -0.9066,  0.5603,  0.0613],\n",
      "         [ 0.7830, -0.7650, -0.8446,  0.7754,  0.0316]],\n",
      "\n",
      "        [[-0.6517, -0.3745,  0.6444,  0.0654, -0.8442],\n",
      "         [-0.6513, -0.3246,  0.6692,  0.0677, -0.8617]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0240 cost = 0.015005\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2816,  0.1888, -0.0848, -0.3460,  0.4650],\n",
      "         [-0.3901,  0.4379,  0.0856, -0.4677,  0.4609]],\n",
      "\n",
      "        [[ 0.8255,  0.8769, -0.8922,  0.8502,  0.8049],\n",
      "         [ 0.8620,  0.8785, -0.9167,  0.8609,  0.7826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0241 cost = 0.019973\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7836, -0.7653, -0.8450,  0.7759,  0.0315],\n",
      "         [ 0.7126, -0.9095, -0.9068,  0.5609,  0.0613]],\n",
      "\n",
      "        [[-0.6516, -0.3251,  0.6695,  0.0671, -0.8619],\n",
      "         [-0.6518, -0.3747,  0.6446,  0.0648, -0.8443]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0241 cost = 0.024270\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9246, -0.3178, -0.4759,  0.9072, -0.5301],\n",
      "         [-0.4270,  0.2711,  0.0723, -0.4127,  0.4174]],\n",
      "\n",
      "        [[-0.2922, -0.3239,  0.4508, -0.0308, -0.6183],\n",
      "         [ 0.7572,  0.7744, -0.8307,  0.7515,  0.7663]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0241 cost = 0.033537\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3899,  0.4395,  0.0860, -0.4672,  0.4620],\n",
      "         [-0.4269,  0.2715,  0.0724, -0.4125,  0.4180]],\n",
      "\n",
      "        [[ 0.8625,  0.8788, -0.9169,  0.8613,  0.7835],\n",
      "         [ 0.7574,  0.7746, -0.8308,  0.7517,  0.7665]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0242 cost = 0.028669\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7846, -0.7659, -0.8456,  0.7767,  0.0312],\n",
      "         [ 0.9248, -0.3190, -0.4769,  0.9074, -0.5307]],\n",
      "\n",
      "        [[-0.6520, -0.3259,  0.6699,  0.0661, -0.8622],\n",
      "         [-0.2928, -0.3244,  0.4512, -0.0311, -0.6187]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0242 cost = 0.029377\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2815,  0.1918, -0.0845, -0.3463,  0.4673],\n",
      "         [ 0.7140, -0.9098, -0.9072,  0.5624,  0.0613]],\n",
      "\n",
      "        [[ 0.8266,  0.8777, -0.8927,  0.8513,  0.8062],\n",
      "         [-0.6521, -0.3754,  0.6450,  0.0634, -0.8445]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0242 cost = 0.022655\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3896,  0.4409,  0.0862, -0.4665,  0.4632],\n",
      "         [-0.2816,  0.1924, -0.0844, -0.3463,  0.4677]],\n",
      "\n",
      "        [[ 0.8630,  0.8791, -0.9171,  0.8618,  0.7844],\n",
      "         [ 0.8268,  0.8778, -0.8928,  0.8515,  0.8065]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0243 cost = 0.020163\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4256,  0.2733,  0.0725, -0.4104,  0.4208],\n",
      "         [ 0.7146, -0.9099, -0.9074,  0.5632,  0.0613]],\n",
      "\n",
      "        [[ 0.7585,  0.7754, -0.8314,  0.7528,  0.7678],\n",
      "         [-0.6522, -0.3757,  0.6452,  0.0627, -0.8446]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0243 cost = 0.025437\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9252, -0.3214, -0.4788,  0.9077, -0.5317],\n",
      "         [ 0.7859, -0.7667, -0.8464,  0.7777,  0.0308]],\n",
      "\n",
      "        [[-0.2938, -0.3255,  0.4521, -0.0317, -0.6194],\n",
      "         [-0.6526, -0.3269,  0.6704,  0.0648, -0.8626]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0243 cost = 0.029140\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3895,  0.4424,  0.0866, -0.4659,  0.4644],\n",
      "         [ 0.7155, -0.9100, -0.9076,  0.5641,  0.0613]],\n",
      "\n",
      "        [[ 0.8635,  0.8794, -0.9173,  0.8623,  0.7854],\n",
      "         [-0.6523, -0.3760,  0.6454,  0.0619, -0.8448]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0244 cost = 0.015911\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4254,  0.2744,  0.0727, -0.4097,  0.4226],\n",
      "         [ 0.7866, -0.7671, -0.8468,  0.7783,  0.0305]],\n",
      "\n",
      "        [[ 0.7594,  0.7760, -0.8319,  0.7535,  0.7688],\n",
      "         [-0.6530, -0.3274,  0.6707,  0.0641, -0.8628]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0244 cost = 0.021298\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2811,  0.1954, -0.0840, -0.3464,  0.4698],\n",
      "         [ 0.9255, -0.3234, -0.4804,  0.9080, -0.5327]],\n",
      "\n",
      "        [[ 0.8279,  0.8785, -0.8934,  0.8525,  0.8078],\n",
      "         [-0.2945, -0.3262,  0.4526, -0.0321, -0.6199]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0244 cost = 0.032421\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7168, -0.9102, -0.9080,  0.5656,  0.0613],\n",
      "         [-0.4256,  0.2751,  0.0729, -0.4099,  0.4237]],\n",
      "\n",
      "        [[-0.6525, -0.3765,  0.6457,  0.0608, -0.8449],\n",
      "         [ 0.7599,  0.7763, -0.8322,  0.7540,  0.7693]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0245 cost = 0.017292\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3899,  0.4440,  0.0872, -0.4656,  0.4655],\n",
      "         [ 0.9257, -0.3246, -0.4814,  0.9082, -0.5333]],\n",
      "\n",
      "        [[ 0.8641,  0.8797, -0.9176,  0.8629,  0.7865],\n",
      "         [-0.2950, -0.3267,  0.4531, -0.0325, -0.6203]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0245 cost = 0.025507\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7880, -0.7678, -0.8475,  0.7794,  0.0301],\n",
      "         [-0.2817,  0.1970, -0.0837, -0.3473,  0.4708]],\n",
      "\n",
      "        [[-0.6535, -0.3284,  0.6712,  0.0627, -0.8632],\n",
      "         [ 0.8285,  0.8788, -0.8937,  0.8531,  0.8084]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0245 cost = 0.016013\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3902,  0.4446,  0.0875, -0.4657,  0.4660],\n",
      "         [-0.4262,  0.2760,  0.0733, -0.4107,  0.4252]],\n",
      "\n",
      "        [[ 0.8644,  0.8799, -0.9177,  0.8631,  0.7869],\n",
      "         [ 0.7605,  0.7767, -0.8326,  0.7546,  0.7700]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0246 cost = 0.027606\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2822,  0.1980, -0.0835, -0.3478,  0.4716],\n",
      "         [ 0.9260, -0.3264, -0.4829,  0.9084, -0.5342]],\n",
      "\n",
      "        [[ 0.8289,  0.8791, -0.8939,  0.8535,  0.8088],\n",
      "         [-0.2959, -0.3276,  0.4537, -0.0331, -0.6210]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0246 cost = 0.031871\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7889, -0.7684, -0.8481,  0.7800,  0.0298],\n",
      "         [ 0.7185, -0.9105, -0.9084,  0.5674,  0.0612]],\n",
      "\n",
      "        [[-0.6540, -0.3291,  0.6716,  0.0618, -0.8635],\n",
      "         [-0.6528, -0.3772,  0.6461,  0.0592, -0.8452]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0246 cost = 0.023052\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9262, -0.3277, -0.4840,  0.9086, -0.5347],\n",
      "         [ 0.7892, -0.7686, -0.8483,  0.7803,  0.0297]],\n",
      "\n",
      "        [[-0.2965, -0.3282,  0.4541, -0.0336, -0.6215],\n",
      "         [-0.6541, -0.3294,  0.6717,  0.0614, -0.8636]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0247 cost = 0.028150\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3905,  0.4461,  0.0880, -0.4653,  0.4672],\n",
      "         [-0.2825,  0.1993, -0.0834, -0.3482,  0.4727]],\n",
      "\n",
      "        [[ 0.8650,  0.8802, -0.9180,  0.8636,  0.7879],\n",
      "         [ 0.8294,  0.8795, -0.8942,  0.8540,  0.8094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0247 cost = 0.019287\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7196, -0.9108, -0.9088,  0.5686,  0.0611],\n",
      "         [-0.4257,  0.2777,  0.0736, -0.4095,  0.4279]],\n",
      "\n",
      "        [[-0.6531, -0.3778,  0.6464,  0.0581, -0.8454],\n",
      "         [ 0.7616,  0.7774, -0.8331,  0.7555,  0.7712]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0247 cost = 0.016873\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7199, -0.9109, -0.9089,  0.5690,  0.0611],\n",
      "         [ 0.7903, -0.7695, -0.8491,  0.7812,  0.0292]],\n",
      "\n",
      "        [[-0.6531, -0.3779,  0.6465,  0.0577, -0.8454],\n",
      "         [-0.6546, -0.3302,  0.6722,  0.0603, -0.8639]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0248 cost = 0.013892\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9267, -0.3307, -0.4865,  0.9091, -0.5362],\n",
      "         [-0.3905,  0.4472,  0.0884, -0.4651,  0.4681]],\n",
      "\n",
      "        [[-0.2977, -0.3296,  0.4550, -0.0347, -0.6225],\n",
      "         [ 0.8654,  0.8804, -0.9181,  0.8640,  0.7886]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0248 cost = 0.023469\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2815,  0.2008, -0.0833, -0.3477,  0.4739],\n",
      "         [-0.4249,  0.2784,  0.0738, -0.4088,  0.4291]],\n",
      "\n",
      "        [[ 0.8301,  0.8799, -0.8945,  0.8547,  0.8102],\n",
      "         [ 0.7621,  0.7777, -0.8333,  0.7560,  0.7718]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0248 cost = 0.031047\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3906,  0.4479,  0.0886, -0.4650,  0.4687],\n",
      "         [ 0.9270, -0.3322, -0.4877,  0.9093, -0.5369]],\n",
      "\n",
      "        [[ 0.8656,  0.8805, -0.9182,  0.8642,  0.7891],\n",
      "         [-0.2982, -0.3302,  0.4554, -0.0352, -0.6229]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0249 cost = 0.024473\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7212, -0.9112, -0.9093,  0.5705,  0.0610],\n",
      "         [-0.2819,  0.2019, -0.0832, -0.3481,  0.4746]],\n",
      "\n",
      "        [[-0.6534, -0.3786,  0.6468,  0.0564, -0.8456],\n",
      "         [ 0.8304,  0.8801, -0.8947,  0.8550,  0.8106]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0249 cost = 0.014694\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4248,  0.2795,  0.0739, -0.4087,  0.4307],\n",
      "         [ 0.7919, -0.7706, -0.8501,  0.7825,  0.0284]],\n",
      "\n",
      "        [[ 0.7627,  0.7781, -0.8336,  0.7565,  0.7724],\n",
      "         [-0.6553, -0.3315,  0.6728,  0.0587, -0.8644]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0249 cost = 0.020226\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4250,  0.2798,  0.0740, -0.4090,  0.4312],\n",
      "         [-0.2825,  0.2030, -0.0831, -0.3489,  0.4753]],\n",
      "\n",
      "        [[ 0.7629,  0.7782, -0.8338,  0.7567,  0.7727],\n",
      "         [ 0.8308,  0.8804, -0.8949,  0.8554,  0.8111]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0250 cost = 0.031007\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7924, -0.7708, -0.8504,  0.7829,  0.0282],\n",
      "         [ 0.9273, -0.3346, -0.4898,  0.9096, -0.5380]],\n",
      "\n",
      "        [[-0.6556, -0.3319,  0.6730,  0.0582, -0.8646],\n",
      "         [-0.2992, -0.3313,  0.4562, -0.0359, -0.6237]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0250 cost = 0.027029\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7222, -0.9114, -0.9096,  0.5715,  0.0610],\n",
      "         [-0.3909,  0.4502,  0.0892, -0.4650,  0.4702]],\n",
      "\n",
      "        [[-0.6536, -0.3791,  0.6471,  0.0555, -0.8458],\n",
      "         [ 0.8663,  0.8809, -0.9185,  0.8649,  0.7903]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0250 cost = 0.009415\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2828,  0.2057, -0.0829, -0.3498,  0.4765],\n",
      "         [ 0.7225, -0.9115, -0.9097,  0.5718,  0.0610]],\n",
      "\n",
      "        [[ 0.8314,  0.8808, -0.8952,  0.8560,  0.8118],\n",
      "         [-0.6537, -0.3792,  0.6472,  0.0552, -0.8458]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0251 cost = 0.020885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4253,  0.2816,  0.0743, -0.4101,  0.4337],\n",
      "         [-0.3908,  0.4511,  0.0894, -0.4649,  0.4707]],\n",
      "\n",
      "        [[ 0.7640,  0.7791, -0.8344,  0.7577,  0.7739],\n",
      "         [ 0.8666,  0.8811, -0.9186,  0.8651,  0.7908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0251 cost = 0.025614\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7934, -0.7712, -0.8510,  0.7836,  0.0280],\n",
      "         [ 0.9276, -0.3362, -0.4914,  0.9097, -0.5388]],\n",
      "\n",
      "        [[-0.6561, -0.3327,  0.6735,  0.0571, -0.8649],\n",
      "         [-0.3001, -0.3321,  0.4569, -0.0363, -0.6243]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0251 cost = 0.026729\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7937, -0.7713, -0.8512,  0.7837,  0.0279],\n",
      "         [ 0.7234, -0.9116, -0.9099,  0.5726,  0.0610]],\n",
      "\n",
      "        [[-0.6562, -0.3329,  0.6736,  0.0568, -0.8650],\n",
      "         [-0.6538, -0.3796,  0.6474,  0.0544, -0.8459]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0252 cost = 0.021969\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4254,  0.2833,  0.0745, -0.4111,  0.4359],\n",
      "         [-0.2837,  0.2093, -0.0825, -0.3516,  0.4783]],\n",
      "\n",
      "        [[ 0.7648,  0.7797, -0.8349,  0.7585,  0.7748],\n",
      "         [ 0.8321,  0.8813, -0.8955,  0.8567,  0.8126]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0252 cost = 0.030228\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3894,  0.4539,  0.0898, -0.4638,  0.4721],\n",
      "         [ 0.9278, -0.3374, -0.4926,  0.9099, -0.5394]],\n",
      "\n",
      "        [[ 0.8671,  0.8814, -0.9188,  0.8656,  0.7918],\n",
      "         [-0.3008, -0.3327,  0.4574, -0.0366, -0.6248]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0252 cost = 0.023661\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4250,  0.2845,  0.0747, -0.4113,  0.4374],\n",
      "         [ 0.7245, -0.9117, -0.9102,  0.5737,  0.0609]],\n",
      "\n",
      "        [[ 0.7653,  0.7802, -0.8353,  0.7591,  0.7754],\n",
      "         [-0.6541, -0.3801,  0.6478,  0.0534, -0.8462]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0253 cost = 0.023093\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3887,  0.4553,  0.0900, -0.4633,  0.4728],\n",
      "         [ 0.9279, -0.3382, -0.4935,  0.9100, -0.5399]],\n",
      "\n",
      "        [[ 0.8673,  0.8816, -0.9190,  0.8659,  0.7922],\n",
      "         [-0.3012, -0.3331,  0.4578, -0.0368, -0.6251]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0253 cost = 0.023517\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2829,  0.2133, -0.0824, -0.3522,  0.4800],\n",
      "         [ 0.7953, -0.7718, -0.8520,  0.7849,  0.0275]],\n",
      "\n",
      "        [[ 0.8328,  0.8817, -0.8959,  0.8574,  0.8134],\n",
      "         [-0.6570, -0.3343,  0.6742,  0.0551, -0.8656]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0253 cost = 0.018171\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7956, -0.7719, -0.8522,  0.7851,  0.0274],\n",
      "         [-0.3884,  0.4564,  0.0902, -0.4630,  0.4733]],\n",
      "\n",
      "        [[-0.6571, -0.3346,  0.6743,  0.0548, -0.8657],\n",
      "         [ 0.8676,  0.8817, -0.9190,  0.8661,  0.7926]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0254 cost = 0.009533\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7261, -0.9120, -0.9106,  0.5753,  0.0609],\n",
      "         [-0.2831,  0.2150, -0.0822, -0.3529,  0.4808]],\n",
      "\n",
      "        [[-0.6543, -0.3808,  0.6481,  0.0520, -0.8465],\n",
      "         [ 0.8331,  0.8819, -0.8960,  0.8577,  0.8137]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0254 cost = 0.013995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4255,  0.2868,  0.0751, -0.4130,  0.4404],\n",
      "         [ 0.9282, -0.3399, -0.4953,  0.9102, -0.5407]],\n",
      "\n",
      "        [[ 0.7665,  0.7811, -0.8359,  0.7602,  0.7766],\n",
      "         [-0.3022, -0.3342,  0.4586, -0.0374, -0.6259]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0254 cost = 0.029878\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7964, -0.7723, -0.8526,  0.7857,  0.0272],\n",
      "         [ 0.9283, -0.3403, -0.4957,  0.9102, -0.5410]],\n",
      "\n",
      "        [[-0.6575, -0.3353,  0.6747,  0.0538, -0.8660],\n",
      "         [-0.3025, -0.3345,  0.4588, -0.0376, -0.6261]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0255 cost = 0.025918\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4259,  0.2875,  0.0753, -0.4139,  0.4414],\n",
      "         [-0.3883,  0.4582,  0.0907, -0.4628,  0.4742]],\n",
      "\n",
      "        [[ 0.7668,  0.7812, -0.8361,  0.7605,  0.7769],\n",
      "         [ 0.8680,  0.8819, -0.9192,  0.8665,  0.7933]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0255 cost = 0.024459\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2842,  0.2180, -0.0819, -0.3546,  0.4822],\n",
      "         [ 0.7273, -0.9122, -0.9109,  0.5765,  0.0608]],\n",
      "\n",
      "        [[ 0.8336,  0.8822, -0.8963,  0.8583,  0.8144],\n",
      "         [-0.6546, -0.3815,  0.6485,  0.0509, -0.8467]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0255 cost = 0.019940\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3877,  0.4595,  0.0909, -0.4623,  0.4748],\n",
      "         [-0.2847,  0.2190, -0.0818, -0.3553,  0.4826]],\n",
      "\n",
      "        [[ 0.8682,  0.8820, -0.9193,  0.8667,  0.7937],\n",
      "         [ 0.8338,  0.8824, -0.8964,  0.8584,  0.8145]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0256 cost = 0.017732\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7279, -0.9123, -0.9110,  0.5771,  0.0608],\n",
      "         [ 0.9285, -0.3419, -0.4974,  0.9104, -0.5418]],\n",
      "\n",
      "        [[-0.6547, -0.3817,  0.6486,  0.0503, -0.8468],\n",
      "         [-0.3036, -0.3356,  0.4596, -0.0384, -0.6270]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0256 cost = 0.023908\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7976, -0.7729, -0.8534,  0.7865,  0.0270],\n",
      "         [-0.4265,  0.2897,  0.0757, -0.4156,  0.4442]],\n",
      "\n",
      "        [[-0.6582, -0.3365,  0.6753,  0.0523, -0.8665],\n",
      "         [ 0.7677,  0.7819, -0.8366,  0.7613,  0.7779]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0256 cost = 0.015847\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3870,  0.4614,  0.0912, -0.4616,  0.4758],\n",
      "         [ 0.9287, -0.3427, -0.4982,  0.9105, -0.5422]],\n",
      "\n",
      "        [[ 0.8685,  0.8822, -0.9194,  0.8670,  0.7943],\n",
      "         [-0.3041, -0.3361,  0.4600, -0.0388, -0.6275]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0257 cost = 0.022684\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2858,  0.2225, -0.0814, -0.3569,  0.4842],\n",
      "         [ 0.7288, -0.9125, -0.9113,  0.5780,  0.0608]],\n",
      "\n",
      "        [[ 0.8344,  0.8828, -0.8967,  0.8591,  0.8152],\n",
      "         [-0.6549, -0.3822,  0.6489,  0.0495, -0.8470]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0257 cost = 0.019611\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4269,  0.2909,  0.0760, -0.4167,  0.4458],\n",
      "         [ 0.7983, -0.7733, -0.8538,  0.7870,  0.0267]],\n",
      "\n",
      "        [[ 0.7682,  0.7823, -0.8369,  0.7618,  0.7784],\n",
      "         [-0.6586, -0.3372,  0.6757,  0.0514, -0.8667]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0257 cost = 0.018562\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7985, -0.7734, -0.8539,  0.7872,  0.0267],\n",
      "         [-0.4273,  0.2912,  0.0761, -0.4173,  0.4464]],\n",
      "\n",
      "        [[-0.6588, -0.3374,  0.6758,  0.0511, -0.8668],\n",
      "         [ 0.7684,  0.7824, -0.8370,  0.7620,  0.7787]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0258 cost = 0.015650\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2873,  0.2248, -0.0809, -0.3588,  0.4853],\n",
      "         [ 0.7297, -0.9126, -0.9115,  0.5789,  0.0608]],\n",
      "\n",
      "        [[ 0.8349,  0.8831, -0.8970,  0.8595,  0.8157],\n",
      "         [-0.6551, -0.3826,  0.6491,  0.0487, -0.8472]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0258 cost = 0.019413\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9289, -0.3445, -0.5000,  0.9107, -0.5431],\n",
      "         [-0.3869,  0.4636,  0.0920, -0.4614,  0.4770]],\n",
      "\n",
      "        [[-0.3054, -0.3374,  0.4609, -0.0397, -0.6285],\n",
      "         [ 0.8691,  0.8825, -0.9197,  0.8676,  0.7952]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0258 cost = 0.021346\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7303, -0.9127, -0.9117,  0.5795,  0.0608],\n",
      "         [-0.4288,  0.2923,  0.0765, -0.4197,  0.4481]],\n",
      "\n",
      "        [[-0.6552, -0.3828,  0.6493,  0.0481, -0.8473],\n",
      "         [ 0.7691,  0.7830, -0.8374,  0.7627,  0.7793]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0259 cost = 0.015136\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2894,  0.2269, -0.0804, -0.3610,  0.4863],\n",
      "         [ 0.9290, -0.3455, -0.5007,  0.9108, -0.5436]],\n",
      "\n",
      "        [[ 0.8354,  0.8834, -0.8972,  0.8601,  0.8163],\n",
      "         [-0.3058, -0.3378,  0.4612, -0.0400, -0.6288]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0259 cost = 0.028071\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3871,  0.4647,  0.0925, -0.4615,  0.4776],\n",
      "         [ 0.7996, -0.7741, -0.8545,  0.7878,  0.0264]],\n",
      "\n",
      "        [[ 0.8694,  0.8827, -0.9198,  0.8679,  0.7957],\n",
      "         [-0.6594, -0.3384,  0.6763,  0.0496, -0.8672]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0259 cost = 0.011477\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4302,  0.2931,  0.0769, -0.4221,  0.4495],\n",
      "         [ 0.9290, -0.3465, -0.5015,  0.9108, -0.5440]],\n",
      "\n",
      "        [[ 0.7697,  0.7834, -0.8377,  0.7632,  0.7799],\n",
      "         [-0.3064, -0.3383,  0.4616, -0.0405, -0.6292]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0260 cost = 0.028326\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3874,  0.4653,  0.0929, -0.4618,  0.4779],\n",
      "         [ 0.8000, -0.7744, -0.8548,  0.7881,  0.0263]],\n",
      "\n",
      "        [[ 0.8696,  0.8828, -0.9199,  0.8680,  0.7960],\n",
      "         [-0.6597, -0.3388,  0.6766,  0.0490, -0.8674]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0260 cost = 0.011405\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7316, -0.9130, -0.9121,  0.5809,  0.0607],\n",
      "         [-0.2922,  0.2291, -0.0797, -0.3639,  0.4874]],\n",
      "\n",
      "        [[-0.6555, -0.3834,  0.6496,  0.0467, -0.8475],\n",
      "         [ 0.8360,  0.8838, -0.8975,  0.8606,  0.8168]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0260 cost = 0.013180\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3879,  0.4659,  0.0932, -0.4621,  0.4782],\n",
      "         [-0.2929,  0.2296, -0.0795, -0.3646,  0.4877]],\n",
      "\n",
      "        [[ 0.8698,  0.8829, -0.9200,  0.8682,  0.7963],\n",
      "         [ 0.8361,  0.8839, -0.8976,  0.8608,  0.8170]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0261 cost = 0.016991\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8006, -0.7748, -0.8552,  0.7886,  0.0260],\n",
      "         [-0.4321,  0.2941,  0.0774, -0.4250,  0.4512]],\n",
      "\n",
      "        [[-0.6601, -0.3394,  0.6769,  0.0481, -0.8676],\n",
      "         [ 0.7703,  0.7838, -0.8380,  0.7638,  0.7805]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0261 cost = 0.015170\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7323, -0.9131, -0.9122,  0.5815,  0.0607],\n",
      "         [ 0.9293, -0.3485, -0.5035,  0.9111, -0.5451]],\n",
      "\n",
      "        [[-0.6556, -0.3837,  0.6498,  0.0461, -0.8476],\n",
      "         [-0.3075, -0.3396,  0.4625, -0.0416, -0.6302]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0261 cost = 0.022773\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9294, -0.3490, -0.5039,  0.9112, -0.5453],\n",
      "         [-0.2944,  0.2315, -0.0791, -0.3663,  0.4885]],\n",
      "\n",
      "        [[-0.3078, -0.3399,  0.4627, -0.0418, -0.6304],\n",
      "         [ 0.8366,  0.8842, -0.8978,  0.8612,  0.8175]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0262 cost = 0.025402\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7328, -0.9133, -0.9124,  0.5820,  0.0606],\n",
      "         [-0.4329,  0.2947,  0.0777, -0.4266,  0.4523]],\n",
      "\n",
      "        [[-0.6558, -0.3839,  0.6500,  0.0457, -0.8477],\n",
      "         [ 0.7708,  0.7841, -0.8382,  0.7642,  0.7810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0262 cost = 0.014716\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8015, -0.7755, -0.8557,  0.7893,  0.0256],\n",
      "         [-0.3887,  0.4672,  0.0940, -0.4626,  0.4792]],\n",
      "\n",
      "        [[-0.6606, -0.3401,  0.6773,  0.0470, -0.8679],\n",
      "         [ 0.8703,  0.8831, -0.9202,  0.8687,  0.7972]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0262 cost = 0.008760\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8017, -0.7757, -0.8559,  0.7894,  0.0254],\n",
      "         [ 0.7333, -0.9134, -0.9125,  0.5826,  0.0606]],\n",
      "\n",
      "        [[-0.6607, -0.3403,  0.6774,  0.0468, -0.8680],\n",
      "         [-0.6559, -0.3841,  0.6501,  0.0452, -0.8477]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0263 cost = 0.020072\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2956,  0.2328, -0.0787, -0.3678,  0.4893],\n",
      "         [-0.3888,  0.4676,  0.0943, -0.4628,  0.4796]],\n",
      "\n",
      "        [[ 0.8371,  0.8845, -0.8981,  0.8617,  0.8180],\n",
      "         [ 0.8705,  0.8832, -0.9203,  0.8688,  0.7974]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0263 cost = 0.015978\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9297, -0.3519, -0.5061,  0.9115, -0.5468],\n",
      "         [-0.4337,  0.2954,  0.0781, -0.4284,  0.4535]],\n",
      "\n",
      "        [[-0.3091, -0.3412,  0.4635, -0.0431, -0.6314],\n",
      "         [ 0.7714,  0.7845, -0.8385,  0.7647,  0.7816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0263 cost = 0.027395\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2965,  0.2337, -0.0784, -0.3688,  0.4898],\n",
      "         [ 0.7341, -0.9136, -0.9128,  0.5835,  0.0604]],\n",
      "\n",
      "        [[ 0.8374,  0.8847, -0.8982,  0.8620,  0.8183],\n",
      "         [-0.6561, -0.3845,  0.6504,  0.0444, -0.8479]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0264 cost = 0.018422\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8026, -0.7766, -0.8564,  0.7901,  0.0248],\n",
      "         [ 0.9298, -0.3532, -0.5069,  0.9116, -0.5474]],\n",
      "\n",
      "        [[-0.6612, -0.3411,  0.6779,  0.0456, -0.8683],\n",
      "         [-0.3095, -0.3417,  0.4639, -0.0435, -0.6318]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0264 cost = 0.023758\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4345,  0.2960,  0.0784, -0.4301,  0.4545],\n",
      "         [-0.3891,  0.4687,  0.0948, -0.4631,  0.4803]],\n",
      "\n",
      "        [[ 0.7718,  0.7847, -0.8387,  0.7650,  0.7821],\n",
      "         [ 0.8708,  0.8834, -0.9204,  0.8692,  0.7980]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0264 cost = 0.022457\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8030, -0.7771, -0.8567,  0.7904,  0.0245],\n",
      "         [-0.4347,  0.2964,  0.0785, -0.4306,  0.4551]],\n",
      "\n",
      "        [[-0.6615, -0.3415,  0.6781,  0.0449, -0.8685],\n",
      "         [ 0.7720,  0.7849, -0.8388,  0.7652,  0.7823]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0265 cost = 0.014708\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3886,  0.4697,  0.0950, -0.4628,  0.4809],\n",
      "         [-0.2988,  0.2358, -0.0779, -0.3712,  0.4910]],\n",
      "\n",
      "        [[ 0.8710,  0.8835, -0.9205,  0.8694,  0.7984],\n",
      "         [ 0.8379,  0.8851, -0.8985,  0.8626,  0.8189]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0265 cost = 0.016442\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9300, -0.3554, -0.5083,  0.9117, -0.5485],\n",
      "         [ 0.7353, -0.9140, -0.9131,  0.5848,  0.0603]],\n",
      "\n",
      "        [[-0.3104, -0.3425,  0.4645, -0.0441, -0.6324],\n",
      "         [-0.6564, -0.3851,  0.6508,  0.0430, -0.8481]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0265 cost = 0.028174\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7356, -0.9141, -0.9132,  0.5851,  0.0603],\n",
      "         [ 0.8035, -0.7777, -0.8571,  0.7908,  0.0242]],\n",
      "\n",
      "        [[-0.6565, -0.3853,  0.6509,  0.0426, -0.8482],\n",
      "         [-0.6619, -0.3420,  0.6784,  0.0441, -0.8687]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0266 cost = 0.011753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9301, -0.3567, -0.5092,  0.9118, -0.5491],\n",
      "         [-0.4348,  0.2979,  0.0789, -0.4320,  0.4571]],\n",
      "\n",
      "        [[-0.3108, -0.3430,  0.4648, -0.0444, -0.6328],\n",
      "         [ 0.7728,  0.7855, -0.8392,  0.7660,  0.7831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0266 cost = 0.026770\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.2992,  0.2379, -0.0777, -0.3722,  0.4922],\n",
      "         [-0.3877,  0.4714,  0.0954, -0.4623,  0.4821]],\n",
      "\n",
      "        [[ 0.8385,  0.8854, -0.8988,  0.8631,  0.8195],\n",
      "         [ 0.8714,  0.8838, -0.9206,  0.8697,  0.7991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0266 cost = 0.015487\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7364, -0.9143, -0.9135,  0.5861,  0.0602],\n",
      "         [-0.4348,  0.2986,  0.0790, -0.4326,  0.4579]],\n",
      "\n",
      "        [[-0.6567, -0.3857,  0.6511,  0.0417, -0.8484],\n",
      "         [ 0.7731,  0.7858, -0.8394,  0.7662,  0.7835]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0267 cost = 0.014155\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3000,  0.2391, -0.0775, -0.3732,  0.4928],\n",
      "         [ 0.9303, -0.3588, -0.5106,  0.9120, -0.5501]],\n",
      "\n",
      "        [[ 0.8388,  0.8856, -0.8989,  0.8634,  0.8199],\n",
      "         [-0.3114, -0.3436,  0.4653, -0.0449, -0.6332]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0267 cost = 0.026023\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8045, -0.7787, -0.8579,  0.7915,  0.0234],\n",
      "         [-0.3873,  0.4728,  0.0957, -0.4621,  0.4829]],\n",
      "\n",
      "        [[-0.6625, -0.3430,  0.6790,  0.0426, -0.8691],\n",
      "         [ 0.8717,  0.8840, -0.9207,  0.8700,  0.7996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0267 cost = 0.008353\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3012,  0.2403, -0.0772, -0.3745,  0.4935],\n",
      "         [ 0.8047, -0.7790, -0.8580,  0.7916,  0.0233]],\n",
      "\n",
      "        [[ 0.8390,  0.8858, -0.8990,  0.8637,  0.8202],\n",
      "         [-0.6626, -0.3431,  0.6791,  0.0423, -0.8692]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0268 cost = 0.015833\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4357,  0.2999,  0.0794, -0.4349,  0.4598],\n",
      "         [ 0.7373, -0.9146, -0.9138,  0.5869,  0.0601]],\n",
      "\n",
      "        [[ 0.7738,  0.7863, -0.8398,  0.7668,  0.7842],\n",
      "         [-0.6570, -0.3862,  0.6515,  0.0407, -0.8486]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0268 cost = 0.019981\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9303, -0.3610, -0.5120,  0.9121, -0.5511],\n",
      "         [-0.3873,  0.4739,  0.0962, -0.4622,  0.4836]],\n",
      "\n",
      "        [[-0.3123, -0.3445,  0.4659, -0.0456, -0.6338],\n",
      "         [ 0.8720,  0.8841, -0.9209,  0.8703,  0.8001]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0268 cost = 0.019460\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9304, -0.3617, -0.5124,  0.9121, -0.5515],\n",
      "         [-0.4366,  0.3005,  0.0796, -0.4363,  0.4607]],\n",
      "\n",
      "        [[-0.3125, -0.3447,  0.4661, -0.0458, -0.6340],\n",
      "         [ 0.7741,  0.7866, -0.8400,  0.7672,  0.7846]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0269 cost = 0.026155\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7380, -0.9148, -0.9140,  0.5877,  0.0600],\n",
      "         [-0.3874,  0.4746,  0.0964, -0.4623,  0.4840]],\n",
      "\n",
      "        [[-0.6572, -0.3865,  0.6517,  0.0399, -0.8487],\n",
      "         [ 0.8722,  0.8842, -0.9209,  0.8704,  0.8004]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0269 cost = 0.007907\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8055, -0.7800, -0.8586,  0.7922,  0.0226],\n",
      "         [-0.3045,  0.2428, -0.0765, -0.3776,  0.4948]],\n",
      "\n",
      "        [[-0.6632, -0.3440,  0.6796,  0.0410, -0.8695],\n",
      "         [ 0.8397,  0.8863, -0.8994,  0.8644,  0.8209]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0269 cost = 0.012709\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4376,  0.3013,  0.0799, -0.4380,  0.4616],\n",
      "         [-0.3051,  0.2432, -0.0764, -0.3782,  0.4951]],\n",
      "\n",
      "        [[ 0.7746,  0.7868, -0.8402,  0.7675,  0.7850],\n",
      "         [ 0.8399,  0.8863, -0.8995,  0.8645,  0.8210]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0270 cost = 0.026032\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7386, -0.9150, -0.9142,  0.5884,  0.0600],\n",
      "         [ 0.8058, -0.7803, -0.8589,  0.7924,  0.0224]],\n",
      "\n",
      "        [[-0.6573, -0.3868,  0.6519,  0.0392, -0.8488],\n",
      "         [-0.6634, -0.3442,  0.6798,  0.0406, -0.8696]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0270 cost = 0.011333\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9306, -0.3649, -0.5146,  0.9123, -0.5530],\n",
      "         [-0.3874,  0.4760,  0.0969, -0.4624,  0.4848]],\n",
      "\n",
      "        [[-0.3134, -0.3456,  0.4668, -0.0466, -0.6347],\n",
      "         [ 0.8726,  0.8845, -0.9211,  0.8708,  0.8011]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0270 cost = 0.019102\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8062, -0.7806, -0.8592,  0.7927,  0.0222],\n",
      "         [ 0.7391, -0.9151, -0.9143,  0.5889,  0.0599]],\n",
      "\n",
      "        [[-0.6636, -0.3445,  0.6800,  0.0401, -0.8698],\n",
      "         [-0.6574, -0.3869,  0.6520,  0.0388, -0.8489]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0271 cost = 0.018858\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3872,  0.4768,  0.0971, -0.4623,  0.4852],\n",
      "         [-0.4382,  0.3026,  0.0803, -0.4397,  0.4632]],\n",
      "\n",
      "        [[ 0.8727,  0.8846, -0.9212,  0.8709,  0.8013],\n",
      "         [ 0.7753,  0.7874, -0.8406,  0.7682,  0.7859]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0271 cost = 0.022987\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3059,  0.2463, -0.0762, -0.3796,  0.4965],\n",
      "         [ 0.9308, -0.3668, -0.5160,  0.9125, -0.5540]],\n",
      "\n",
      "        [[ 0.8405,  0.8868, -0.8998,  0.8652,  0.8218],\n",
      "         [-0.3140, -0.3462,  0.4672, -0.0470, -0.6351]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0271 cost = 0.025021\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8069, -0.7812, -0.8597,  0.7933,  0.0217],\n",
      "         [-0.4380,  0.3034,  0.0803, -0.4396,  0.4642]],\n",
      "\n",
      "        [[-0.6640, -0.3451,  0.6803,  0.0393, -0.8700],\n",
      "         [ 0.7757,  0.7878, -0.8408,  0.7686,  0.7863]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0272 cost = 0.013821\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7401, -0.9153, -0.9147,  0.5901,  0.0598],\n",
      "         [ 0.9309, -0.3680, -0.5169,  0.9126, -0.5545]],\n",
      "\n",
      "        [[-0.6576, -0.3875,  0.6523,  0.0377, -0.8491],\n",
      "         [-0.3144, -0.3466,  0.4675, -0.0473, -0.6353]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0272 cost = 0.020725\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3869,  0.4782,  0.0974, -0.4619,  0.4862],\n",
      "         [-0.3061,  0.2478, -0.0761, -0.3800,  0.4973]],\n",
      "\n",
      "        [[ 0.8731,  0.8848, -0.9213,  0.8713,  0.8021],\n",
      "         [ 0.8409,  0.8870, -0.9000,  0.8655,  0.8223]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0272 cost = 0.015512\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8076, -0.7818, -0.8602,  0.7939,  0.0212],\n",
      "         [-0.4378,  0.3043,  0.0805, -0.4397,  0.4654]],\n",
      "\n",
      "        [[-0.6644, -0.3457,  0.6806,  0.0384, -0.8703],\n",
      "         [ 0.7763,  0.7881, -0.8411,  0.7691,  0.7868]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0273 cost = 0.013703\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9312, -0.3698, -0.5183,  0.9128, -0.5553],\n",
      "         [-0.3868,  0.4789,  0.0976, -0.4617,  0.4867]],\n",
      "\n",
      "        [[-0.3150, -0.3473,  0.4680, -0.0478, -0.6359],\n",
      "         [ 0.8733,  0.8849, -0.9214,  0.8715,  0.8024]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0273 cost = 0.018639\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3059,  0.2493, -0.0760, -0.3802,  0.4981],\n",
      "         [ 0.7411, -0.9156, -0.9149,  0.5912,  0.0597]],\n",
      "\n",
      "        [[ 0.8413,  0.8873, -0.9002,  0.8659,  0.8227],\n",
      "         [-0.6578, -0.3880,  0.6526,  0.0367, -0.8493]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0273 cost = 0.016905\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8083, -0.7825, -0.8607,  0.7944,  0.0208],\n",
      "         [-0.3868,  0.4795,  0.0978, -0.4616,  0.4872]],\n",
      "\n",
      "        [[-0.6647, -0.3462,  0.6809,  0.0376, -0.8705],\n",
      "         [ 0.8735,  0.8850, -0.9215,  0.8717,  0.8028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0274 cost = 0.007876\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7416, -0.9157, -0.9151,  0.5917,  0.0596],\n",
      "         [ 0.9314, -0.3717, -0.5197,  0.9130, -0.5562]],\n",
      "\n",
      "        [[-0.6579, -0.3882,  0.6527,  0.0362, -0.8494],\n",
      "         [-0.3157, -0.3479,  0.4685, -0.0483, -0.6363]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0274 cost = 0.020358\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4376,  0.3056,  0.0808, -0.4400,  0.4672],\n",
      "         [-0.3063,  0.2505, -0.0758, -0.3807,  0.4988]],\n",
      "\n",
      "        [[ 0.7770,  0.7887, -0.8415,  0.7697,  0.7876],\n",
      "         [ 0.8417,  0.8875, -0.9004,  0.8663,  0.8231]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0274 cost = 0.025091\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7421, -0.9158, -0.9153,  0.5923,  0.0596],\n",
      "         [-0.3063,  0.2512, -0.0758, -0.3808,  0.4991]],\n",
      "\n",
      "        [[-0.6579, -0.3884,  0.6528,  0.0357, -0.8495],\n",
      "         [ 0.8419,  0.8876, -0.9004,  0.8664,  0.8233]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0275 cost = 0.011666\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4375,  0.3064,  0.0809, -0.4402,  0.4681],\n",
      "         [-0.3866,  0.4809,  0.0982, -0.4614,  0.4881]],\n",
      "\n",
      "        [[ 0.7774,  0.7890, -0.8417,  0.7701,  0.7881],\n",
      "         [ 0.8739,  0.8853, -0.9216,  0.8721,  0.8034]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0275 cost = 0.020417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8093, -0.7831, -0.8614,  0.7952,  0.0203],\n",
      "         [ 0.9316, -0.3736, -0.5212,  0.9132, -0.5571]],\n",
      "\n",
      "        [[-0.6653, -0.3470,  0.6814,  0.0364, -0.8708],\n",
      "         [-0.3164, -0.3486,  0.4690, -0.0488, -0.6369]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0275 cost = 0.021529\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3858,  0.4822,  0.0983, -0.4608,  0.4887],\n",
      "         [-0.3065,  0.2538, -0.0756, -0.3815,  0.5002]],\n",
      "\n",
      "        [[ 0.8741,  0.8854, -0.9217,  0.8723,  0.8038],\n",
      "         [ 0.8423,  0.8879, -0.9006,  0.8668,  0.8238]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0276 cost = 0.015078\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8097, -0.7833, -0.8616,  0.7954,  0.0201],\n",
      "         [ 0.7430, -0.9160, -0.9155,  0.5932,  0.0596]],\n",
      "\n",
      "        [[-0.6655, -0.3473,  0.6816,  0.0360, -0.8710],\n",
      "         [-0.6581, -0.3888,  0.6531,  0.0349, -0.8497]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0276 cost = 0.018095\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9318, -0.3748, -0.5223,  0.9133, -0.5577],\n",
      "         [-0.4368,  0.3084,  0.0811, -0.4403,  0.4704]],\n",
      "\n",
      "        [[-0.3170, -0.3492,  0.4695, -0.0490, -0.6373],\n",
      "         [ 0.7783,  0.7897, -0.8422,  0.7709,  0.7890]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0276 cost = 0.024523\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3848,  0.4837,  0.0985, -0.4599,  0.4896],\n",
      "         [-0.4366,  0.3088,  0.0812, -0.4403,  0.4708]],\n",
      "\n",
      "        [[ 0.8744,  0.8856, -0.9219,  0.8726,  0.8044],\n",
      "         [ 0.7784,  0.7899, -0.8423,  0.7711,  0.7892]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0277 cost = 0.022038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3055,  0.2569, -0.0756, -0.3814,  0.5016],\n",
      "         [ 0.7438, -0.9161, -0.9157,  0.5941,  0.0595]],\n",
      "\n",
      "        [[ 0.8428,  0.8882, -0.9009,  0.8674,  0.8245],\n",
      "         [-0.6582, -0.3892,  0.6533,  0.0341, -0.8498]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0277 cost = 0.016393\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8106, -0.7838, -0.8622,  0.7961,  0.0197],\n",
      "         [ 0.9320, -0.3763, -0.5235,  0.9135, -0.5583]],\n",
      "\n",
      "        [[-0.6659, -0.3480,  0.6820,  0.0350, -0.8713],\n",
      "         [-0.3175, -0.3496,  0.4699, -0.0492, -0.6376]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0277 cost = 0.021204\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3839,  0.4851,  0.0986, -0.4590,  0.4906],\n",
      "         [ 0.7443, -0.9162, -0.9159,  0.5946,  0.0594]],\n",
      "\n",
      "        [[ 0.8748,  0.8858, -0.9220,  0.8729,  0.8051],\n",
      "         [-0.6583, -0.3894,  0.6534,  0.0335, -0.8499]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0278 cost = 0.011612\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8111, -0.7841, -0.8625,  0.7965,  0.0194],\n",
      "         [-0.3051,  0.2588, -0.0755, -0.3814,  0.5026]],\n",
      "\n",
      "        [[-0.6662, -0.3484,  0.6822,  0.0344, -0.8715],\n",
      "         [ 0.8433,  0.8885, -0.9011,  0.8678,  0.8250]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0278 cost = 0.011796\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9322, -0.3778, -0.5246,  0.9137, -0.5590],\n",
      "         [-0.4353,  0.3109,  0.0814, -0.4392,  0.4734]],\n",
      "\n",
      "        [[-0.3181, -0.3502,  0.4703, -0.0495, -0.6380],\n",
      "         [ 0.7795,  0.7907, -0.8430,  0.7721,  0.7904]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0278 cost = 0.024130\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3050,  0.2598, -0.0755, -0.3815,  0.5032],\n",
      "         [ 0.8115, -0.7845, -0.8628,  0.7968,  0.0191]],\n",
      "\n",
      "        [[ 0.8435,  0.8887, -0.9013,  0.8681,  0.8253],\n",
      "         [-0.6664, -0.3488,  0.6824,  0.0339, -0.8716]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0279 cost = 0.014389\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7454, -0.9165, -0.9162,  0.5959,  0.0593],\n",
      "         [ 0.9324, -0.3791, -0.5254,  0.9138, -0.5596]],\n",
      "\n",
      "        [[-0.6585, -0.3899,  0.6537,  0.0324, -0.8501],\n",
      "         [-0.3185, -0.3506,  0.4706, -0.0498, -0.6383]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0279 cost = 0.019561\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3833,  0.4867,  0.0990, -0.4583,  0.4918],\n",
      "         [-0.4351,  0.3117,  0.0815, -0.4393,  0.4745]],\n",
      "\n",
      "        [[ 0.8753,  0.8861, -0.9222,  0.8734,  0.8060],\n",
      "         [ 0.7800,  0.7911, -0.8432,  0.7726,  0.7909]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0279 cost = 0.021573\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3053,  0.2611, -0.0753, -0.3819,  0.5040],\n",
      "         [-0.3832,  0.4871,  0.0990, -0.4580,  0.4921]],\n",
      "\n",
      "        [[ 0.8440,  0.8890, -0.9015,  0.8685,  0.8258],\n",
      "         [ 0.8754,  0.8862, -0.9223,  0.8735,  0.8062]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0280 cost = 0.013722\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9325, -0.3808, -0.5266,  0.9139, -0.5603],\n",
      "         [ 0.7460, -0.9166, -0.9164,  0.5967,  0.0593]],\n",
      "\n",
      "        [[-0.3191, -0.3512,  0.4711, -0.0503, -0.6388],\n",
      "         [-0.6586, -0.3902,  0.6539,  0.0316, -0.8502]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0280 cost = 0.024985\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8125, -0.7854, -0.8635,  0.7976,  0.0184],\n",
      "         [-0.4347,  0.3129,  0.0816, -0.4388,  0.4760]],\n",
      "\n",
      "        [[-0.6670, -0.3497,  0.6829,  0.0326, -0.8720],\n",
      "         [ 0.7806,  0.7915, -0.8436,  0.7731,  0.7916]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0280 cost = 0.012822\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3059,  0.2628, -0.0751, -0.3825,  0.5050],\n",
      "         [-0.4346,  0.3133,  0.0816, -0.4387,  0.4765]],\n",
      "\n",
      "        [[ 0.8444,  0.8893, -0.9017,  0.8689,  0.8263],\n",
      "         [ 0.7808,  0.7916, -0.8437,  0.7733,  0.7918]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0281 cost = 0.024176\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9327, -0.3826, -0.5277,  0.9140, -0.5611],\n",
      "         [ 0.7466, -0.9168, -0.9166,  0.5973,  0.0592]],\n",
      "\n",
      "        [[-0.3197, -0.3518,  0.4715, -0.0506, -0.6391],\n",
      "         [-0.6587, -0.3905,  0.6541,  0.0308, -0.8504]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0281 cost = 0.024789\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3827,  0.4891,  0.0994, -0.4573,  0.4936],\n",
      "         [ 0.8130, -0.7859, -0.8639,  0.7979,  0.0181]],\n",
      "\n",
      "        [[ 0.8759,  0.8865, -0.9225,  0.8740,  0.8073],\n",
      "         [-0.6673, -0.3501,  0.6832,  0.0318, -0.8722]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0281 cost = 0.009425\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3070,  0.2646, -0.0749, -0.3835,  0.5061],\n",
      "         [ 0.8131, -0.7861, -0.8641,  0.7981,  0.0180]],\n",
      "\n",
      "        [[ 0.8449,  0.8896, -0.9020,  0.8694,  0.8269],\n",
      "         [-0.6674, -0.3503,  0.6833,  0.0316, -0.8723]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0282 cost = 0.013997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3827,  0.4898,  0.0995, -0.4571,  0.4941],\n",
      "         [ 0.9328, -0.3846, -0.5289,  0.9142, -0.5619]],\n",
      "\n",
      "        [[ 0.8762,  0.8867, -0.9226,  0.8742,  0.8077],\n",
      "         [-0.3202, -0.3524,  0.4719, -0.0509, -0.6395]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0282 cost = 0.018092\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7475, -0.9171, -0.9169,  0.5983,  0.0591],\n",
      "         [-0.4348,  0.3152,  0.0818, -0.4393,  0.4790]],\n",
      "\n",
      "        [[-0.6589, -0.3911,  0.6544,  0.0296, -0.8505],\n",
      "         [ 0.7818,  0.7924, -0.8443,  0.7742,  0.7929]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0282 cost = 0.012417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3828,  0.4904,  0.0997, -0.4571,  0.4945],\n",
      "         [ 0.9329, -0.3859, -0.5297,  0.9143, -0.5624]],\n",
      "\n",
      "        [[ 0.8763,  0.8868, -0.9227,  0.8744,  0.8081],\n",
      "         [-0.3206, -0.3528,  0.4722, -0.0513, -0.6398]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0283 cost = 0.017984\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3084,  0.2663, -0.0745, -0.3848,  0.5072],\n",
      "         [ 0.7479, -0.9172, -0.9170,  0.5988,  0.0590]],\n",
      "\n",
      "        [[ 0.8454,  0.8899, -0.9022,  0.8699,  0.8275],\n",
      "         [-0.6590, -0.3913,  0.6546,  0.0291, -0.8506]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0283 cost = 0.015540\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4351,  0.3160,  0.0820, -0.4399,  0.4801],\n",
      "         [ 0.8140, -0.7871, -0.8648,  0.7988,  0.0173]],\n",
      "\n",
      "        [[ 0.7822,  0.7927, -0.8445,  0.7745,  0.7933],\n",
      "         [-0.6680, -0.3512,  0.6838,  0.0302, -0.8726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0283 cost = 0.014691\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3832,  0.4911,  0.1000, -0.4572,  0.4950],\n",
      "         [ 0.7484, -0.9173, -0.9172,  0.5993,  0.0590]],\n",
      "\n",
      "        [[ 0.8766,  0.8869, -0.9228,  0.8746,  0.8085],\n",
      "         [-0.6592, -0.3915,  0.6547,  0.0285, -0.8507]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0284 cost = 0.011015\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9331, -0.3881, -0.5312,  0.9145, -0.5634],\n",
      "         [-0.3099,  0.2677, -0.0741, -0.3861,  0.5079]],\n",
      "\n",
      "        [[-0.3214, -0.3537,  0.4728, -0.0520, -0.6404],\n",
      "         [ 0.8458,  0.8902, -0.9024,  0.8703,  0.8279]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0284 cost = 0.020900\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8146, -0.7875, -0.8651,  0.7992,  0.0169],\n",
      "         [-0.4361,  0.3166,  0.0822, -0.4412,  0.4811]],\n",
      "\n",
      "        [[-0.6683, -0.3518,  0.6841,  0.0294, -0.8728],\n",
      "         [ 0.7826,  0.7930, -0.8447,  0.7749,  0.7938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0284 cost = 0.012400\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4364,  0.3168,  0.0823, -0.4416,  0.4814],\n",
      "         [ 0.7492, -0.9175, -0.9174,  0.6002,  0.0589]],\n",
      "\n",
      "        [[ 0.7828,  0.7931, -0.8448,  0.7750,  0.7940],\n",
      "         [-0.6593, -0.3918,  0.6549,  0.0277, -0.8508]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0285 cost = 0.017316\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3839,  0.4917,  0.1005, -0.4574,  0.4956],\n",
      "         [-0.3113,  0.2689, -0.0737, -0.3874,  0.5085]],\n",
      "\n",
      "        [[ 0.8770,  0.8871, -0.9229,  0.8750,  0.8091],\n",
      "         [ 0.8462,  0.8904, -0.9026,  0.8706,  0.8284]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0285 cost = 0.013909\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8151, -0.7879, -0.8654,  0.7995,  0.0166],\n",
      "         [ 0.9332, -0.3901, -0.5326,  0.9146, -0.5643]],\n",
      "\n",
      "        [[-0.6686, -0.3522,  0.6844,  0.0287, -0.8729],\n",
      "         [-0.3220, -0.3544,  0.4733, -0.0525, -0.6408]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0285 cost = 0.019843\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3119,  0.2698, -0.0734, -0.3881,  0.5090],\n",
      "         [ 0.7500, -0.9177, -0.9176,  0.6011,  0.0588]],\n",
      "\n",
      "        [[ 0.8464,  0.8906, -0.9027,  0.8709,  0.8287],\n",
      "         [-0.6595, -0.3921,  0.6551,  0.0269, -0.8509]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0286 cost = 0.015167\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8155, -0.7882, -0.8656,  0.7998,  0.0164],\n",
      "         [ 0.9333, -0.3911, -0.5332,  0.9147, -0.5648]],\n",
      "\n",
      "        [[-0.6689, -0.3526,  0.6846,  0.0281, -0.8731],\n",
      "         [-0.3224, -0.3548,  0.4735, -0.0528, -0.6411]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0286 cost = 0.019734\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3846,  0.4924,  0.1010, -0.4576,  0.4962],\n",
      "         [-0.4380,  0.3177,  0.0827, -0.4439,  0.4830]],\n",
      "\n",
      "        [[ 0.8774,  0.8873, -0.9231,  0.8753,  0.8098],\n",
      "         [ 0.7836,  0.7936, -0.8452,  0.7758,  0.7948]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0286 cost = 0.020541\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3847,  0.4927,  0.1011, -0.4575,  0.4964],\n",
      "         [ 0.7509, -0.9178, -0.9179,  0.6022,  0.0588]],\n",
      "\n",
      "        [[ 0.8775,  0.8874, -0.9232,  0.8754,  0.8100],\n",
      "         [-0.6596, -0.3925,  0.6553,  0.0260, -0.8510]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0287 cost = 0.010717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9335, -0.3926, -0.5343,  0.9149, -0.5655],\n",
      "         [-0.4382,  0.3182,  0.0828, -0.4439,  0.4838]],\n",
      "\n",
      "        [[-0.3229, -0.3555,  0.4740, -0.0533, -0.6416],\n",
      "         [ 0.7840,  0.7939, -0.8455,  0.7761,  0.7952]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0287 cost = 0.022481\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8164, -0.7888, -0.8662,  0.8005,  0.0158],\n",
      "         [-0.3130,  0.2719, -0.0728, -0.3892,  0.5102]],\n",
      "\n",
      "        [[-0.6693, -0.3533,  0.6850,  0.0270, -0.8733],\n",
      "         [ 0.8471,  0.8910, -0.9031,  0.8715,  0.8294]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0287 cost = 0.010864\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3851,  0.4932,  0.1014, -0.4574,  0.4969],\n",
      "         [ 0.8166, -0.7890, -0.8663,  0.8007,  0.0157]],\n",
      "\n",
      "        [[ 0.8778,  0.8875, -0.9233,  0.8757,  0.8105],\n",
      "         [-0.6694, -0.3535,  0.6851,  0.0268, -0.8734]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0288 cost = 0.008908\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7520, -0.9180, -0.9182,  0.6034,  0.0587],\n",
      "         [ 0.9337, -0.3944, -0.5354,  0.9151, -0.5662]],\n",
      "\n",
      "        [[-0.6598, -0.3929,  0.6556,  0.0249, -0.8512],\n",
      "         [-0.3235, -0.3560,  0.4744, -0.0538, -0.6419]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0288 cost = 0.018162\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4384,  0.3189,  0.0831, -0.4442,  0.4850],\n",
      "         [-0.3134,  0.2727, -0.0726, -0.3896,  0.5108]],\n",
      "\n",
      "        [[ 0.7846,  0.7943, -0.8458,  0.7767,  0.7958],\n",
      "         [ 0.8474,  0.8912, -0.9032,  0.8718,  0.8298]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0288 cost = 0.022371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3134,  0.2734, -0.0725, -0.3897,  0.5111],\n",
      "         [ 0.8173, -0.7895, -0.8667,  0.8012,  0.0152]],\n",
      "\n",
      "        [[ 0.8476,  0.8913, -0.9033,  0.8720,  0.8299],\n",
      "         [-0.6698, -0.3540,  0.6854,  0.0260, -0.8736]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0289 cost = 0.013171\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4386,  0.3195,  0.0832, -0.4445,  0.4857],\n",
      "         [-0.3854,  0.4940,  0.1018, -0.4572,  0.4977]],\n",
      "\n",
      "        [[ 0.7850,  0.7946, -0.8460,  0.7770,  0.7962],\n",
      "         [ 0.8782,  0.8877, -0.9235,  0.8760,  0.8112]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0289 cost = 0.018057\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7529, -0.9182, -0.9184,  0.6044,  0.0587],\n",
      "         [ 0.9339, -0.3960, -0.5367,  0.9152, -0.5670]],\n",
      "\n",
      "        [[-0.6599, -0.3933,  0.6558,  0.0240, -0.8513],\n",
      "         [-0.3242, -0.3567,  0.4749, -0.0541, -0.6424]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0289 cost = 0.017985\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3140,  0.2758, -0.0722, -0.3906,  0.5121],\n",
      "         [-0.3849,  0.4950,  0.1019, -0.4566,  0.4982]],\n",
      "\n",
      "        [[ 0.8480,  0.8915, -0.9035,  0.8724,  0.8304],\n",
      "         [ 0.8784,  0.8879, -0.9235,  0.8763,  0.8116]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0290 cost = 0.012530\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8179, -0.7897, -0.8671,  0.8016,  0.0150],\n",
      "         [-0.4388,  0.3210,  0.0834, -0.4452,  0.4875]],\n",
      "\n",
      "        [[-0.6702, -0.3546,  0.6857,  0.0251, -0.8739],\n",
      "         [ 0.7857,  0.7951, -0.8464,  0.7777,  0.7969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0290 cost = 0.011819\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9340, -0.3969, -0.5374,  0.9152, -0.5674],\n",
      "         [ 0.7534, -0.9182, -0.9186,  0.6049,  0.0587]],\n",
      "\n",
      "        [[-0.3246, -0.3570,  0.4753, -0.0542, -0.6427],\n",
      "         [-0.6601, -0.3935,  0.6560,  0.0234, -0.8514]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0290 cost = 0.023060\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3842,  0.4964,  0.1022, -0.4560,  0.4990],\n",
      "         [ 0.9340, -0.3975, -0.5377,  0.9153, -0.5677]],\n",
      "\n",
      "        [[ 0.8787,  0.8880, -0.9237,  0.8765,  0.8121],\n",
      "         [-0.3248, -0.3572,  0.4754, -0.0543, -0.6428]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0291 cost = 0.016841\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3154,  0.2788, -0.0717, -0.3920,  0.5134],\n",
      "         [-0.4390,  0.3222,  0.0836, -0.4460,  0.4889]],\n",
      "\n",
      "        [[ 0.8485,  0.8919, -0.9038,  0.8729,  0.8310],\n",
      "         [ 0.7862,  0.7955, -0.8467,  0.7782,  0.7975]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0291 cost = 0.022395\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7540, -0.9184, -0.9188,  0.6055,  0.0586],\n",
      "         [ 0.8185, -0.7901, -0.8675,  0.8020,  0.0147]],\n",
      "\n",
      "        [[-0.6602, -0.3939,  0.6562,  0.0227, -0.8516],\n",
      "         [-0.6706, -0.3552,  0.6861,  0.0242, -0.8741]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0291 cost = 0.009498\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9341, -0.3989, -0.5387,  0.9153, -0.5683],\n",
      "         [-0.4393,  0.3231,  0.0837, -0.4466,  0.4899]],\n",
      "\n",
      "        [[-0.3254, -0.3579,  0.4758, -0.0547, -0.6432],\n",
      "         [ 0.7865,  0.7958, -0.8469,  0.7785,  0.7979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0292 cost = 0.021686\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8187, -0.7903, -0.8677,  0.8022,  0.0145],\n",
      "         [ 0.7544, -0.9185, -0.9189,  0.6059,  0.0586]],\n",
      "\n",
      "        [[-0.6708, -0.3556,  0.6863,  0.0237, -0.8742],\n",
      "         [-0.6603, -0.3941,  0.6563,  0.0222, -0.8516]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0292 cost = 0.015999\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3168,  0.2813, -0.0713, -0.3933,  0.5146],\n",
      "         [-0.3837,  0.4983,  0.1026, -0.4553,  0.5002]],\n",
      "\n",
      "        [[ 0.8489,  0.8922, -0.9040,  0.8733,  0.8315],\n",
      "         [ 0.8791,  0.8883, -0.9238,  0.8769,  0.8129]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0292 cost = 0.012195\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9342, -0.4005, -0.5398,  0.9154, -0.5690],\n",
      "         [-0.3172,  0.2819, -0.0712, -0.3938,  0.5149]],\n",
      "\n",
      "        [[-0.3260, -0.3585,  0.4763, -0.0552, -0.6437],\n",
      "         [ 0.8491,  0.8923, -0.9041,  0.8735,  0.8317]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0293 cost = 0.019501\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8192, -0.7908, -0.8681,  0.8025,  0.0141],\n",
      "         [ 0.7551, -0.9187, -0.9191,  0.6067,  0.0584]],\n",
      "\n",
      "        [[-0.6712, -0.3562,  0.6866,  0.0228, -0.8745],\n",
      "         [-0.6605, -0.3945,  0.6565,  0.0213, -0.8518]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0293 cost = 0.015871\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3834,  0.4993,  0.1028, -0.4551,  0.5008],\n",
      "         [-0.4397,  0.3247,  0.0839, -0.4479,  0.4919]],\n",
      "\n",
      "        [[ 0.8793,  0.8884, -0.9239,  0.8771,  0.8133],\n",
      "         [ 0.7872,  0.7963, -0.8473,  0.7791,  0.7987]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0293 cost = 0.019542\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9343, -0.4024, -0.5410,  0.9155, -0.5698],\n",
      "         [ 0.7556, -0.9188, -0.9193,  0.6072,  0.0583]],\n",
      "\n",
      "        [[-0.3265, -0.3592,  0.4767, -0.0557, -0.6441],\n",
      "         [-0.6606, -0.3949,  0.6567,  0.0207, -0.8519]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0294 cost = 0.022448\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8198, -0.7915, -0.8685,  0.8030,  0.0135],\n",
      "         [-0.3831,  0.5000,  0.1028, -0.4547,  0.5013]],\n",
      "\n",
      "        [[-0.6715, -0.3569,  0.6869,  0.0219, -0.8747],\n",
      "         [ 0.8795,  0.8885, -0.9240,  0.8773,  0.8136]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0294 cost = 0.006563\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3178,  0.2841, -0.0711, -0.3946,  0.5162],\n",
      "         [-0.4391,  0.3257,  0.0839, -0.4474,  0.4930]],\n",
      "\n",
      "        [[ 0.8496,  0.8926, -0.9043,  0.8740,  0.8324],\n",
      "         [ 0.7876,  0.7966, -0.8475,  0.7795,  0.7991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0294 cost = 0.021868\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7563, -0.9191, -0.9195,  0.6082,  0.0582],\n",
      "         [-0.4391,  0.3262,  0.0839, -0.4475,  0.4935]],\n",
      "\n",
      "        [[-0.6608, -0.3954,  0.6570,  0.0196, -0.8521],\n",
      "         [ 0.7878,  0.7968, -0.8477,  0.7797,  0.7994]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0295 cost = 0.011238\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8203, -0.7920, -0.8689,  0.8033,  0.0131],\n",
      "         [ 0.9345, -0.4049, -0.5425,  0.9157, -0.5708]],\n",
      "\n",
      "        [[-0.6718, -0.3575,  0.6872,  0.0212, -0.8749],\n",
      "         [-0.3272, -0.3600,  0.4772, -0.0561, -0.6445]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0295 cost = 0.018379\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3184,  0.2856, -0.0709, -0.3953,  0.5171],\n",
      "         [-0.3826,  0.5013,  0.1030, -0.4542,  0.5023]],\n",
      "\n",
      "        [[ 0.8500,  0.8929, -0.9045,  0.8744,  0.8329],\n",
      "         [ 0.8798,  0.8887, -0.9241,  0.8776,  0.8143]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0295 cost = 0.011874\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3189,  0.2862, -0.0707, -0.3957,  0.5175],\n",
      "         [ 0.7569, -0.9193, -0.9197,  0.6089,  0.0581]],\n",
      "\n",
      "        [[ 0.8501,  0.8930, -0.9046,  0.8745,  0.8330],\n",
      "         [-0.6610, -0.3957,  0.6572,  0.0188, -0.8522]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0296 cost = 0.013887\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4394,  0.3276,  0.0840, -0.4484,  0.4955],\n",
      "         [ 0.9346, -0.4063, -0.5434,  0.9157, -0.5714]],\n",
      "\n",
      "        [[ 0.7885,  0.7974, -0.8481,  0.7804,  0.8002],\n",
      "         [-0.3277, -0.3606,  0.4776, -0.0564, -0.6449]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0296 cost = 0.020790\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8208, -0.7926, -0.8693,  0.8036,  0.0127],\n",
      "         [-0.3825,  0.5023,  0.1032, -0.4540,  0.5030]],\n",
      "\n",
      "        [[-0.6723, -0.3583,  0.6876,  0.0202, -0.8752],\n",
      "         [ 0.8801,  0.8889, -0.9242,  0.8779,  0.8148]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0296 cost = 0.006433\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9346, -0.4071, -0.5439,  0.9158, -0.5717],\n",
      "         [ 0.8209, -0.7928, -0.8694,  0.8037,  0.0126]],\n",
      "\n",
      "        [[-0.3280, -0.3610,  0.4778, -0.0567, -0.6451],\n",
      "         [-0.6724, -0.3585,  0.6877,  0.0199, -0.8752]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0297 cost = 0.018216\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4402,  0.3285,  0.0841, -0.4495,  0.4969],\n",
      "         [-0.3213,  0.2886, -0.0702, -0.3976,  0.5186]],\n",
      "\n",
      "        [[ 0.7889,  0.7977, -0.8483,  0.7807,  0.8007],\n",
      "         [ 0.8506,  0.8934, -0.9048,  0.8750,  0.8336]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0297 cost = 0.020799\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3826,  0.5033,  0.1034, -0.4539,  0.5036],\n",
      "         [ 0.7578, -0.9195, -0.9201,  0.6098,  0.0580]],\n",
      "\n",
      "        [[ 0.8803,  0.8890, -0.9243,  0.8781,  0.8153],\n",
      "         [-0.6613, -0.3964,  0.6576,  0.0175, -0.8524]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0297 cost = 0.009794\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9347, -0.4085, -0.5450,  0.9158, -0.5724],\n",
      "         [-0.4405,  0.3293,  0.0842, -0.4501,  0.4978]],\n",
      "\n",
      "        [[-0.3285, -0.3616,  0.4782, -0.0571, -0.6455],\n",
      "         [ 0.7893,  0.7979, -0.8485,  0.7810,  0.8010]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0298 cost = 0.020691\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8215, -0.7933, -0.8699,  0.8041,  0.0121],\n",
      "         [-0.3826,  0.5039,  0.1036, -0.4537,  0.5040]],\n",
      "\n",
      "        [[-0.6728, -0.3592,  0.6880,  0.0189, -0.8755],\n",
      "         [ 0.8805,  0.8891, -0.9244,  0.8782,  0.8156]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0298 cost = 0.006343\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3223,  0.2912, -0.0699, -0.3987,  0.5197],\n",
      "         [ 0.7584, -0.9197, -0.9202,  0.6105,  0.0579]],\n",
      "\n",
      "        [[ 0.8510,  0.8936, -0.9050,  0.8754,  0.8342],\n",
      "         [-0.6614, -0.3967,  0.6578,  0.0168, -0.8525]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0298 cost = 0.013558\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4409,  0.3302,  0.0844, -0.4509,  0.4989],\n",
      "         [-0.3826,  0.5045,  0.1038, -0.4536,  0.5043]],\n",
      "\n",
      "        [[ 0.7898,  0.7983, -0.8488,  0.7815,  0.8015],\n",
      "         [ 0.8806,  0.8892, -0.9245,  0.8784,  0.8159]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0299 cost = 0.016497\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3231,  0.2926, -0.0697, -0.3994,  0.5204],\n",
      "         [ 0.8219, -0.7936, -0.8702,  0.8044,  0.0119]],\n",
      "\n",
      "        [[ 0.8513,  0.8938, -0.9052,  0.8757,  0.8345],\n",
      "         [-0.6731, -0.3596,  0.6883,  0.0183, -0.8756]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0299 cost = 0.012001\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9349, -0.4106, -0.5466,  0.9159, -0.5733],\n",
      "         [ 0.7589, -0.9198, -0.9204,  0.6111,  0.0579]],\n",
      "\n",
      "        [[-0.3292, -0.3622,  0.4787, -0.0574, -0.6459],\n",
      "         [-0.6615, -0.3969,  0.6579,  0.0161, -0.8526]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0299 cost = 0.021527\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7592, -0.9199, -0.9205,  0.6113,  0.0579],\n",
      "         [-0.3240,  0.2942, -0.0694, -0.4003,  0.5210]],\n",
      "\n",
      "        [[-0.6616, -0.3971,  0.6580,  0.0159, -0.8527],\n",
      "         [ 0.8515,  0.8940, -0.9053,  0.8759,  0.8348]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0300 cost = 0.009438\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4417,  0.3319,  0.0846, -0.4525,  0.5009],\n",
      "         [ 0.9349, -0.4115, -0.5472,  0.9159, -0.5737]],\n",
      "\n",
      "        [[ 0.7905,  0.7989, -0.8493,  0.7822,  0.8023],\n",
      "         [-0.3294, -0.3625,  0.4790, -0.0575, -0.6461]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0300 cost = 0.020145\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8224, -0.7940, -0.8706,  0.8047,  0.0116],\n",
      "         [-0.3817,  0.5066,  0.1041, -0.4527,  0.5054]],\n",
      "\n",
      "        [[-0.6734, -0.3602,  0.6886,  0.0174, -0.8759],\n",
      "         [ 0.8810,  0.8895, -0.9246,  0.8788,  0.8166]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0300 cost = 0.006226\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3253,  0.2960, -0.0691, -0.4014,  0.5218],\n",
      "         [ 0.8225, -0.7941, -0.8707,  0.8048,  0.0115]],\n",
      "\n",
      "        [[ 0.8518,  0.8942, -0.9055,  0.8763,  0.8351],\n",
      "         [-0.6735, -0.3604,  0.6887,  0.0172, -0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0301 cost = 0.011822\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3817,  0.5072,  0.1043, -0.4526,  0.5057],\n",
      "         [ 0.7600, -0.9201, -0.9207,  0.6122,  0.0578]],\n",
      "\n",
      "        [[ 0.8812,  0.8895, -0.9247,  0.8789,  0.8169],\n",
      "         [-0.6618, -0.3975,  0.6582,  0.0149, -0.8528]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0301 cost = 0.009511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9350, -0.4131, -0.5484,  0.9160, -0.5745],\n",
      "         [-0.4428,  0.3331,  0.0849, -0.4543,  0.5023]],\n",
      "\n",
      "        [[-0.3300, -0.3632,  0.4794, -0.0580, -0.6465],\n",
      "         [ 0.7910,  0.7993, -0.8496,  0.7827,  0.8029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0301 cost = 0.020145\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7605, -0.9202, -0.9209,  0.6127,  0.0577],\n",
      "         [-0.3268,  0.2977, -0.0688, -0.4027,  0.5225]],\n",
      "\n",
      "        [[-0.6619, -0.3977,  0.6584,  0.0144, -0.8529],\n",
      "         [ 0.8521,  0.8944, -0.9056,  0.8766,  0.8355]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0302 cost = 0.009286\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8230, -0.7946, -0.8711,  0.8051,  0.0111],\n",
      "         [-0.4434,  0.3336,  0.0850, -0.4552,  0.5029]],\n",
      "\n",
      "        [[-0.6739, -0.3610,  0.6890,  0.0163, -0.8761],\n",
      "         [ 0.7913,  0.7995, -0.8497,  0.7829,  0.8032]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0302 cost = 0.010709\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9350, -0.4144, -0.5493,  0.9161, -0.5751],\n",
      "         [-0.3818,  0.5081,  0.1047, -0.4525,  0.5063]],\n",
      "\n",
      "        [[-0.3304, -0.3636,  0.4797, -0.0584, -0.6468],\n",
      "         [ 0.8815,  0.8897, -0.9248,  0.8792,  0.8174]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0302 cost = 0.014677\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8233, -0.7949, -0.8713,  0.8053,  0.0108],\n",
      "         [-0.3281,  0.2989, -0.0684, -0.4038,  0.5231]],\n",
      "\n",
      "        [[-0.6741, -0.3612,  0.6892,  0.0159, -0.8762],\n",
      "         [ 0.8524,  0.8946, -0.9058,  0.8769,  0.8358]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0303 cost = 0.009530\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9351, -0.4156, -0.5500,  0.9161, -0.5757],\n",
      "         [ 0.7612, -0.9204, -0.9211,  0.6136,  0.0576]],\n",
      "\n",
      "        [[-0.3307, -0.3639,  0.4799, -0.0587, -0.6470],\n",
      "         [-0.6620, -0.3980,  0.6586,  0.0136, -0.8530]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0303 cost = 0.020971\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3819,  0.5086,  0.1050, -0.4526,  0.5067],\n",
      "         [-0.4443,  0.3343,  0.0853, -0.4567,  0.5038]],\n",
      "\n",
      "        [[ 0.8816,  0.8898, -0.9249,  0.8794,  0.8176],\n",
      "         [ 0.7917,  0.7998, -0.8499,  0.7833,  0.8036]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0303 cost = 0.018318\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9352, -0.4170, -0.5509,  0.9163, -0.5763],\n",
      "         [-0.3819,  0.5089,  0.1051, -0.4525,  0.5069]],\n",
      "\n",
      "        [[-0.3310, -0.3643,  0.4802, -0.0591, -0.6473],\n",
      "         [ 0.8817,  0.8898, -0.9249,  0.8794,  0.8178]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0304 cost = 0.014504\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8239, -0.7958, -0.8718,  0.8058,  0.0100],\n",
      "         [-0.4442,  0.3348,  0.0854, -0.4566,  0.5044]],\n",
      "\n",
      "        [[-0.6745, -0.3618,  0.6895,  0.0150, -0.8765],\n",
      "         [ 0.7920,  0.8000, -0.8501,  0.7835,  0.8039]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0304 cost = 0.010558\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3291,  0.3002, -0.0682, -0.4045,  0.5240],\n",
      "         [ 0.7620, -0.9207, -0.9214,  0.6146,  0.0574]],\n",
      "\n",
      "        [[ 0.8529,  0.8949, -0.9060,  0.8773,  0.8363],\n",
      "         [-0.6622, -0.3984,  0.6588,  0.0127, -0.8532]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0304 cost = 0.012895\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8242, -0.7962, -0.8721,  0.8061,  0.0097],\n",
      "         [ 0.7622, -0.9207, -0.9215,  0.6148,  0.0574]],\n",
      "\n",
      "        [[-0.6746, -0.3620,  0.6897,  0.0145, -0.8766],\n",
      "         [-0.6622, -0.3985,  0.6588,  0.0125, -0.8532]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0305 cost = 0.014570\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9354, -0.4198, -0.5526,  0.9165, -0.5775],\n",
      "         [-0.3294,  0.3007, -0.0681, -0.4049,  0.5243]],\n",
      "\n",
      "        [[-0.3317, -0.3650,  0.4806, -0.0597, -0.6477],\n",
      "         [ 0.8531,  0.8950, -0.9061,  0.8775,  0.8365]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0305 cost = 0.017678\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3819,  0.5098,  0.1054, -0.4523,  0.5077],\n",
      "         [-0.4442,  0.3356,  0.0856, -0.4570,  0.5054]],\n",
      "\n",
      "        [[ 0.8821,  0.8900, -0.9251,  0.8797,  0.8184],\n",
      "         [ 0.7924,  0.8003, -0.8503,  0.7839,  0.8043]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0305 cost = 0.018124\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9355, -0.4213, -0.5534,  0.9166, -0.5782],\n",
      "         [-0.4441,  0.3359,  0.0856, -0.4569,  0.5057]],\n",
      "\n",
      "        [[-0.3320, -0.3654,  0.4808, -0.0600, -0.6480],\n",
      "         [ 0.7926,  0.8005, -0.8504,  0.7840,  0.8045]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0306 cost = 0.019480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3296,  0.3014, -0.0681, -0.4051,  0.5249],\n",
      "         [ 0.7631, -0.9211, -0.9218,  0.6160,  0.0572]],\n",
      "\n",
      "        [[ 0.8534,  0.8952, -0.9062,  0.8778,  0.8369],\n",
      "         [-0.6624, -0.3989,  0.6591,  0.0113, -0.8534]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0306 cost = 0.012733\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8250, -0.7973, -0.8728,  0.8068,  0.0086],\n",
      "         [-0.3818,  0.5104,  0.1056, -0.4521,  0.5083]],\n",
      "\n",
      "        [[-0.6751, -0.3627,  0.6901,  0.0133, -0.8768],\n",
      "         [ 0.8823,  0.8902, -0.9252,  0.8800,  0.8189]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0306 cost = 0.005929\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3300,  0.3019, -0.0680, -0.4054,  0.5253],\n",
      "         [-0.3819,  0.5106,  0.1057, -0.4520,  0.5085]],\n",
      "\n",
      "        [[ 0.8536,  0.8953, -0.9064,  0.8780,  0.8372],\n",
      "         [ 0.8824,  0.8902, -0.9252,  0.8800,  0.8190]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0307 cost = 0.010751\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9357, -0.4239, -0.5549,  0.9168, -0.5792],\n",
      "         [ 0.7637, -0.9212, -0.9220,  0.6167,  0.0571]],\n",
      "\n",
      "        [[-0.3325, -0.3660,  0.4812, -0.0605, -0.6483],\n",
      "         [-0.6625, -0.3992,  0.6592,  0.0106, -0.8535]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0307 cost = 0.020277\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8254, -0.7979, -0.8731,  0.8071,  0.0082],\n",
      "         [-0.4443,  0.3373,  0.0858, -0.4572,  0.5074]],\n",
      "\n",
      "        [[-0.6754, -0.3631,  0.6903,  0.0127, -0.8770],\n",
      "         [ 0.7933,  0.8010, -0.8508,  0.7846,  0.8053]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0307 cost = 0.010304\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9358, -0.4252, -0.5557,  0.9169, -0.5798],\n",
      "         [ 0.7641, -0.9214, -0.9222,  0.6172,  0.0571]],\n",
      "\n",
      "        [[-0.3328, -0.3663,  0.4814, -0.0608, -0.6485],\n",
      "         [-0.6626, -0.3994,  0.6593,  0.0101, -0.8536]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0308 cost = 0.020163\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4443,  0.3377,  0.0859, -0.4574,  0.5079],\n",
      "         [-0.3819,  0.5116,  0.1059, -0.4519,  0.5092]],\n",
      "\n",
      "        [[ 0.7935,  0.8011, -0.8509,  0.7848,  0.8055],\n",
      "         [ 0.8827,  0.8904, -0.9253,  0.8803,  0.8195]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0308 cost = 0.015352\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8259, -0.7985, -0.8736,  0.8075,  0.0076],\n",
      "         [-0.3313,  0.3036, -0.0677, -0.4064,  0.5265]],\n",
      "\n",
      "        [[-0.6757, -0.3635,  0.6906,  0.0120, -0.8771],\n",
      "         [ 0.8541,  0.8956, -0.9066,  0.8785,  0.8377]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0308 cost = 0.009128\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9359, -0.4274, -0.5569,  0.9170, -0.5807],\n",
      "         [-0.4441,  0.3385,  0.0860, -0.4575,  0.5087]],\n",
      "\n",
      "        [[-0.3332, -0.3668,  0.4817, -0.0612, -0.6488],\n",
      "         [ 0.7938,  0.8014, -0.8510,  0.7851,  0.8058]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0309 cost = 0.019027\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3811,  0.5126,  0.1061, -0.4512,  0.5099],\n",
      "         [ 0.7650, -0.9217, -0.9225,  0.6184,  0.0569]],\n",
      "\n",
      "        [[ 0.8829,  0.8906, -0.9254,  0.8805,  0.8199],\n",
      "         [-0.6628, -0.3998,  0.6596,  0.0089, -0.8538]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0309 cost = 0.008929\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3317,  0.3047, -0.0676, -0.4068,  0.5272],\n",
      "         [ 0.8262, -0.7991, -0.8739,  0.8078,  0.0070]],\n",
      "\n",
      "        [[ 0.8544,  0.8958, -0.9067,  0.8788,  0.8381],\n",
      "         [-0.6760, -0.3639,  0.6908,  0.0113, -0.8773]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0309 cost = 0.011050\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7654, -0.9219, -0.9226,  0.6189,  0.0568],\n",
      "         [-0.3321,  0.3051, -0.0675, -0.4071,  0.5274]],\n",
      "\n",
      "        [[-0.6629, -0.4000,  0.6597,  0.0084, -0.8539],\n",
      "         [ 0.8545,  0.8959, -0.9068,  0.8789,  0.8382]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0310 cost = 0.008755\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3807,  0.5134,  0.1063, -0.4509,  0.5105],\n",
      "         [ 0.8265, -0.7995, -0.8741,  0.8080,  0.0067]],\n",
      "\n",
      "        [[ 0.8831,  0.8907, -0.9255,  0.8807,  0.8203],\n",
      "         [-0.6761, -0.3642,  0.6910,  0.0109, -0.8774]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0310 cost = 0.007441\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4444,  0.3399,  0.0863, -0.4586,  0.5103],\n",
      "         [ 0.9360, -0.4305, -0.5586,  0.9172, -0.5820]],\n",
      "\n",
      "        [[ 0.7945,  0.8019, -0.8514,  0.7857,  0.8065],\n",
      "         [-0.3338, -0.3675,  0.4822, -0.0617, -0.6492]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0310 cost = 0.018655\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7660, -0.9220, -0.9228,  0.6196,  0.0567],\n",
      "         [-0.3807,  0.5138,  0.1065, -0.4508,  0.5108]],\n",
      "\n",
      "        [[-0.6630, -0.4003,  0.6598,  0.0077, -0.8540],\n",
      "         [ 0.8832,  0.8908, -0.9255,  0.8808,  0.8205]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0311 cost = 0.005580\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3335,  0.3066, -0.0670, -0.4083,  0.5282],\n",
      "         [ 0.9361, -0.4314, -0.5592,  0.9172, -0.5825]],\n",
      "\n",
      "        [[ 0.8549,  0.8961, -0.9070,  0.8793,  0.8386],\n",
      "         [-0.3341, -0.3678,  0.4824, -0.0619, -0.6494]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0311 cost = 0.018044\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4451,  0.3405,  0.0865, -0.4597,  0.5111],\n",
      "         [ 0.8270, -0.8000, -0.8745,  0.8084,  0.0062]],\n",
      "\n",
      "        [[ 0.7948,  0.8021, -0.8516,  0.7859,  0.8068],\n",
      "         [-0.6765, -0.3647,  0.6913,  0.0100, -0.8776]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0311 cost = 0.011673\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7665, -0.9222, -0.9230,  0.6202,  0.0566],\n",
      "         [-0.3345,  0.3074, -0.0667, -0.4091,  0.5286]],\n",
      "\n",
      "        [[-0.6631, -0.4006,  0.6600,  0.0071, -0.8541],\n",
      "         [ 0.8550,  0.8962, -0.9071,  0.8794,  0.8388]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0312 cost = 0.008626\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9361, -0.4326, -0.5600,  0.9173, -0.5830],\n",
      "         [-0.3810,  0.5145,  0.1069, -0.4508,  0.5113]],\n",
      "\n",
      "        [[-0.3345, -0.3683,  0.4827, -0.0624, -0.6498],\n",
      "         [ 0.8835,  0.8909, -0.9256,  0.8811,  0.8209]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0312 cost = 0.013526\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8273, -0.8004, -0.8747,  0.8086,  0.0058],\n",
      "         [-0.4461,  0.3410,  0.0867, -0.4611,  0.5118]],\n",
      "\n",
      "        [[-0.6767, -0.3652,  0.6915,  0.0094, -0.8777],\n",
      "         [ 0.7951,  0.8023, -0.8517,  0.7862,  0.8071]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0312 cost = 0.009936\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8274, -0.8005, -0.8748,  0.8087,  0.0057],\n",
      "         [-0.3812,  0.5149,  0.1071, -0.4509,  0.5115]],\n",
      "\n",
      "        [[-0.6768, -0.3653,  0.6916,  0.0092, -0.8778],\n",
      "         [ 0.8836,  0.8909, -0.9257,  0.8812,  0.8211]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0313 cost = 0.005642\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3364,  0.3088, -0.0662, -0.4105,  0.5292],\n",
      "         [ 0.9362, -0.4340, -0.5609,  0.9174, -0.5837]],\n",
      "\n",
      "        [[ 0.8554,  0.8964, -0.9072,  0.8798,  0.8391],\n",
      "         [-0.3350, -0.3689,  0.4830, -0.0630, -0.6501]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0313 cost = 0.017760\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4471,  0.3414,  0.0870, -0.4624,  0.5124],\n",
      "         [ 0.7673, -0.9224, -0.9232,  0.6211,  0.0564]],\n",
      "\n",
      "        [[ 0.7954,  0.8025, -0.8518,  0.7864,  0.8074],\n",
      "         [-0.6633, -0.4010,  0.6603,  0.0062, -0.8542]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0313 cost = 0.013705\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3817,  0.5153,  0.1074, -0.4511,  0.5118],\n",
      "         [-0.3376,  0.3095, -0.0659, -0.4113,  0.5295]],\n",
      "\n",
      "        [[ 0.8838,  0.8910, -0.9257,  0.8813,  0.8214],\n",
      "         [ 0.8555,  0.8965, -0.9073,  0.8799,  0.8393]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0314 cost = 0.011237\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4479,  0.3417,  0.0871, -0.4634,  0.5129],\n",
      "         [ 0.8279, -0.8010, -0.8752,  0.8091,  0.0052]],\n",
      "\n",
      "        [[ 0.7956,  0.8026, -0.8520,  0.7866,  0.8076],\n",
      "         [-0.6772, -0.3658,  0.6919,  0.0084, -0.8780]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0314 cost = 0.011443\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7678, -0.9225, -0.9234,  0.6216,  0.0564],\n",
      "         [ 0.9363, -0.4354, -0.5620,  0.9176, -0.5844]],\n",
      "\n",
      "        [[-0.6635, -0.4012,  0.6604,  0.0057, -0.8543],\n",
      "         [-0.3355, -0.3695,  0.4834, -0.0635, -0.6506]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0314 cost = 0.014934\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4487,  0.3420,  0.0873, -0.4645,  0.5135],\n",
      "         [-0.3390,  0.3111, -0.0654, -0.4124,  0.5300]],\n",
      "\n",
      "        [[ 0.7959,  0.8028, -0.8521,  0.7868,  0.8079],\n",
      "         [ 0.8558,  0.8967, -0.9074,  0.8802,  0.8396]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0315 cost = 0.018420\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7682, -0.9226, -0.9235,  0.6220,  0.0564],\n",
      "         [ 0.9364, -0.4359, -0.5625,  0.9176, -0.5847]],\n",
      "\n",
      "        [[-0.6635, -0.4014,  0.6605,  0.0054, -0.8544],\n",
      "         [-0.3358, -0.3698,  0.4837, -0.0637, -0.6508]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0315 cost = 0.014869\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8285, -0.8012, -0.8755,  0.8095,  0.0050],\n",
      "         [-0.3824,  0.5162,  0.1080, -0.4511,  0.5123]],\n",
      "\n",
      "        [[-0.6775, -0.3664,  0.6922,  0.0076, -0.8782],\n",
      "         [ 0.8841,  0.8912, -0.9259,  0.8816,  0.8220]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0315 cost = 0.005533\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4495,  0.3428,  0.0875, -0.4655,  0.5143],\n",
      "         [ 0.8286, -0.8012, -0.8756,  0.8096,  0.0049]],\n",
      "\n",
      "        [[ 0.7963,  0.8031, -0.8523,  0.7872,  0.8083],\n",
      "         [-0.6776, -0.3665,  0.6923,  0.0075, -0.8782]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0316 cost = 0.011280\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3398,  0.3138, -0.0650, -0.4133,  0.5308],\n",
      "         [-0.3826,  0.5166,  0.1082, -0.4509,  0.5125]],\n",
      "\n",
      "        [[ 0.8562,  0.8969, -0.9076,  0.8805,  0.8400],\n",
      "         [ 0.8842,  0.8913, -0.9259,  0.8818,  0.8222]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0316 cost = 0.009969\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7689, -0.9226, -0.9236,  0.6227,  0.0563],\n",
      "         [ 0.9365, -0.4369, -0.5635,  0.9177, -0.5852]],\n",
      "\n",
      "        [[-0.6637, -0.4018,  0.6607,  0.0047, -0.8545],\n",
      "         [-0.3364, -0.3704,  0.4841, -0.0641, -0.6512]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0316 cost = 0.014739\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8291, -0.8013, -0.8758,  0.8099,  0.0047],\n",
      "         [-0.3406,  0.3151, -0.0647, -0.4140,  0.5312]],\n",
      "\n",
      "        [[-0.6779, -0.3670,  0.6926,  0.0069, -0.8784],\n",
      "         [ 0.8563,  0.8970, -0.9077,  0.8807,  0.8402]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0317 cost = 0.008572\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4508,  0.3438,  0.0878, -0.4672,  0.5155],\n",
      "         [ 0.7692, -0.9227, -0.9237,  0.6230,  0.0563]],\n",
      "\n",
      "        [[ 0.7968,  0.8035, -0.8526,  0.7877,  0.8088],\n",
      "         [-0.6638, -0.4020,  0.6609,  0.0044, -0.8546]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0317 cost = 0.013297\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9366, -0.4375, -0.5641,  0.9178, -0.5856],\n",
      "         [-0.3830,  0.5175,  0.1086, -0.4508,  0.5130]],\n",
      "\n",
      "        [[-0.3368, -0.3709,  0.4844, -0.0644, -0.6515],\n",
      "         [ 0.8845,  0.8914, -0.9261,  0.8820,  0.8226]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0317 cost = 0.013034\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3419,  0.3169, -0.0642, -0.4150,  0.5318],\n",
      "         [-0.4516,  0.3442,  0.0880, -0.4682,  0.5160]],\n",
      "\n",
      "        [[ 0.8566,  0.8972, -0.9078,  0.8810,  0.8404],\n",
      "         [ 0.7970,  0.8036, -0.8527,  0.7879,  0.8090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0318 cost = 0.018788\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3833,  0.5179,  0.1088, -0.4508,  0.5132],\n",
      "         [ 0.8295, -0.8015, -0.8760,  0.8102,  0.0045]],\n",
      "\n",
      "        [[ 0.8846,  0.8915, -0.9261,  0.8821,  0.8228],\n",
      "         [-0.6782, -0.3676,  0.6929,  0.0061, -0.8786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0318 cost = 0.007019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9366, -0.4383, -0.5648,  0.9178, -0.5860],\n",
      "         [ 0.7699, -0.9228, -0.9239,  0.6237,  0.0563]],\n",
      "\n",
      "        [[-0.3372, -0.3714,  0.4848, -0.0647, -0.6518],\n",
      "         [-0.6640, -0.4023,  0.6611,  0.0038, -0.8547]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0318 cost = 0.018769\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3435,  0.3187, -0.0638, -0.4161,  0.5324],\n",
      "         [ 0.7701, -0.9228, -0.9240,  0.6239,  0.0562]],\n",
      "\n",
      "        [[ 0.8569,  0.8974, -0.9080,  0.8813,  0.8408],\n",
      "         [-0.6640, -0.4024,  0.6611,  0.0036, -0.8547]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0319 cost = 0.011517\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3838,  0.5185,  0.1090, -0.4508,  0.5135],\n",
      "         [-0.4532,  0.3452,  0.0882, -0.4700,  0.5173]],\n",
      "\n",
      "        [[ 0.8848,  0.8916, -0.9262,  0.8823,  0.8232],\n",
      "         [ 0.7976,  0.8040, -0.8531,  0.7884,  0.8096]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0319 cost = 0.016820\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9367, -0.4394, -0.5656,  0.9179, -0.5864],\n",
      "         [ 0.8300, -0.8017, -0.8763,  0.8105,  0.0042]],\n",
      "\n",
      "        [[-0.3376, -0.3718,  0.4851, -0.0649, -0.6521],\n",
      "         [-0.6786, -0.3682,  0.6932,  0.0054, -0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0319 cost = 0.015369\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9368, -0.4400, -0.5659,  0.9179, -0.5866],\n",
      "         [ 0.8301, -0.8018, -0.8764,  0.8106,  0.0041]],\n",
      "\n",
      "        [[-0.3377, -0.3720,  0.4852, -0.0650, -0.6522],\n",
      "         [-0.6786, -0.3683,  0.6933,  0.0052, -0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0320 cost = 0.015329\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3447,  0.3207, -0.0634, -0.4169,  0.5333],\n",
      "         [-0.3840,  0.5192,  0.1092, -0.4506,  0.5139]],\n",
      "\n",
      "        [[ 0.8573,  0.8977, -0.9082,  0.8816,  0.8413],\n",
      "         [ 0.8851,  0.8917, -0.9263,  0.8825,  0.8236]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0320 cost = 0.009624\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4537,  0.3464,  0.0883, -0.4705,  0.5186],\n",
      "         [ 0.7711, -0.9230, -0.9243,  0.6250,  0.0562]],\n",
      "\n",
      "        [[ 0.7981,  0.8043, -0.8533,  0.7888,  0.8102],\n",
      "         [-0.6642, -0.4029,  0.6614,  0.0025, -0.8549]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0320 cost = 0.012943\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9369, -0.4417, -0.5670,  0.9180, -0.5874],\n",
      "         [ 0.7713, -0.9231, -0.9243,  0.6253,  0.0561]],\n",
      "\n",
      "        [[-0.3381, -0.3726,  0.4855, -0.0655, -0.6525],\n",
      "         [-0.6642, -0.4030,  0.6614,  0.0023, -0.8549]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0321 cost = 0.018453\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3843,  0.5199,  0.1094, -0.4504,  0.5144],\n",
      "         [ 0.8308, -0.8024, -0.8769,  0.8110,  0.0035]],\n",
      "\n",
      "        [[ 0.8853,  0.8918, -0.9264,  0.8827,  0.8240],\n",
      "         [-0.6790, -0.3690,  0.6936,  0.0043, -0.8790]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0321 cost = 0.006859\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4543,  0.3471,  0.0884, -0.4711,  0.5193],\n",
      "         [-0.3459,  0.3224, -0.0632, -0.4177,  0.5342]],\n",
      "\n",
      "        [[ 0.7984,  0.8045, -0.8535,  0.7891,  0.8105],\n",
      "         [ 0.8576,  0.8979, -0.9083,  0.8820,  0.8417]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0321 cost = 0.017511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3460,  0.3231, -0.0631, -0.4179,  0.5344],\n",
      "         [-0.4544,  0.3475,  0.0884, -0.4712,  0.5196]],\n",
      "\n",
      "        [[ 0.8577,  0.8979, -0.9084,  0.8821,  0.8418],\n",
      "         [ 0.7986,  0.8046, -0.8536,  0.7892,  0.8107]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0322 cost = 0.018311\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9370, -0.4437, -0.5684,  0.9182, -0.5882],\n",
      "         [ 0.8312, -0.8026, -0.8772,  0.8113,  0.0032]],\n",
      "\n",
      "        [[-0.3386, -0.3732,  0.4859, -0.0658, -0.6529],\n",
      "         [-0.6792, -0.3695,  0.6938,  0.0037, -0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0322 cost = 0.015044\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7724, -0.9233, -0.9247,  0.6265,  0.0561],\n",
      "         [-0.3845,  0.5211,  0.1096, -0.4500,  0.5150]],\n",
      "\n",
      "        [[-0.6644, -0.4036,  0.6617,  0.0012, -0.8551],\n",
      "         [ 0.8856,  0.8920, -0.9265,  0.8830,  0.8246]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0322 cost = 0.005126\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9371, -0.4446, -0.5691,  0.9182, -0.5886],\n",
      "         [-0.3845,  0.5213,  0.1096, -0.4499,  0.5151]],\n",
      "\n",
      "        [[-0.3388, -0.3735,  0.4860, -0.0660, -0.6530],\n",
      "         [ 0.8856,  0.8920, -0.9265,  0.8831,  0.8247]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0323 cost = 0.012529\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3160e-01, -8.0280e-01, -8.7759e-01,  8.1159e-01,  2.7891e-03],\n",
      "         [ 7.7275e-01, -9.2336e-01, -9.2479e-01,  6.2697e-01,  5.6019e-02]],\n",
      "\n",
      "        [[-6.7947e-01, -3.6996e-01,  6.9404e-01,  3.1662e-03, -8.7931e-01],\n",
      "         [-6.6450e-01, -4.0376e-01,  6.6177e-01,  7.7589e-04, -8.5512e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0323 cost = 0.012890\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4550,  0.3493,  0.0885, -0.4722,  0.5213],\n",
      "         [-0.3469,  0.3263, -0.0629, -0.4187,  0.5357]],\n",
      "\n",
      "        [[ 0.7993,  0.8052, -0.8540,  0.7899,  0.8115],\n",
      "         [ 0.8582,  0.8983, -0.9086,  0.8826,  0.8424]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0323 cost = 0.017214\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3196e-01, -8.0299e-01, -8.7784e-01,  8.1185e-01,  2.4577e-03],\n",
      "         [ 7.7324e-01, -9.2345e-01, -9.2494e-01,  6.2755e-01,  5.5951e-02]],\n",
      "\n",
      "        [[-6.7965e-01, -3.7033e-01,  6.9420e-01,  2.7237e-03, -8.7943e-01],\n",
      "         [-6.6459e-01, -4.0403e-01,  6.6189e-01,  2.6915e-04, -8.5521e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0324 cost = 0.012822\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4549,  0.3500,  0.0885, -0.4723,  0.5220],\n",
      "         [-0.3842,  0.5224,  0.1097, -0.4494,  0.5157]],\n",
      "\n",
      "        [[ 0.7996,  0.8054, -0.8542,  0.7902,  0.8118],\n",
      "         [ 0.8859,  0.8921, -0.9266,  0.8833,  0.8252]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0324 cost = 0.013513\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3464,  0.3284, -0.0630, -0.4187,  0.5365],\n",
      "         [ 0.9373, -0.4470, -0.5710,  0.9184, -0.5897]],\n",
      "\n",
      "        [[ 0.8584,  0.8984, -0.9087,  0.8828,  0.8428],\n",
      "         [-0.3394, -0.3742,  0.4865, -0.0663, -0.6534]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0324 cost = 0.016315\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3463,  0.3293, -0.0629, -0.4188,  0.5368],\n",
      "         [-0.3834,  0.5233,  0.1098, -0.4487,  0.5161]],\n",
      "\n",
      "        [[ 0.8585,  0.8985, -0.9088,  0.8829,  0.8429],\n",
      "         [ 0.8860,  0.8922, -0.9267,  0.8834,  0.8255]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0325 cost = 0.009258\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3256e-01, -8.0317e-01, -8.7825e-01,  8.1222e-01,  2.0425e-03],\n",
      "         [ 7.7420e-01, -9.2362e-01, -9.2521e-01,  6.2861e-01,  5.5822e-02]],\n",
      "\n",
      "        [[-6.8003e-01, -3.7112e-01,  6.9453e-01,  1.8290e-03, -8.7968e-01],\n",
      "         [-6.6480e-01, -4.0461e-01,  6.6217e-01, -8.1175e-04, -8.5546e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0325 cost = 0.012691\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4546,  0.3518,  0.0886, -0.4728,  0.5237],\n",
      "         [ 0.9374, -0.4479, -0.5717,  0.9184, -0.5902]],\n",
      "\n",
      "        [[ 0.8002,  0.8060, -0.8546,  0.7908,  0.8125],\n",
      "         [-0.3398, -0.3747,  0.4868, -0.0664, -0.6536]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0325 cost = 0.016678\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9374, -0.4481, -0.5720,  0.9184, -0.5903],\n",
      "         [ 0.8328, -0.8033, -0.8784,  0.8124,  0.0018]],\n",
      "\n",
      "        [[-0.3399, -0.3749,  0.4869, -0.0665, -0.6537],\n",
      "         [-0.6802, -0.3716,  0.6947,  0.0013, -0.8798]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0326 cost = 0.014660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7749, -0.9238, -0.9254,  0.6295,  0.0557],\n",
      "         [-0.3469,  0.3322, -0.0627, -0.4194,  0.5379]],\n",
      "\n",
      "        [[-0.6650, -0.4052,  0.6624, -0.0017, -0.8557],\n",
      "         [ 0.8588,  0.8987, -0.9089,  0.8833,  0.8433]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0326 cost = 0.007758\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4547,  0.3528,  0.0887, -0.4733,  0.5246],\n",
      "         [-0.3823,  0.5251,  0.1099, -0.4475,  0.5171]],\n",
      "\n",
      "        [[ 0.8005,  0.8061, -0.8547,  0.7911,  0.8128],\n",
      "         [ 0.8863,  0.8924, -0.9268,  0.8837,  0.8260]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0326 cost = 0.013196\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9375, -0.4494, -0.5731,  0.9185, -0.5910],\n",
      "         [-0.3818,  0.5255,  0.1100, -0.4472,  0.5173]],\n",
      "\n",
      "        [[-0.3404, -0.3755,  0.4873, -0.0669, -0.6541],\n",
      "         [ 0.8863,  0.8924, -0.9268,  0.8838,  0.8261]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0327 cost = 0.012172\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 7.7553e-01, -9.2393e-01, -9.2563e-01,  6.3015e-01,  5.5568e-02],\n",
      "         [ 8.3339e-01, -8.0368e-01, -8.7887e-01,  8.1280e-01,  1.2918e-03]],\n",
      "\n",
      "        [[-6.6518e-01, -4.0561e-01,  6.6263e-01, -2.4327e-03, -8.5582e-01],\n",
      "         [-6.8061e-01, -3.7246e-01,  6.9505e-01,  3.6091e-04, -8.8007e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0327 cost = 0.007292\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4544,  0.3541,  0.0887, -0.4735,  0.5257],\n",
      "         [-0.3471,  0.3348, -0.0626, -0.4198,  0.5389]],\n",
      "\n",
      "        [[ 0.8008,  0.8064, -0.8549,  0.7914,  0.8132],\n",
      "         [ 0.8591,  0.8988, -0.9091,  0.8836,  0.8436]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0327 cost = 0.016611\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3807,  0.5268,  0.1100, -0.4462,  0.5180],\n",
      "         [ 0.7759, -0.9240, -0.9258,  0.6306,  0.0555]],\n",
      "\n",
      "        [[ 0.8864,  0.8925, -0.9269,  0.8839,  0.8264],\n",
      "         [-0.6653, -0.4059,  0.6627, -0.0029, -0.8559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0328 cost = 0.007745\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4542,  0.3550,  0.0888, -0.4737,  0.5265],\n",
      "         [ 0.9376, -0.4508, -0.5743,  0.9185, -0.5917]],\n",
      "\n",
      "        [[ 0.8011,  0.8067, -0.8551,  0.7917,  0.8135],\n",
      "         [-0.3409, -0.3761,  0.4877, -0.0672, -0.6545]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0328 cost = 0.016335\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.4699e-01,  3.3718e-01, -6.2493e-02, -4.1996e-01,  5.3976e-01],\n",
      "         [ 8.3394e-01, -8.0382e-01, -8.7924e-01,  8.1315e-01,  9.0966e-04]],\n",
      "\n",
      "        [[ 8.5928e-01,  8.9898e-01, -9.0916e-01,  8.8379e-01,  8.4392e-01],\n",
      "         [-6.8095e-01, -3.7319e-01,  6.9535e-01, -4.9364e-04, -8.8031e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0328 cost = 0.009496\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9377, -0.4513, -0.5749,  0.9186, -0.5920],\n",
      "         [ 0.7766, -0.9241, -0.9259,  0.6313,  0.0554]],\n",
      "\n",
      "        [[-0.3411, -0.3764,  0.4879, -0.0674, -0.6547],\n",
      "         [-0.6654, -0.4063,  0.6629, -0.0035, -0.8561]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0329 cost = 0.017409\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3472,  0.3385, -0.0624, -0.4202,  0.5402],\n",
      "         [-0.4544,  0.3561,  0.0889, -0.4742,  0.5275]],\n",
      "\n",
      "        [[ 0.8594,  0.8991, -0.9092,  0.8839,  0.8441],\n",
      "         [ 0.8014,  0.8069, -0.8553,  0.7921,  0.8139]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0329 cost = 0.017431\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3432e-01, -8.0392e-01, -8.7951e-01,  8.1339e-01,  6.5378e-04],\n",
      "         [-3.7958e-01,  5.2848e-01,  1.1027e-01, -4.4494e-01,  5.1887e-01]],\n",
      "\n",
      "        [[-6.8121e-01, -3.7376e-01,  6.9558e-01, -1.1476e-03, -8.8047e-01],\n",
      "         [ 8.8672e-01,  8.9267e-01, -9.2697e-01,  8.8423e-01,  8.2692e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0329 cost = 0.004973\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4547,  0.3568,  0.0889, -0.4748,  0.5283],\n",
      "         [-0.3795,  0.5288,  0.1103, -0.4448,  0.5190]],\n",
      "\n",
      "        [[ 0.8017,  0.8072, -0.8555,  0.7923,  0.8142],\n",
      "         [ 0.8868,  0.8927, -0.9270,  0.8843,  0.8270]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0330 cost = 0.012753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9377, -0.4524, -0.5761,  0.9186, -0.5925],\n",
      "         [-0.3480,  0.3409, -0.0621, -0.4208,  0.5411]],\n",
      "\n",
      "        [[-0.3416, -0.3770,  0.4882, -0.0676, -0.6550],\n",
      "         [ 0.8597,  0.8992, -0.9094,  0.8842,  0.8444]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0330 cost = 0.014717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3457e-01, -8.0400e-01, -8.7973e-01,  8.1352e-01,  5.2569e-04],\n",
      "         [ 7.7745e-01, -9.2427e-01, -9.2621e-01,  6.3220e-01,  5.5319e-02]],\n",
      "\n",
      "        [[-6.8144e-01, -3.7419e-01,  6.9579e-01, -1.6763e-03, -8.8062e-01],\n",
      "         [-6.6563e-01, -4.0683e-01,  6.6320e-01, -4.5778e-03, -8.5627e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0330 cost = 0.012223\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 7.7765e-01, -9.2432e-01, -9.2627e-01,  6.3243e-01,  5.5279e-02],\n",
      "         [ 8.3469e-01, -8.0409e-01, -8.7982e-01,  8.1360e-01,  4.0870e-04]],\n",
      "\n",
      "        [[-6.6568e-01, -4.0697e-01,  6.6327e-01, -4.8308e-03, -8.5632e-01],\n",
      "         [-6.8153e-01, -3.7438e-01,  6.9587e-01, -1.8978e-03, -8.8068e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0331 cost = 0.007106\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4547,  0.3585,  0.0890, -0.4754,  0.5299],\n",
      "         [-0.3483,  0.3428, -0.0620, -0.4212,  0.5419]],\n",
      "\n",
      "        [[ 0.8022,  0.8077, -0.8558,  0.7929,  0.8148],\n",
      "         [ 0.8599,  0.8994, -0.9095,  0.8845,  0.8447]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0331 cost = 0.016049\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3778,  0.5308,  0.1105, -0.4432,  0.5201],\n",
      "         [ 0.9378, -0.4536, -0.5771,  0.9186, -0.5932]],\n",
      "\n",
      "        [[ 0.8871,  0.8929, -0.9271,  0.8846,  0.8276],\n",
      "         [-0.3421, -0.3776,  0.4886, -0.0678, -0.6553]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0331 cost = 0.012458\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4546,  0.3594,  0.0891, -0.4756,  0.5307],\n",
      "         [-0.3483,  0.3444, -0.0620, -0.4213,  0.5424]],\n",
      "\n",
      "        [[ 0.8025,  0.8080, -0.8560,  0.7932,  0.8152],\n",
      "         [ 0.8601,  0.8995, -0.9096,  0.8846,  0.8450]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0332 cost = 0.015940\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 7.7847e-01, -9.2443e-01, -9.2649e-01,  6.3328e-01,  5.5167e-02],\n",
      "         [ 8.3521e-01, -8.0416e-01, -8.8017e-01,  8.1391e-01,  1.0064e-04]],\n",
      "\n",
      "        [[-6.6585e-01, -4.0749e-01,  6.6351e-01, -5.7408e-03, -8.5653e-01],\n",
      "         [-6.8188e-01, -3.7511e-01,  6.9617e-01, -2.7546e-03, -8.8092e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0332 cost = 0.007039\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3768,  0.5319,  0.1105, -0.4422,  0.5207],\n",
      "         [ 0.9379, -0.4541, -0.5779,  0.9186, -0.5935]],\n",
      "\n",
      "        [[ 0.8873,  0.8930, -0.9272,  0.8848,  0.8280],\n",
      "         [-0.3424, -0.3779,  0.4889, -0.0678, -0.6555]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0332 cost = 0.012385\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3478,  0.3471, -0.0619, -0.4213,  0.5433],\n",
      "         [-0.4543,  0.3606,  0.0891, -0.4758,  0.5319]],\n",
      "\n",
      "        [[ 0.8603,  0.8997, -0.9097,  0.8849,  0.8453],\n",
      "         [ 0.8030,  0.8084, -0.8563,  0.7937,  0.8157]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0333 cost = 0.016955\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3560e-01, -8.0409e-01, -8.8041e-01,  8.1413e-01, -3.2631e-05],\n",
      "         [ 9.3794e-01, -4.5440e-01, -5.7844e-01,  9.1864e-01, -5.9365e-01]],\n",
      "\n",
      "        [[-6.8213e-01, -3.7564e-01,  6.9639e-01, -3.3671e-03, -8.8109e-01],\n",
      "         [-3.4263e-01, -3.7820e-01,  4.8910e-01, -6.7889e-02, -6.5562e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0333 cost = 0.014106\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.5329,  0.1106, -0.4415,  0.5213],\n",
      "         [ 0.7792, -0.9245, -0.9267,  0.6340,  0.0551]],\n",
      "\n",
      "        [[ 0.8875,  0.8932, -0.9273,  0.8850,  0.8284],\n",
      "         [-0.6660, -0.4080,  0.6637, -0.0065, -0.8567]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0333 cost = 0.007420\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.3588e-01, -8.0415e-01, -8.8059e-01,  8.1431e-01, -1.9621e-04],\n",
      "         [-3.4800e-01,  3.4948e-01, -6.1789e-02, -4.2159e-01,  5.4412e-01]],\n",
      "\n",
      "        [[-6.8232e-01, -3.7602e-01,  6.9656e-01, -3.8141e-03, -8.8121e-01],\n",
      "         [ 8.6059e-01,  8.9984e-01, -9.0982e-01,  8.8516e-01,  8.4562e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0334 cost = 0.007543\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4543,  0.3620,  0.0892, -0.4763,  0.5334],\n",
      "         [ 0.9380, -0.4550, -0.5792,  0.9187, -0.5940]],\n",
      "\n",
      "        [[ 0.8036,  0.8089, -0.8567,  0.7943,  0.8163],\n",
      "         [-0.3431, -0.3787,  0.4894, -0.0681, -0.6559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0334 cost = 0.015628\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7797, -0.9246, -0.9268,  0.6345,  0.0550],\n",
      "         [-0.3758,  0.5337,  0.1107, -0.4410,  0.5217]],\n",
      "\n",
      "        [[-0.6661, -0.4083,  0.6639, -0.0071, -0.8568],\n",
      "         [ 0.8876,  0.8933, -0.9274,  0.8852,  0.8288]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0334 cost = 0.004676\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.3811e-01, -4.5541e-01, -5.7975e-01,  9.1876e-01, -5.9427e-01],\n",
      "         [ 8.3628e-01, -8.0428e-01, -8.8084e-01,  8.1459e-01, -4.4719e-04]],\n",
      "\n",
      "        [[-3.4337e-01, -3.7911e-01,  4.8963e-01, -6.8390e-02, -6.5618e-01],\n",
      "         [-6.8258e-01, -3.7655e-01,  6.9680e-01, -4.4578e-03, -8.8138e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0335 cost = 0.013824\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3757,  0.5341,  0.1108, -0.4407,  0.5220],\n",
      "         [-0.4545,  0.3627,  0.0892, -0.4768,  0.5342]],\n",
      "\n",
      "        [[ 0.8877,  0.8933, -0.9274,  0.8853,  0.8289],\n",
      "         [ 0.8039,  0.8091, -0.8568,  0.7945,  0.8166]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0335 cost = 0.015161\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3483,  0.3523, -0.0616, -0.4220,  0.5452],\n",
      "         [ 0.7802, -0.9247, -0.9270,  0.6351,  0.0549]],\n",
      "\n",
      "        [[ 0.8609,  0.9001, -0.9100,  0.8855,  0.8461],\n",
      "         [-0.6662, -0.4087,  0.6640, -0.0077, -0.8569]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0335 cost = 0.010063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3484,  0.3529, -0.0615, -0.4221,  0.5454],\n",
      "         [ 0.7804, -0.9247, -0.9270,  0.6353,  0.0549]],\n",
      "\n",
      "        [[ 0.8610,  0.9001, -0.9100,  0.8856,  0.8462],\n",
      "         [-0.6662, -0.4088,  0.6641, -0.0078, -0.8569]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0336 cost = 0.010037\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9383, -0.4566, -0.5809,  0.9189, -0.5948],\n",
      "         [-0.3757,  0.5347,  0.1109, -0.4403,  0.5225]],\n",
      "\n",
      "        [[-0.3439, -0.3798,  0.4900, -0.0688, -0.6566],\n",
      "         [ 0.8879,  0.8934, -0.9275,  0.8855,  0.8293]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0336 cost = 0.011460\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4547,  0.3638,  0.0893, -0.4770,  0.5356],\n",
      "         [ 0.8369, -0.8046, -0.8813,  0.8151, -0.0009]],\n",
      "\n",
      "        [[ 0.8044,  0.8095, -0.8571,  0.7950,  0.8171],\n",
      "         [-0.6830, -0.3774,  0.6972, -0.0055, -0.8816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0336 cost = 0.009489\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8371, -0.8047, -0.8814,  0.8152, -0.0010],\n",
      "         [-0.3492,  0.3545, -0.0612, -0.4226,  0.5461]],\n",
      "\n",
      "        [[-0.6831, -0.3775,  0.6973, -0.0057, -0.8817],\n",
      "         [ 0.8613,  0.9003, -0.9102,  0.8859,  0.8465]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0337 cost = 0.007386\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4552,  0.3643,  0.0894, -0.4775,  0.5362],\n",
      "         [ 0.7811, -0.9248, -0.9272,  0.6361,  0.0549]],\n",
      "\n",
      "        [[ 0.8046,  0.8096, -0.8573,  0.7952,  0.8174],\n",
      "         [-0.6663, -0.4092,  0.6642, -0.0086, -0.8570]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0337 cost = 0.011229\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.5354,  0.1112, -0.4400,  0.5230],\n",
      "         [ 0.9384, -0.4577, -0.5819,  0.9190, -0.5954]],\n",
      "\n",
      "        [[ 0.8882,  0.8936, -0.9276,  0.8857,  0.8298],\n",
      "         [-0.3444, -0.3804,  0.4904, -0.0692, -0.6569]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0337 cost = 0.011973\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3502,  0.3560, -0.0609, -0.4232,  0.5466],\n",
      "         [ 0.9384, -0.4580, -0.5821,  0.9190, -0.5955]],\n",
      "\n",
      "        [[ 0.8615,  0.9004, -0.9103,  0.8861,  0.8468],\n",
      "         [-0.3445, -0.3805,  0.4905, -0.0693, -0.6570]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0338 cost = 0.014823\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4561,  0.3648,  0.0895, -0.4784,  0.5369],\n",
      "         [ 0.7817, -0.9249, -0.9274,  0.6368,  0.0548]],\n",
      "\n",
      "        [[ 0.8049,  0.8098, -0.8574,  0.7955,  0.8177],\n",
      "         [-0.6664, -0.4094,  0.6644, -0.0091, -0.8571]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0338 cost = 0.011141\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.5357,  0.1114, -0.4399,  0.5232],\n",
      "         [ 0.8377, -0.8049, -0.8817,  0.8156, -0.0014]],\n",
      "\n",
      "        [[ 0.8884,  0.8936, -0.9277,  0.8858,  0.8301],\n",
      "         [-0.6835, -0.3783,  0.6976, -0.0066, -0.8819]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0338 cost = 0.006035\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3768,  0.5358,  0.1115, -0.4399,  0.5232],\n",
      "         [-0.4569,  0.3651,  0.0896, -0.4791,  0.5373]],\n",
      "\n",
      "        [[ 0.8884,  0.8936, -0.9277,  0.8859,  0.8302],\n",
      "         [ 0.8051,  0.8099, -0.8576,  0.7956,  0.8179]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0339 cost = 0.014856\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8379, -0.8050, -0.8819,  0.8157, -0.0016],\n",
      "         [ 0.7823, -0.9250, -0.9275,  0.6375,  0.0548]],\n",
      "\n",
      "        [[-0.6836, -0.3786,  0.6978, -0.0070, -0.8820],\n",
      "         [-0.6665, -0.4097,  0.6645, -0.0097, -0.8572]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0339 cost = 0.011565\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3518,  0.3582, -0.0602, -0.4241,  0.5474],\n",
      "         [ 0.9385, -0.4593, -0.5833,  0.9192, -0.5961]],\n",
      "\n",
      "        [[ 0.8619,  0.9007, -0.9105,  0.8865,  0.8472],\n",
      "         [-0.3452, -0.3814,  0.4910, -0.0699, -0.6575]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0339 cost = 0.014644\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3772,  0.5361,  0.1117, -0.4397,  0.5235],\n",
      "         [-0.3521,  0.3587, -0.0601, -0.4243,  0.5476]],\n",
      "\n",
      "        [[ 0.8886,  0.8938, -0.9278,  0.8861,  0.8305],\n",
      "         [ 0.8620,  0.9007, -0.9105,  0.8866,  0.8473]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0340 cost = 0.009227\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8383, -0.8052, -0.8821,  0.8161, -0.0018],\n",
      "         [ 0.9386, -0.4598, -0.5838,  0.9192, -0.5964]],\n",
      "\n",
      "        [[-0.6839, -0.3791,  0.6980, -0.0076, -0.8821],\n",
      "         [-0.3454, -0.3817,  0.4912, -0.0702, -0.6577]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0340 cost = 0.013480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7831, -0.9252, -0.9278,  0.6385,  0.0547],\n",
      "         [-0.4578,  0.3660,  0.0898, -0.4798,  0.5387]],\n",
      "\n",
      "        [[-0.6667, -0.4102,  0.6648, -0.0106, -0.8574],\n",
      "         [ 0.8057,  0.8103, -0.8579,  0.7962,  0.8185]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0340 cost = 0.008051\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3776,  0.5364,  0.1119, -0.4395,  0.5239],\n",
      "         [ 0.8387, -0.8054, -0.8823,  0.8163, -0.0021]],\n",
      "\n",
      "        [[ 0.8888,  0.8938, -0.9279,  0.8863,  0.8309],\n",
      "         [-0.6841, -0.3795,  0.6982, -0.0081, -0.8822]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0341 cost = 0.005930\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4579,  0.3662,  0.0898, -0.4801,  0.5392],\n",
      "         [-0.3526,  0.3604, -0.0597, -0.4247,  0.5483]],\n",
      "\n",
      "        [[ 0.8059,  0.8104, -0.8580,  0.7963,  0.8187],\n",
      "         [ 0.8623,  0.9009, -0.9107,  0.8869,  0.8477]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0341 cost = 0.014796\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7837, -0.9253, -0.9279,  0.6392,  0.0546],\n",
      "         [ 0.9388, -0.4608, -0.5847,  0.9194, -0.5969]],\n",
      "\n",
      "        [[-0.6669, -0.4105,  0.6649, -0.0112, -0.8575],\n",
      "         [-0.3460, -0.3825,  0.4916, -0.0706, -0.6582]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0341 cost = 0.012494\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8391, -0.8055, -0.8825,  0.8166, -0.0024],\n",
      "         [ 0.7839, -0.9253, -0.9280,  0.6394,  0.0546]],\n",
      "\n",
      "        [[-0.6843, -0.3800,  0.6984, -0.0087, -0.8824],\n",
      "         [-0.6669, -0.4106,  0.6650, -0.0114, -0.8575]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0342 cost = 0.011354\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4579,  0.3668,  0.0899, -0.4803,  0.5400],\n",
      "         [-0.3523,  0.3619, -0.0595, -0.4248,  0.5488]],\n",
      "\n",
      "        [[ 0.8062,  0.8107, -0.8582,  0.7967,  0.8191],\n",
      "         [ 0.8625,  0.9010, -0.9108,  0.8871,  0.8479]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0342 cost = 0.014680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9389, -0.4614, -0.5855,  0.9195, -0.5973],\n",
      "         [-0.3776,  0.5371,  0.1121, -0.4389,  0.5245]],\n",
      "\n",
      "        [[-0.3464, -0.3830,  0.4919, -0.0709, -0.6585],\n",
      "         [ 0.8891,  0.8940, -0.9280,  0.8866,  0.8314]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0342 cost = 0.011002\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3519,  0.3632, -0.0595, -0.4248,  0.5492],\n",
      "         [ 0.9390, -0.4617, -0.5858,  0.9195, -0.5975]],\n",
      "\n",
      "        [[ 0.8627,  0.9011, -0.9108,  0.8872,  0.8481],\n",
      "         [-0.3466, -0.3832,  0.4920, -0.0710, -0.6586]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0343 cost = 0.014312\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8398, -0.8057, -0.8829,  0.8171, -0.0029],\n",
      "         [-0.3774,  0.5375,  0.1122, -0.4385,  0.5247]],\n",
      "\n",
      "        [[-0.6847, -0.3808,  0.6988, -0.0096, -0.8826],\n",
      "         [ 0.8892,  0.8941, -0.9280,  0.8867,  0.8316]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0343 cost = 0.004506\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7850, -0.9255, -0.9283,  0.6408,  0.0544],\n",
      "         [-0.4577,  0.3677,  0.0900, -0.4804,  0.5409]],\n",
      "\n",
      "        [[-0.6672, -0.4113,  0.6654, -0.0126, -0.8577],\n",
      "         [ 0.8066,  0.8110, -0.8584,  0.7971,  0.8195]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0343 cost = 0.007890\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3518,  0.3648, -0.0594, -0.4249,  0.5498],\n",
      "         [ 0.7852, -0.9255, -0.9283,  0.6410,  0.0543]],\n",
      "\n",
      "        [[ 0.8628,  0.9012, -0.9109,  0.8874,  0.8483],\n",
      "         [-0.6672, -0.4114,  0.6654, -0.0128, -0.8578]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0344 cost = 0.009448\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8402, -0.8059, -0.8832,  0.8175, -0.0033],\n",
      "         [-0.4578,  0.3681,  0.0901, -0.4808,  0.5414]],\n",
      "\n",
      "        [[-0.6850, -0.3813,  0.6990, -0.0103, -0.8828],\n",
      "         [ 0.8068,  0.8111, -0.8585,  0.7973,  0.8197]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0344 cost = 0.007894\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.5381,  0.1124, -0.4382,  0.5252],\n",
      "         [ 0.9392, -0.4631, -0.5871,  0.9197, -0.5982]],\n",
      "\n",
      "        [[ 0.8894,  0.8942, -0.9281,  0.8869,  0.8320],\n",
      "         [-0.3473, -0.3840,  0.4925, -0.0716, -0.6592]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0344 cost = 0.011396\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9393, -0.4633, -0.5873,  0.9198, -0.5984],\n",
      "         [-0.3774,  0.5382,  0.1125, -0.4381,  0.5253]],\n",
      "\n",
      "        [[-0.3475, -0.3842,  0.4926, -0.0717, -0.6593],\n",
      "         [ 0.8894,  0.8942, -0.9281,  0.8869,  0.8321]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0345 cost = 0.010829\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7860, -0.9257, -0.9285,  0.6419,  0.0542],\n",
      "         [-0.3522,  0.3665, -0.0590, -0.4254,  0.5503]],\n",
      "\n",
      "        [[-0.6674, -0.4117,  0.6657, -0.0136, -0.8579],\n",
      "         [ 0.8631,  0.9014, -0.9110,  0.8877,  0.8486]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0345 cost = 0.006784\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8408, -0.8062, -0.8835,  0.8179, -0.0038],\n",
      "         [-0.4581,  0.3686,  0.0903, -0.4816,  0.5422]],\n",
      "\n",
      "        [[-0.6853, -0.3818,  0.6993, -0.0111, -0.8829],\n",
      "         [ 0.8071,  0.8114, -0.8586,  0.7975,  0.8200]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0345 cost = 0.007830\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7863, -0.9258, -0.9286,  0.6423,  0.0541],\n",
      "         [ 0.9394, -0.4645, -0.5881,  0.9199, -0.5990]],\n",
      "\n",
      "        [[-0.6675, -0.4119,  0.6658, -0.0140, -0.8580],\n",
      "         [-0.3479, -0.3848,  0.4929, -0.0723, -0.6597]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0346 cost = 0.012122\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3776,  0.5386,  0.1127, -0.4381,  0.5257],\n",
      "         [ 0.8410, -0.8065, -0.8837,  0.8181, -0.0042]],\n",
      "\n",
      "        [[ 0.8896,  0.8943, -0.9282,  0.8870,  0.8323],\n",
      "         [-0.6855, -0.3821,  0.6995, -0.0115, -0.8830]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0346 cost = 0.005708\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3525,  0.3676, -0.0587, -0.4258,  0.5508],\n",
      "         [-0.4583,  0.3688,  0.0904, -0.4821,  0.5426]],\n",
      "\n",
      "        [[ 0.8633,  0.9015, -0.9111,  0.8879,  0.8488],\n",
      "         [ 0.8073,  0.8115, -0.8587,  0.7977,  0.8201]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0346 cost = 0.015703\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3777,  0.5388,  0.1129, -0.4381,  0.5259],\n",
      "         [ 0.8413, -0.8067, -0.8838,  0.8183, -0.0045]],\n",
      "\n",
      "        [[ 0.8897,  0.8943, -0.9282,  0.8871,  0.8325],\n",
      "         [-0.6856, -0.3824,  0.6996, -0.0120, -0.8831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0347 cost = 0.005682\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4587,  0.3691,  0.0905, -0.4826,  0.5431],\n",
      "         [-0.3531,  0.3683, -0.0584, -0.4262,  0.5511]],\n",
      "\n",
      "        [[ 0.8075,  0.8116, -0.8588,  0.7978,  0.8204],\n",
      "         [ 0.8634,  0.9016, -0.9112,  0.8880,  0.8489]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0347 cost = 0.014212\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7870, -0.9259, -0.9288,  0.6432,  0.0539],\n",
      "         [ 0.9396, -0.4660, -0.5892,  0.9201, -0.5997]],\n",
      "\n",
      "        [[-0.6677, -0.4123,  0.6661, -0.0148, -0.8581],\n",
      "         [-0.3486, -0.3856,  0.4934, -0.0729, -0.6602]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0347 cost = 0.011995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3778,  0.5392,  0.1131, -0.4379,  0.5263],\n",
      "         [-0.4589,  0.3695,  0.0906, -0.4831,  0.5437]],\n",
      "\n",
      "        [[ 0.8899,  0.8944, -0.9283,  0.8873,  0.8328],\n",
      "         [ 0.8078,  0.8119, -0.8590,  0.7981,  0.8206]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0348 cost = 0.014172\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8417, -0.8068, -0.8841,  0.8186, -0.0048],\n",
      "         [-0.3534,  0.3700, -0.0582, -0.4265,  0.5517]],\n",
      "\n",
      "        [[-0.6860, -0.3828,  0.6999, -0.0126, -0.8833],\n",
      "         [ 0.8637,  0.9018, -0.9113,  0.8883,  0.8492]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0348 cost = 0.006844\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7874, -0.9259, -0.9289,  0.6435,  0.0539],\n",
      "         [ 0.9396, -0.4665, -0.5898,  0.9201, -0.6000]],\n",
      "\n",
      "        [[-0.6678, -0.4124,  0.6662, -0.0152, -0.8582],\n",
      "         [-0.3489, -0.3859,  0.4937, -0.0729, -0.6604]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0348 cost = 0.011937\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8419, -0.8069, -0.8842,  0.8187, -0.0050],\n",
      "         [-0.3777,  0.5398,  0.1132, -0.4375,  0.5268]],\n",
      "\n",
      "        [[-0.6861, -0.3830,  0.7001, -0.0130, -0.8834],\n",
      "         [ 0.8901,  0.8946, -0.9284,  0.8875,  0.8333]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0349 cost = 0.004336\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4588,  0.3705,  0.0906, -0.4832,  0.5450],\n",
      "         [-0.3536,  0.3715, -0.0580, -0.4267,  0.5522]],\n",
      "\n",
      "        [[ 0.8083,  0.8123, -0.8593,  0.7986,  0.8212],\n",
      "         [ 0.8639,  0.9019, -0.9114,  0.8885,  0.8495]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0349 cost = 0.013979\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7877, -0.9260, -0.9290,  0.6438,  0.0538],\n",
      "         [ 0.9397, -0.4671, -0.5904,  0.9202, -0.6003]],\n",
      "\n",
      "        [[-0.6679, -0.4126,  0.6664, -0.0156, -0.8583],\n",
      "         [-0.3493, -0.3863,  0.4939, -0.0731, -0.6606]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0349 cost = 0.011875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.5404,  0.1133, -0.4371,  0.5273],\n",
      "         [ 0.7878, -0.9260, -0.9290,  0.6440,  0.0538]],\n",
      "\n",
      "        [[ 0.8903,  0.8947, -0.9285,  0.8877,  0.8336],\n",
      "         [-0.6679, -0.4127,  0.6664, -0.0157, -0.8583]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0350 cost = 0.006592\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8424, -0.8070, -0.8845,  0.8191, -0.0053],\n",
      "         [-0.4586,  0.3713,  0.0907, -0.4832,  0.5459]],\n",
      "\n",
      "        [[-0.6864, -0.3835,  0.7004, -0.0136, -0.8836],\n",
      "         [ 0.8087,  0.8127, -0.8596,  0.7990,  0.8216]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0350 cost = 0.007592\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9398, -0.4677, -0.5910,  0.9203, -0.6006],\n",
      "         [-0.3534,  0.3736, -0.0578, -0.4267,  0.5530]],\n",
      "\n",
      "        [[-0.3497, -0.3867,  0.4942, -0.0732, -0.6609],\n",
      "         [ 0.8642,  0.9021, -0.9116,  0.8888,  0.8499]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0350 cost = 0.012961\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8426, -0.8072, -0.8846,  0.8193, -0.0055],\n",
      "         [ 0.7883, -0.9261, -0.9291,  0.6446,  0.0537]],\n",
      "\n",
      "        [[-0.6866, -0.3838,  0.7005, -0.0140, -0.8837],\n",
      "         [-0.6680, -0.4129,  0.6666, -0.0162, -0.8584]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0351 cost = 0.010748\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3532,  0.3744, -0.0577, -0.4267,  0.5533],\n",
      "         [-0.3772,  0.5410,  0.1134, -0.4365,  0.5278]],\n",
      "\n",
      "        [[ 0.8644,  0.9022, -0.9116,  0.8889,  0.8501],\n",
      "         [ 0.8905,  0.8948, -0.9286,  0.8879,  0.8340]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0351 cost = 0.007537\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9400, -0.4687, -0.5917,  0.9204, -0.6011],\n",
      "         [-0.4585,  0.3720,  0.0908, -0.4834,  0.5469]],\n",
      "\n",
      "        [[-0.3501, -0.3871,  0.4945, -0.0735, -0.6612],\n",
      "         [ 0.8091,  0.8130, -0.8598,  0.7994,  0.8221]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0351 cost = 0.014384\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7889, -0.9262, -0.9293,  0.6452,  0.0536],\n",
      "         [-0.3771,  0.5413,  0.1135, -0.4364,  0.5281]],\n",
      "\n",
      "        [[-0.6681, -0.4132,  0.6667, -0.0168, -0.8585],\n",
      "         [ 0.8906,  0.8949, -0.9286,  0.8880,  0.8342]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0352 cost = 0.004150\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4585,  0.3723,  0.0908, -0.4836,  0.5473],\n",
      "         [-0.3534,  0.3755, -0.0575, -0.4270,  0.5538]],\n",
      "\n",
      "        [[ 0.8093,  0.8131, -0.8599,  0.7995,  0.8222],\n",
      "         [ 0.8646,  0.9023, -0.9117,  0.8891,  0.8503]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0352 cost = 0.013680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8432, -0.8076, -0.8850,  0.8197, -0.0062],\n",
      "         [ 0.9400, -0.4697, -0.5924,  0.9205, -0.6015]],\n",
      "\n",
      "        [[-0.6870, -0.3844,  0.7009, -0.0149, -0.8839],\n",
      "         [-0.3505, -0.3875,  0.4948, -0.0738, -0.6614]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0352 cost = 0.012470\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7893, -0.9263, -0.9294,  0.6458,  0.0535],\n",
      "         [-0.3534,  0.3765, -0.0574, -0.4270,  0.5541]],\n",
      "\n",
      "        [[-0.6682, -0.4135,  0.6669, -0.0173, -0.8586],\n",
      "         [ 0.8647,  0.9024, -0.9118,  0.8892,  0.8505]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0353 cost = 0.006468\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3767,  0.5420,  0.1137, -0.4359,  0.5287],\n",
      "         [ 0.9401, -0.4703, -0.5928,  0.9205, -0.6018]],\n",
      "\n",
      "        [[ 0.8908,  0.8950, -0.9287,  0.8882,  0.8346],\n",
      "         [-0.3508, -0.3878,  0.4950, -0.0740, -0.6616]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0353 cost = 0.010781\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4583,  0.3732,  0.0910, -0.4839,  0.5483],\n",
      "         [ 0.8435, -0.8078, -0.8852,  0.8200, -0.0066]],\n",
      "\n",
      "        [[ 0.8097,  0.8135, -0.8601,  0.7999,  0.8227],\n",
      "         [-0.6872, -0.3848,  0.7011, -0.0155, -0.8840]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0353 cost = 0.008388\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9402, -0.4709, -0.5933,  0.9206, -0.6022],\n",
      "         [-0.4584,  0.3734,  0.0910, -0.4841,  0.5484]],\n",
      "\n",
      "        [[-0.3511, -0.3882,  0.4952, -0.0742, -0.6619],\n",
      "         [ 0.8098,  0.8135, -0.8601,  0.8000,  0.8228]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0354 cost = 0.014181\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.5424,  0.1138, -0.4356,  0.5290],\n",
      "         [ 0.8438, -0.8080, -0.8854,  0.8202, -0.0069]],\n",
      "\n",
      "        [[ 0.8909,  0.8951, -0.9287,  0.8883,  0.8348],\n",
      "         [-0.6874, -0.3851,  0.7013, -0.0159, -0.8841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0354 cost = 0.005413\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7901, -0.9264, -0.9296,  0.6467,  0.0533],\n",
      "         [-0.3536,  0.3785, -0.0572, -0.4274,  0.5548]],\n",
      "\n",
      "        [[-0.6684, -0.4139,  0.6672, -0.0181, -0.8588],\n",
      "         [ 0.8650,  0.9026, -0.9119,  0.8895,  0.8508]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0354 cost = 0.006404\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7902, -0.9265, -0.9297,  0.6468,  0.0532],\n",
      "         [-0.3767,  0.5426,  0.1139, -0.4355,  0.5292]],\n",
      "\n",
      "        [[-0.6685, -0.4140,  0.6672, -0.0182, -0.8588],\n",
      "         [ 0.8910,  0.8951, -0.9288,  0.8884,  0.8350]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0355 cost = 0.004071\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9404, -0.4724, -0.5944,  0.9208, -0.6029],\n",
      "         [-0.4587,  0.3738,  0.0912, -0.4847,  0.5490]],\n",
      "\n",
      "        [[-0.3516, -0.3888,  0.4956, -0.0748, -0.6623],\n",
      "         [ 0.8101,  0.8137, -0.8603,  0.8002,  0.8231]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0355 cost = 0.014063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3538,  0.3793, -0.0569, -0.4276,  0.5552],\n",
      "         [ 0.8443, -0.8084, -0.8857,  0.8206, -0.0076]],\n",
      "\n",
      "        [[ 0.8651,  0.9027, -0.9119,  0.8897,  0.8510],\n",
      "         [-0.6877, -0.3856,  0.7016, -0.0166, -0.8843]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0355 cost = 0.007830\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3541,  0.3796, -0.0568, -0.4278,  0.5553],\n",
      "         [ 0.9404, -0.4733, -0.5949,  0.9209, -0.6033]],\n",
      "\n",
      "        [[ 0.8652,  0.9027, -0.9120,  0.8897,  0.8510],\n",
      "         [-0.3519, -0.3891,  0.4957, -0.0751, -0.6625]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0356 cost = 0.013136\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3769,  0.5429,  0.1142, -0.4355,  0.5296],\n",
      "         [ 0.8445, -0.8086, -0.8859,  0.8208, -0.0079]],\n",
      "\n",
      "        [[ 0.8912,  0.8952, -0.9288,  0.8885,  0.8353],\n",
      "         [-0.6879, -0.3858,  0.7017, -0.0169, -0.8843]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0356 cost = 0.005343\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7908, -0.9266, -0.9298,  0.6476,  0.0530],\n",
      "         [-0.4593,  0.3742,  0.0914, -0.4857,  0.5495]],\n",
      "\n",
      "        [[-0.6686, -0.4143,  0.6675, -0.0189, -0.8589],\n",
      "         [ 0.8104,  0.8139, -0.8604,  0.8004,  0.8233]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0356 cost = 0.007289\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7909, -0.9266, -0.9299,  0.6477,  0.0530],\n",
      "         [-0.3550,  0.3804, -0.0565, -0.4285,  0.5556]],\n",
      "\n",
      "        [[-0.6687, -0.4144,  0.6675, -0.0190, -0.8589],\n",
      "         [ 0.8654,  0.9028, -0.9121,  0.8899,  0.8512]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0357 cost = 0.006319\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3773,  0.5430,  0.1145, -0.4356,  0.5299],\n",
      "         [-0.4597,  0.3743,  0.0915, -0.4864,  0.5498]],\n",
      "\n",
      "        [[ 0.8913,  0.8953, -0.9289,  0.8886,  0.8355],\n",
      "         [ 0.8105,  0.8140, -0.8604,  0.8005,  0.8235]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0357 cost = 0.013517\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8448, -0.8089, -0.8861,  0.8211, -0.0084],\n",
      "         [ 0.9406, -0.4749, -0.5959,  0.9210, -0.6041]],\n",
      "\n",
      "        [[-0.6882, -0.3861,  0.7020, -0.0176, -0.8845],\n",
      "         [-0.3525, -0.3898,  0.4962, -0.0758, -0.6629]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0357 cost = 0.012071\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.5432,  0.1146, -0.4356,  0.5301],\n",
      "         [ 0.7912, -0.9267, -0.9300,  0.6480,  0.0529]],\n",
      "\n",
      "        [[ 0.8914,  0.8954, -0.9289,  0.8887,  0.8357],\n",
      "         [-0.6688, -0.4145,  0.6677, -0.0194, -0.8590]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0358 cost = 0.006263\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8450, -0.8091, -0.8862,  0.8213, -0.0087],\n",
      "         [ 0.9406, -0.4755, -0.5962,  0.9211, -0.6044]],\n",
      "\n",
      "        [[-0.6883, -0.3864,  0.7021, -0.0179, -0.8846],\n",
      "         [-0.3527, -0.3900,  0.4963, -0.0760, -0.6631]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0358 cost = 0.012022\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4599,  0.3747,  0.0917, -0.4869,  0.5507],\n",
      "         [-0.3559,  0.3818, -0.0559, -0.4291,  0.5562]],\n",
      "\n",
      "        [[ 0.8109,  0.8143, -0.8606,  0.8008,  0.8238],\n",
      "         [ 0.8657,  0.9031, -0.9122,  0.8902,  0.8516]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0358 cost = 0.013181\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.5435,  0.1147, -0.4354,  0.5305],\n",
      "         [ 0.8453, -0.8093, -0.8864,  0.8215, -0.0091]],\n",
      "\n",
      "        [[ 0.8915,  0.8954, -0.9290,  0.8888,  0.8360],\n",
      "         [-0.6885, -0.3866,  0.7023, -0.0183, -0.8847]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0359 cost = 0.005252\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7918, -0.9268, -0.9301,  0.6487,  0.0528],\n",
      "         [-0.4598,  0.3751,  0.0917, -0.4870,  0.5511]],\n",
      "\n",
      "        [[-0.6690, -0.4148,  0.6679, -0.0200, -0.8591],\n",
      "         [ 0.8111,  0.8145, -0.8608,  0.8010,  0.8241]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0359 cost = 0.007178\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3558,  0.3830, -0.0557, -0.4292,  0.5566],\n",
      "         [ 0.9408, -0.4766, -0.5970,  0.9212, -0.6050]],\n",
      "\n",
      "        [[ 0.8659,  0.9032, -0.9123,  0.8904,  0.8518],\n",
      "         [-0.3533, -0.3906,  0.4967, -0.0763, -0.6634]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0359 cost = 0.012830\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7921, -0.9269, -0.9302,  0.6490,  0.0527],\n",
      "         [-0.4599,  0.3753,  0.0918, -0.4873,  0.5516]],\n",
      "\n",
      "        [[-0.6691, -0.4150,  0.6680, -0.0204, -0.8591],\n",
      "         [ 0.8113,  0.8147, -0.8609,  0.8012,  0.8243]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0360 cost = 0.007148\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9408, -0.4771, -0.5974,  0.9213, -0.6053],\n",
      "         [-0.3561,  0.3837, -0.0554, -0.4295,  0.5569]],\n",
      "\n",
      "        [[-0.3535, -0.3909,  0.4969, -0.0765, -0.6636],\n",
      "         [ 0.8660,  0.9033, -0.9123,  0.8905,  0.8520]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0360 cost = 0.012207\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.5439,  0.1150, -0.4351,  0.5311],\n",
      "         [ 0.8459, -0.8096, -0.8867,  0.8219, -0.0097]],\n",
      "\n",
      "        [[ 0.8918,  0.8956, -0.9291,  0.8891,  0.8365],\n",
      "         [-0.6889, -0.3872,  0.7027, -0.0192, -0.8849]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0360 cost = 0.005195\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8460, -0.8097, -0.8868,  0.8220, -0.0099],\n",
      "         [-0.3775,  0.5440,  0.1151, -0.4351,  0.5312]],\n",
      "\n",
      "        [[-0.6890, -0.3873,  0.7027, -0.0194, -0.8849],\n",
      "         [ 0.8918,  0.8956, -0.9291,  0.8891,  0.8366]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0361 cost = 0.004006\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3566,  0.3846, -0.0551, -0.4299,  0.5573],\n",
      "         [-0.4603,  0.3757,  0.0920, -0.4881,  0.5524]],\n",
      "\n",
      "        [[ 0.8662,  0.9034, -0.9124,  0.8906,  0.8521],\n",
      "         [ 0.8116,  0.8149, -0.8610,  0.8015,  0.8246]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0361 cost = 0.014556\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7926, -0.9270, -0.9303,  0.6497,  0.0525],\n",
      "         [ 0.9409, -0.4782, -0.5981,  0.9214, -0.6059]],\n",
      "\n",
      "        [[-0.6693, -0.4152,  0.6683, -0.0211, -0.8592],\n",
      "         [-0.3540, -0.3913,  0.4972, -0.0769, -0.6639]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0361 cost = 0.011033\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3572,  0.3853, -0.0549, -0.4303,  0.5576],\n",
      "         [ 0.8462, -0.8099, -0.8869,  0.8222, -0.0102]],\n",
      "\n",
      "        [[ 0.8663,  0.9035, -0.9125,  0.8908,  0.8523],\n",
      "         [-0.6892, -0.3875,  0.7030, -0.0199, -0.8850]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0362 cost = 0.007511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4608,  0.3761,  0.0921, -0.4889,  0.5531],\n",
      "         [ 0.7928, -0.9270, -0.9304,  0.6498,  0.0525]],\n",
      "\n",
      "        [[ 0.8120,  0.8152, -0.8612,  0.8018,  0.8250],\n",
      "         [-0.6694, -0.4153,  0.6684, -0.0213, -0.8592]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0362 cost = 0.009438\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9410, -0.4789, -0.5986,  0.9215, -0.6062],\n",
      "         [-0.3780,  0.5444,  0.1154, -0.4351,  0.5317]],\n",
      "\n",
      "        [[-0.3544, -0.3917,  0.4975, -0.0772, -0.6642],\n",
      "         [ 0.8921,  0.8958, -0.9292,  0.8893,  0.8371]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0362 cost = 0.009703\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3781,  0.5445,  0.1155, -0.4351,  0.5318],\n",
      "         [-0.3583,  0.3864, -0.0544, -0.4309,  0.5579]],\n",
      "\n",
      "        [[ 0.8922,  0.8958, -0.9292,  0.8894,  0.8372],\n",
      "         [ 0.8665,  0.9036, -0.9126,  0.8910,  0.8526]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0363 cost = 0.007960\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7932, -0.9271, -0.9305,  0.6503,  0.0524],\n",
      "         [ 0.8466, -0.8102, -0.8872,  0.8225, -0.0106]],\n",
      "\n",
      "        [[-0.6695, -0.4155,  0.6686, -0.0218, -0.8593],\n",
      "         [-0.6895, -0.3879,  0.7032, -0.0206, -0.8852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0363 cost = 0.005764\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9411, -0.4798, -0.5992,  0.9215, -0.6066],\n",
      "         [-0.4617,  0.3766,  0.0922, -0.4900,  0.5539]],\n",
      "\n",
      "        [[-0.3547, -0.3921,  0.4978, -0.0775, -0.6644],\n",
      "         [ 0.8123,  0.8155, -0.8614,  0.8021,  0.8254]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0363 cost = 0.013402\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8468, -0.8103, -0.8873,  0.8226, -0.0109],\n",
      "         [-0.3785,  0.5447,  0.1156, -0.4351,  0.5321]],\n",
      "\n",
      "        [[-0.6896, -0.3882,  0.7034, -0.0209, -0.8852],\n",
      "         [ 0.8923,  0.8959, -0.9293,  0.8895,  0.8374]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0364 cost = 0.003932\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9411, -0.4805, -0.5996,  0.9216, -0.6069],\n",
      "         [-0.4620,  0.3768,  0.0923, -0.4904,  0.5542]],\n",
      "\n",
      "        [[-0.3549, -0.3924,  0.4979, -0.0778, -0.6646],\n",
      "         [ 0.8125,  0.8156, -0.8615,  0.8022,  0.8255]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0364 cost = 0.013349\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3594,  0.3880, -0.0540, -0.4316,  0.5586],\n",
      "         [ 0.7937, -0.9272, -0.9306,  0.6509,  0.0523]],\n",
      "\n",
      "        [[ 0.8668,  0.9038, -0.9127,  0.8913,  0.8530],\n",
      "         [-0.6696, -0.4157,  0.6688, -0.0223, -0.8593]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0364 cost = 0.008266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3597,  0.3882, -0.0539, -0.4318,  0.5587],\n",
      "         [ 0.9412, -0.4814, -0.6002,  0.9217, -0.6073]],\n",
      "\n",
      "        [[ 0.8669,  0.9039, -0.9128,  0.8913,  0.8530],\n",
      "         [-0.3552, -0.3926,  0.4981, -0.0780, -0.6647]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0365 cost = 0.012396\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3790,  0.5450,  0.1159, -0.4352,  0.5324],\n",
      "         [ 0.7939, -0.9273, -0.9307,  0.6512,  0.0522]],\n",
      "\n",
      "        [[ 0.8925,  0.8960, -0.9293,  0.8897,  0.8378],\n",
      "         [-0.6697, -0.4158,  0.6689, -0.0226, -0.8594]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0365 cost = 0.005986\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8473, -0.8109, -0.8877,  0.8231, -0.0116],\n",
      "         [-0.4628,  0.3771,  0.0925, -0.4914,  0.5548]],\n",
      "\n",
      "        [[-0.6900, -0.3887,  0.7037, -0.0218, -0.8854],\n",
      "         [ 0.8128,  0.8158, -0.8616,  0.8024,  0.8258]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0365 cost = 0.006916\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3607,  0.3890, -0.0535, -0.4324,  0.5590],\n",
      "         [ 0.8474, -0.8110, -0.8877,  0.8231, -0.0118]],\n",
      "\n",
      "        [[ 0.8671,  0.9040, -0.9128,  0.8915,  0.8532],\n",
      "         [-0.6900, -0.3888,  0.7038, -0.0219, -0.8854]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0366 cost = 0.007312\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.5451,  0.1161, -0.4354,  0.5326],\n",
      "         [ 0.9413, -0.4829, -0.6010,  0.9218, -0.6080]],\n",
      "\n",
      "        [[ 0.8926,  0.8961, -0.9294,  0.8898,  0.8380],\n",
      "         [-0.3557, -0.3932,  0.4984, -0.0787, -0.6651]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0366 cost = 0.009905\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4636,  0.3772,  0.0926, -0.4924,  0.5553],\n",
      "         [ 0.7944, -0.9274, -0.9309,  0.6519,  0.0521]],\n",
      "\n",
      "        [[ 0.8130,  0.8159, -0.8617,  0.8026,  0.8260],\n",
      "         [-0.6698, -0.4161,  0.6690, -0.0232, -0.8594]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0366 cost = 0.009190\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8477, -0.8113, -0.8879,  0.8234, -0.0122],\n",
      "         [-0.4639,  0.3773,  0.0927, -0.4928,  0.5555]],\n",
      "\n",
      "        [[-0.6903, -0.3891,  0.7040, -0.0225, -0.8855],\n",
      "         [ 0.8131,  0.8159, -0.8617,  0.8026,  0.8260]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0367 cost = 0.006863\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3803,  0.5451,  0.1164, -0.4356,  0.5328],\n",
      "         [ 0.7948, -0.9275, -0.9309,  0.6523,  0.0521]],\n",
      "\n",
      "        [[ 0.8927,  0.8961, -0.9295,  0.8899,  0.8382],\n",
      "         [-0.6699, -0.4162,  0.6691, -0.0236, -0.8595]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0367 cost = 0.005910\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3626,  0.3903, -0.0527, -0.4334,  0.5595],\n",
      "         [ 0.9414, -0.4841, -0.6017,  0.9219, -0.6086]],\n",
      "\n",
      "        [[ 0.8674,  0.9042, -0.9130,  0.8918,  0.8535],\n",
      "         [-0.3561, -0.3937,  0.4987, -0.0792, -0.6654]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0367 cost = 0.012172\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9414, -0.4844, -0.6019,  0.9220, -0.6087],\n",
      "         [ 0.7951, -0.9276, -0.9310,  0.6528,  0.0520]],\n",
      "\n",
      "        [[-0.3562, -0.3939,  0.4988, -0.0794, -0.6655],\n",
      "         [-0.6700, -0.4163,  0.6692, -0.0239, -0.8596]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0368 cost = 0.013789\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3809,  0.5451,  0.1166, -0.4357,  0.5329],\n",
      "         [-0.3632,  0.3907, -0.0524, -0.4338,  0.5596]],\n",
      "\n",
      "        [[ 0.8929,  0.8962, -0.9295,  0.8900,  0.8383],\n",
      "         [ 0.8675,  0.9042, -0.9130,  0.8919,  0.8536]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0368 cost = 0.007733\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8482, -0.8117, -0.8882,  0.8238, -0.0129],\n",
      "         [-0.4654,  0.3773,  0.0930, -0.4944,  0.5561]],\n",
      "\n",
      "        [[-0.6907, -0.3897,  0.7043, -0.0235, -0.8857],\n",
      "         [ 0.8134,  0.8161, -0.8619,  0.8029,  0.8263]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0368 cost = 0.006798\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3636,  0.3912, -0.0521, -0.4341,  0.5599],\n",
      "         [ 0.8483, -0.8118, -0.8882,  0.8239, -0.0130]],\n",
      "\n",
      "        [[ 0.8676,  0.9043, -0.9131,  0.8920,  0.8537],\n",
      "         [-0.6907, -0.3898,  0.7044, -0.0237, -0.8857]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0369 cost = 0.007168\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7959, -0.9278, -0.9312,  0.6538,  0.0518],\n",
      "         [ 0.9416, -0.4859, -0.6029,  0.9221, -0.6094]],\n",
      "\n",
      "        [[-0.6701, -0.4166,  0.6694, -0.0248, -0.8597],\n",
      "         [-0.3567, -0.3945,  0.4991, -0.0800, -0.6659]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0369 cost = 0.010524\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3817,  0.5451,  0.1169, -0.4359,  0.5332],\n",
      "         [-0.4661,  0.3774,  0.0931, -0.4953,  0.5565]],\n",
      "\n",
      "        [[ 0.8930,  0.8962, -0.9296,  0.8901,  0.8386],\n",
      "         [ 0.8136,  0.8162, -0.8619,  0.8030,  0.8265]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0369 cost = 0.012822\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3818,  0.5452,  0.1169, -0.4358,  0.5333],\n",
      "         [ 0.9416, -0.4865, -0.6033,  0.9222, -0.6097]],\n",
      "\n",
      "        [[ 0.8931,  0.8963, -0.9296,  0.8902,  0.8387],\n",
      "         [-0.3570, -0.3948,  0.4993, -0.0803, -0.6661]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0370 cost = 0.009660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8488, -0.8122, -0.8885,  0.8242, -0.0135],\n",
      "         [-0.4662,  0.3777,  0.0932, -0.4955,  0.5570]],\n",
      "\n",
      "        [[-0.6910, -0.3903,  0.7047, -0.0245, -0.8859],\n",
      "         [ 0.8138,  0.8163, -0.8620,  0.8032,  0.8267]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0370 cost = 0.006733\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.7965, -0.9279, -0.9314,  0.6546,  0.0517],\n",
      "         [-0.3646,  0.3926, -0.0516, -0.4348,  0.5605]],\n",
      "\n",
      "        [[-0.6703, -0.4170,  0.6696, -0.0255, -0.8598],\n",
      "         [ 0.8679,  0.9045, -0.9132,  0.8923,  0.8540]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0370 cost = 0.005818\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4664,  0.3778,  0.0933, -0.4957,  0.5574],\n",
      "         [ 0.9417, -0.4875, -0.6039,  0.9223, -0.6101]],\n",
      "\n",
      "        [[ 0.8139,  0.8165, -0.8621,  0.8033,  0.8268],\n",
      "         [-0.3573, -0.3953,  0.4995, -0.0807, -0.6664]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0371 cost = 0.012253\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3650,  0.3932, -0.0513, -0.4350,  0.5607],\n",
      "         [-0.3824,  0.5454,  0.1172, -0.4358,  0.5337]],\n",
      "\n",
      "        [[ 0.8680,  0.9046, -0.9133,  0.8924,  0.8541],\n",
      "         [ 0.8933,  0.8964, -0.9297,  0.8903,  0.8391]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0371 cost = 0.006621\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8491, -0.8125, -0.8888,  0.8245, -0.0140],\n",
      "         [ 0.7968, -0.9280, -0.9315,  0.6550,  0.0516]],\n",
      "\n",
      "        [[-0.6913, -0.3907,  0.7050, -0.0253, -0.8860],\n",
      "         [-0.6704, -0.4171,  0.6698, -0.0260, -0.8599]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0371 cost = 0.009576\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3657,  0.3938, -0.0510, -0.4354,  0.5610],\n",
      "         [ 0.7970, -0.9280, -0.9316,  0.6552,  0.0516]],\n",
      "\n",
      "        [[ 0.8681,  0.9046, -0.9133,  0.8925,  0.8543],\n",
      "         [-0.6704, -0.4173,  0.6698, -0.0262, -0.8599]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0372 cost = 0.007869\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9418, -0.4883, -0.6046,  0.9224, -0.6105],\n",
      "         [ 0.8493, -0.8126, -0.8888,  0.8247, -0.0142]],\n",
      "\n",
      "        [[-0.3578, -0.3959,  0.4999, -0.0812, -0.6668],\n",
      "         [-0.6915, -0.3910,  0.7051, -0.0257, -0.8861]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0372 cost = 0.010939\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4675,  0.3783,  0.0935, -0.4969,  0.5585],\n",
      "         [-0.3829,  0.5456,  0.1174, -0.4358,  0.5342]],\n",
      "\n",
      "        [[ 0.8143,  0.8167, -0.8623,  0.8036,  0.8272],\n",
      "         [ 0.8935,  0.8965, -0.9297,  0.8905,  0.8394]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0372 cost = 0.009428\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8495, -0.8128, -0.8890,  0.8248, -0.0144],\n",
      "         [ 0.7975, -0.9281, -0.9317,  0.6558,  0.0514]],\n",
      "\n",
      "        [[-0.6917, -0.3914,  0.7053, -0.0261, -0.8862],\n",
      "         [-0.6706, -0.4176,  0.6700, -0.0268, -0.8600]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0373 cost = 0.009495\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4675,  0.3787,  0.0935, -0.4972,  0.5590],\n",
      "         [-0.3665,  0.3952, -0.0505, -0.4359,  0.5616]],\n",
      "\n",
      "        [[ 0.8144,  0.8168, -0.8624,  0.8037,  0.8273],\n",
      "         [ 0.8683,  0.9048, -0.9134,  0.8927,  0.8545]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0373 cost = 0.012136\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9419, -0.4893, -0.6054,  0.9225, -0.6110],\n",
      "         [-0.3824,  0.5462,  0.1175, -0.4352,  0.5346]],\n",
      "\n",
      "        [[-0.3583, -0.3965,  0.5002, -0.0816, -0.6671],\n",
      "         [ 0.8936,  0.8965, -0.9298,  0.8906,  0.8396]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0373 cost = 0.009042\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8498, -0.8130, -0.8892,  0.8250, -0.0148],\n",
      "         [ 0.7980, -0.9283, -0.9319,  0.6566,  0.0513]],\n",
      "\n",
      "        [[-0.6919, -0.3919,  0.7055, -0.0267, -0.8863],\n",
      "         [-0.6707, -0.4180,  0.6702, -0.0275, -0.8602]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0374 cost = 0.009430\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3664,  0.3966, -0.0504, -0.4359,  0.5621],\n",
      "         [-0.3819,  0.5466,  0.1176, -0.4347,  0.5349]],\n",
      "\n",
      "        [[ 0.8685,  0.9049, -0.9135,  0.8929,  0.8547],\n",
      "         [ 0.8937,  0.8966, -0.9298,  0.8907,  0.8397]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0374 cost = 0.006483\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9420, -0.4901, -0.6060,  0.9226, -0.6115],\n",
      "         [-0.4674,  0.3796,  0.0936, -0.4975,  0.5600]],\n",
      "\n",
      "        [[-0.3586, -0.3970,  0.5005, -0.0818, -0.6673],\n",
      "         [ 0.8148,  0.8171, -0.8626,  0.8041,  0.8277]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0374 cost = 0.012569\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4674,  0.3799,  0.0936, -0.4976,  0.5603],\n",
      "         [ 0.7986, -0.9284, -0.9320,  0.6573,  0.0511]],\n",
      "\n",
      "        [[ 0.8149,  0.8172, -0.8626,  0.8042,  0.8278],\n",
      "         [-0.6709, -0.4184,  0.6704, -0.0282, -0.8603]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0375 cost = 0.008680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9420, -0.4907, -0.6064,  0.9226, -0.6118],\n",
      "         [ 0.8503, -0.8133, -0.8894,  0.8253, -0.0154]],\n",
      "\n",
      "        [[-0.3588, -0.3972,  0.5006, -0.0820, -0.6675],\n",
      "         [-0.6923, -0.3926,  0.7058, -0.0276, -0.8865]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0375 cost = 0.010743\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3814,  0.5473,  0.1177, -0.4340,  0.5354],\n",
      "         [-0.3667,  0.3982, -0.0501, -0.4361,  0.5628]],\n",
      "\n",
      "        [[ 0.8938,  0.8966, -0.9299,  0.8909,  0.8400],\n",
      "         [ 0.8687,  0.9050, -0.9136,  0.8931,  0.8549]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0375 cost = 0.007408\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8505, -0.8135, -0.8896,  0.8255, -0.0157],\n",
      "         [-0.3667,  0.3986, -0.0500, -0.4361,  0.5629]],\n",
      "\n",
      "        [[-0.6924, -0.3929,  0.7060, -0.0280, -0.8866],\n",
      "         [ 0.8687,  0.9050, -0.9136,  0.8932,  0.8550]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0376 cost = 0.005748\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4676,  0.3806,  0.0937, -0.4981,  0.5611],\n",
      "         [ 0.7994, -0.9286, -0.9323,  0.6583,  0.0509]],\n",
      "\n",
      "        [[ 0.8152,  0.8174, -0.8628,  0.8045,  0.8281],\n",
      "         [-0.6711, -0.4188,  0.6706, -0.0291, -0.8605]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0376 cost = 0.008594\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9421, -0.4922, -0.6074,  0.9227, -0.6124],\n",
      "         [-0.3813,  0.5477,  0.1178, -0.4336,  0.5357]],\n",
      "\n",
      "        [[-0.3592, -0.3978,  0.5009, -0.0823, -0.6677],\n",
      "         [ 0.8939,  0.8967, -0.9299,  0.8910,  0.8403]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0376 cost = 0.008875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.5478,  0.1179, -0.4335,  0.5358],\n",
      "         [ 0.8508, -0.8137, -0.8898,  0.8258, -0.0161]],\n",
      "\n",
      "        [[ 0.8940,  0.8967, -0.9299,  0.8910,  0.8403],\n",
      "         [-0.6926, -0.3933,  0.7062, -0.0285, -0.8867]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0377 cost = 0.004674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3671,  0.4001, -0.0498, -0.4363,  0.5635],\n",
      "         [ 0.7999, -0.9287, -0.9324,  0.6591,  0.0508]],\n",
      "\n",
      "        [[ 0.8689,  0.9051, -0.9137,  0.8934,  0.8552],\n",
      "         [-0.6712, -0.4191,  0.6708, -0.0296, -0.8606]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0377 cost = 0.007575\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9422, -0.4933, -0.6081,  0.9228, -0.6130],\n",
      "         [-0.4681,  0.3811,  0.0939, -0.4988,  0.5617]],\n",
      "\n",
      "        [[-0.3595, -0.3982,  0.5011, -0.0826, -0.6679],\n",
      "         [ 0.8155,  0.8176, -0.8629,  0.8047,  0.8285]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0377 cost = 0.012349\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3815,  0.5480,  0.1180, -0.4333,  0.5360],\n",
      "         [-0.4683,  0.3812,  0.0939, -0.4990,  0.5618]],\n",
      "\n",
      "        [[ 0.8941,  0.8968, -0.9300,  0.8911,  0.8405],\n",
      "         [ 0.8155,  0.8176, -0.8630,  0.8048,  0.8285]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0378 cost = 0.012351\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8513, -0.8141, -0.8901,  0.8261, -0.0167],\n",
      "         [ 0.9423, -0.4940, -0.6086,  0.9228, -0.6133]],\n",
      "\n",
      "        [[-0.6929, -0.3938,  0.7064, -0.0292, -0.8869],\n",
      "         [-0.3597, -0.3984,  0.5013, -0.0828, -0.6681]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0378 cost = 0.010643\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8006, -0.9289, -0.9326,  0.6599,  0.0507],\n",
      "         [-0.3676,  0.4013, -0.0495, -0.4366,  0.5640]],\n",
      "\n",
      "        [[-0.6713, -0.4194,  0.6709, -0.0303, -0.8607],\n",
      "         [ 0.8691,  0.9053, -0.9138,  0.8936,  0.8555]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0378 cost = 0.005534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3678,  0.4016, -0.0494, -0.4367,  0.5642],\n",
      "         [ 0.8515, -0.8143, -0.8903,  0.8263, -0.0170]],\n",
      "\n",
      "        [[ 0.8692,  0.9053, -0.9138,  0.8937,  0.8556],\n",
      "         [-0.6931, -0.3940,  0.7066, -0.0295, -0.8870]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0379 cost = 0.006709\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3816,  0.5485,  0.1182, -0.4330,  0.5365],\n",
      "         [ 0.9423, -0.4952, -0.6092,  0.9229, -0.6138]],\n",
      "\n",
      "        [[ 0.8943,  0.8969, -0.9301,  0.8913,  0.8409],\n",
      "         [-0.3600, -0.3988,  0.5015, -0.0831, -0.6683]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0379 cost = 0.009109\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4686,  0.3820,  0.0941, -0.4994,  0.5628],\n",
      "         [ 0.8010, -0.9290, -0.9327,  0.6605,  0.0506]],\n",
      "\n",
      "        [[ 0.8160,  0.8180, -0.8632,  0.8052,  0.8290],\n",
      "         [-0.6714, -0.4197,  0.6711, -0.0308, -0.8608]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0379 cost = 0.008401\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3818,  0.5486,  0.1183, -0.4329,  0.5367],\n",
      "         [ 0.8012, -0.9290, -0.9328,  0.6607,  0.0505]],\n",
      "\n",
      "        [[ 0.8944,  0.8969, -0.9301,  0.8914,  0.8411],\n",
      "         [-0.6715, -0.4197,  0.6711, -0.0310, -0.8608]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0380 cost = 0.005411\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4690,  0.3821,  0.0941, -0.4998,  0.5631],\n",
      "         [ 0.9424, -0.4961, -0.6098,  0.9230, -0.6142]],\n",
      "\n",
      "        [[ 0.8161,  0.8181, -0.8633,  0.8053,  0.8291],\n",
      "         [-0.3603, -0.3992,  0.5018, -0.0835, -0.6685]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0380 cost = 0.011554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8520, -0.8147, -0.8906,  0.8267, -0.0177],\n",
      "         [-0.3687,  0.4031, -0.0489, -0.4372,  0.5647]],\n",
      "\n",
      "        [[-0.6934, -0.3947,  0.7069, -0.0304, -0.8871],\n",
      "         [ 0.8695,  0.9055, -0.9140,  0.8940,  0.8559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0380 cost = 0.005586\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9425, -0.4966, -0.6102,  0.9231, -0.6145],\n",
      "         [-0.3690,  0.4034, -0.0487, -0.4373,  0.5648]],\n",
      "\n",
      "        [[-0.3605, -0.3995,  0.5019, -0.0837, -0.6687],\n",
      "         [ 0.8695,  0.9055, -0.9140,  0.8940,  0.8560]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0381 cost = 0.010750\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3824,  0.5487,  0.1185, -0.4327,  0.5368],\n",
      "         [ 0.8018, -0.9291, -0.9330,  0.6616,  0.0504]],\n",
      "\n",
      "        [[ 0.8945,  0.8970, -0.9302,  0.8915,  0.8414],\n",
      "         [-0.6716, -0.4201,  0.6713, -0.0317, -0.8609]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0381 cost = 0.005361\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4699,  0.3824,  0.0943, -0.5006,  0.5636],\n",
      "         [ 0.8523, -0.8149, -0.8908,  0.8269, -0.0181]],\n",
      "\n",
      "        [[ 0.8164,  0.8182, -0.8634,  0.8055,  0.8294],\n",
      "         [-0.6936, -0.3951,  0.7071, -0.0309, -0.8872]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0381 cost = 0.006994\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3698,  0.4041, -0.0484, -0.4377,  0.5651],\n",
      "         [ 0.8022, -0.9292, -0.9331,  0.6621,  0.0503]],\n",
      "\n",
      "        [[ 0.8697,  0.9056, -0.9140,  0.8942,  0.8561],\n",
      "         [-0.6717, -0.4202,  0.6714, -0.0320, -0.8610]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0382 cost = 0.007343\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4705,  0.3824,  0.0944, -0.5011,  0.5638],\n",
      "         [-0.3830,  0.5487,  0.1187, -0.4327,  0.5369]],\n",
      "\n",
      "        [[ 0.8165,  0.8182, -0.8635,  0.8056,  0.8295],\n",
      "         [ 0.8947,  0.8971, -0.9302,  0.8916,  0.8416]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0382 cost = 0.008843\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8526, -0.8150, -0.8909,  0.8271, -0.0184],\n",
      "         [ 0.9426, -0.4980, -0.6112,  0.9232, -0.6151]],\n",
      "\n",
      "        [[-0.6938, -0.3954,  0.7073, -0.0314, -0.8873],\n",
      "         [-0.3610, -0.4002,  0.5023, -0.0843, -0.6691]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0382 cost = 0.010368\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8027, -0.9293, -0.9332,  0.6627,  0.0503],\n",
      "         [-0.4709,  0.3828,  0.0945, -0.5015,  0.5642]],\n",
      "\n",
      "        [[-0.6718, -0.4204,  0.6715, -0.0325, -0.8610],\n",
      "         [ 0.8167,  0.8184, -0.8636,  0.8058,  0.8297]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0383 cost = 0.006255\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3709,  0.4055, -0.0478, -0.4381,  0.5656],\n",
      "         [ 0.8527, -0.8152, -0.8910,  0.8273, -0.0186]],\n",
      "\n",
      "        [[ 0.8699,  0.9057, -0.9142,  0.8944,  0.8564],\n",
      "         [-0.6940, -0.3957,  0.7074, -0.0317, -0.8874]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0383 cost = 0.006521\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3828,  0.5491,  0.1189, -0.4321,  0.5373],\n",
      "         [ 0.9426, -0.4986, -0.6116,  0.9233, -0.6155]],\n",
      "\n",
      "        [[ 0.8948,  0.8972, -0.9303,  0.8918,  0.8419],\n",
      "         [-0.3613, -0.4005,  0.5025, -0.0846, -0.6693]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0383 cost = 0.008864\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8529, -0.8153, -0.8911,  0.8274, -0.0188],\n",
      "         [-0.3714,  0.4061, -0.0475, -0.4384,  0.5658]],\n",
      "\n",
      "        [[-0.6941, -0.3959,  0.7076, -0.0321, -0.8875],\n",
      "         [ 0.8700,  0.9058, -0.9142,  0.8945,  0.8565]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0384 cost = 0.005473\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4717,  0.3832,  0.0947, -0.5024,  0.5649],\n",
      "         [ 0.9427, -0.4991, -0.6119,  0.9233, -0.6157]],\n",
      "\n",
      "        [[ 0.8170,  0.8186, -0.8637,  0.8060,  0.8300],\n",
      "         [-0.3616, -0.4008,  0.5027, -0.0848, -0.6694]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0384 cost = 0.011264\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3830,  0.5493,  0.1191, -0.4319,  0.5375],\n",
      "         [ 0.8033, -0.9294, -0.9334,  0.6635,  0.0501]],\n",
      "\n",
      "        [[ 0.8949,  0.8972, -0.9303,  0.8919,  0.8420],\n",
      "         [-0.6720, -0.4208,  0.6718, -0.0333, -0.8612]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0384 cost = 0.005246\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8532, -0.8155, -0.8912,  0.8276, -0.0191],\n",
      "         [ 0.9427, -0.4995, -0.6122,  0.9234, -0.6160]],\n",
      "\n",
      "        [[-0.6943, -0.3963,  0.7078, -0.0326, -0.8876],\n",
      "         [-0.3618, -0.4011,  0.5028, -0.0851, -0.6696]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0385 cost = 0.010229\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3726,  0.4072, -0.0469, -0.4389,  0.5661],\n",
      "         [ 0.8036, -0.9295, -0.9335,  0.6639,  0.0500]],\n",
      "\n",
      "        [[ 0.8702,  0.9059, -0.9143,  0.8947,  0.8567],\n",
      "         [-0.6721, -0.4210,  0.6719, -0.0336, -0.8612]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0385 cost = 0.007178\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4725,  0.3834,  0.0949, -0.5033,  0.5654],\n",
      "         [-0.3833,  0.5494,  0.1193, -0.4318,  0.5376]],\n",
      "\n",
      "        [[ 0.8172,  0.8187, -0.8638,  0.8062,  0.8302],\n",
      "         [ 0.8950,  0.8972, -0.9304,  0.8920,  0.8422]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0385 cost = 0.008653\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9428, -0.5002, -0.6127,  0.9235, -0.6164],\n",
      "         [-0.3831,  0.5496,  0.1194, -0.4315,  0.5377]],\n",
      "\n",
      "        [[-0.3622, -0.4016,  0.5031, -0.0855, -0.6700],\n",
      "         [ 0.8951,  0.8972, -0.9304,  0.8920,  0.8422]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0386 cost = 0.008379\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8535, -0.8157, -0.8914,  0.8279, -0.0196],\n",
      "         [ 0.8041, -0.9296, -0.9336,  0.6645,  0.0499]],\n",
      "\n",
      "        [[-0.6946, -0.3969,  0.7081, -0.0334, -0.8877],\n",
      "         [-0.6723, -0.4213,  0.6721, -0.0342, -0.8613]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0386 cost = 0.008751\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3733,  0.4084, -0.0464, -0.4391,  0.5666],\n",
      "         [-0.4727,  0.3839,  0.0950, -0.5036,  0.5659]],\n",
      "\n",
      "        [[ 0.8703,  0.9060, -0.9143,  0.8949,  0.8568],\n",
      "         [ 0.8173,  0.8188, -0.8639,  0.8063,  0.8303]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0386 cost = 0.012970\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8537, -0.8159, -0.8915,  0.8281, -0.0199],\n",
      "         [-0.3736,  0.4088, -0.0463, -0.4392,  0.5668]],\n",
      "\n",
      "        [[-0.6948, -0.3972,  0.7082, -0.0338, -0.8878],\n",
      "         [ 0.8704,  0.9060, -0.9144,  0.8949,  0.8569]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0387 cost = 0.005375\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8045, -0.9297, -0.9337,  0.6651,  0.0498],\n",
      "         [-0.3827,  0.5502,  0.1195, -0.4309,  0.5382]],\n",
      "\n",
      "        [[-0.6724, -0.4216,  0.6722, -0.0347, -0.8614],\n",
      "         [ 0.8952,  0.8973, -0.9304,  0.8921,  0.8424]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0387 cost = 0.003328\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4731,  0.3844,  0.0951, -0.5041,  0.5666],\n",
      "         [ 0.9428, -0.5013, -0.6134,  0.9236, -0.6170]],\n",
      "\n",
      "        [[ 0.8176,  0.8190, -0.8640,  0.8066,  0.8306],\n",
      "         [-0.3627, -0.4023,  0.5034, -0.0860, -0.6704]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0387 cost = 0.011025\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3826,  0.5505,  0.1196, -0.4307,  0.5383],\n",
      "         [ 0.8539, -0.8161, -0.8916,  0.8282, -0.0201]],\n",
      "\n",
      "        [[ 0.8953,  0.8974, -0.9304,  0.8922,  0.8426],\n",
      "         [-0.6950, -0.3976,  0.7084, -0.0343, -0.8879]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0388 cost = 0.004363\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3747,  0.4102, -0.0457, -0.4396,  0.5673],\n",
      "         [-0.4735,  0.3846,  0.0951, -0.5045,  0.5670]],\n",
      "\n",
      "        [[ 0.8706,  0.9062, -0.9145,  0.8951,  0.8572],\n",
      "         [ 0.8178,  0.8191, -0.8641,  0.8067,  0.8308]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0388 cost = 0.012856\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8050, -0.9298, -0.9339,  0.6657,  0.0497],\n",
      "         [ 0.9429, -0.5016, -0.6138,  0.9236, -0.6172]],\n",
      "\n",
      "        [[-0.6725, -0.4219,  0.6724, -0.0353, -0.8615],\n",
      "         [-0.3630, -0.4027,  0.5037, -0.0862, -0.6706]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0388 cost = 0.009371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4739,  0.3850,  0.0951, -0.5050,  0.5675],\n",
      "         [ 0.8541, -0.8162, -0.8918,  0.8283, -0.0203]],\n",
      "\n",
      "        [[ 0.8180,  0.8193, -0.8643,  0.8069,  0.8310],\n",
      "         [-0.6952, -0.3980,  0.7086, -0.0348, -0.8880]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0389 cost = 0.006660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3758,  0.4114, -0.0453, -0.4400,  0.5677],\n",
      "         [ 0.9429, -0.5018, -0.6141,  0.9236, -0.6174]],\n",
      "\n",
      "        [[ 0.8708,  0.9063, -0.9146,  0.8953,  0.8574],\n",
      "         [-0.3632, -0.4030,  0.5038, -0.0863, -0.6707]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0389 cost = 0.010579\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3829,  0.5510,  0.1199, -0.4304,  0.5388],\n",
      "         [ 0.8053, -0.9299, -0.9340,  0.6660,  0.0496]],\n",
      "\n",
      "        [[ 0.8955,  0.8975, -0.9305,  0.8924,  0.8430],\n",
      "         [-0.6727, -0.4222,  0.6726, -0.0357, -0.8616]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0389 cost = 0.005086\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9429, -0.5021, -0.6144,  0.9236, -0.6175],\n",
      "         [ 0.8054, -0.9299, -0.9340,  0.6661,  0.0496]],\n",
      "\n",
      "        [[-0.3635, -0.4033,  0.5040, -0.0865, -0.6709],\n",
      "         [-0.6727, -0.4223,  0.6726, -0.0359, -0.8616]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0390 cost = 0.012107\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3832,  0.5511,  0.1200, -0.4303,  0.5389],\n",
      "         [-0.4750,  0.3855,  0.0952, -0.5060,  0.5683]],\n",
      "\n",
      "        [[ 0.8955,  0.8975, -0.9306,  0.8925,  0.8431],\n",
      "         [ 0.8183,  0.8195, -0.8644,  0.8072,  0.8313]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0390 cost = 0.011746\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8545, -0.8164, -0.8920,  0.8286, -0.0207],\n",
      "         [-0.3770,  0.4128, -0.0448, -0.4405,  0.5682]],\n",
      "\n",
      "        [[-0.6956, -0.3987,  0.7089, -0.0356, -0.8882],\n",
      "         [ 0.8710,  0.9064, -0.9147,  0.8955,  0.8577]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0390 cost = 0.005251\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3772,  0.4131, -0.0447, -0.4405,  0.5684],\n",
      "         [ 0.8546, -0.8164, -0.8921,  0.8287, -0.0208]],\n",
      "\n",
      "        [[ 0.8711,  0.9065, -0.9147,  0.8956,  0.8578],\n",
      "         [-0.6956, -0.3988,  0.7090, -0.0358, -0.8882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0391 cost = 0.006193\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8059, -0.9301, -0.9342,  0.6668,  0.0494],\n",
      "         [ 0.9430, -0.5030, -0.6151,  0.9237, -0.6179]],\n",
      "\n",
      "        [[-0.6728, -0.4226,  0.6728, -0.0366, -0.8617],\n",
      "         [-0.3639, -0.4039,  0.5043, -0.0869, -0.6712]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0391 cost = 0.009236\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4755,  0.3861,  0.0952, -0.5065,  0.5692],\n",
      "         [-0.3835,  0.5515,  0.1201, -0.4300,  0.5393]],\n",
      "\n",
      "        [[ 0.8187,  0.8198, -0.8646,  0.8075,  0.8317],\n",
      "         [ 0.8957,  0.8976, -0.9306,  0.8926,  0.8435]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0391 cost = 0.008298\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4755,  0.3864,  0.0952, -0.5065,  0.5695],\n",
      "         [-0.3833,  0.5518,  0.1201, -0.4298,  0.5394]],\n",
      "\n",
      "        [[ 0.8188,  0.8199, -0.8647,  0.8076,  0.8318],\n",
      "         [ 0.8958,  0.8976, -0.9307,  0.8927,  0.8435]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0392 cost = 0.008274\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8548, -0.8166, -0.8922,  0.8289, -0.0211],\n",
      "         [-0.3781,  0.4148, -0.0442, -0.4408,  0.5690]],\n",
      "\n",
      "        [[-0.6959, -0.3994,  0.7093, -0.0364, -0.8884],\n",
      "         [ 0.8713,  0.9066, -0.9148,  0.8958,  0.8581]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0392 cost = 0.005195\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9430, -0.5036, -0.6155,  0.9237, -0.6182],\n",
      "         [ 0.8063, -0.9301, -0.9343,  0.6673,  0.0493]],\n",
      "\n",
      "        [[-0.3643, -0.4044,  0.5046, -0.0870, -0.6715],\n",
      "         [-0.6729, -0.4229,  0.6730, -0.0371, -0.8618]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0392 cost = 0.011947\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4753,  0.3875,  0.0953, -0.5064,  0.5705],\n",
      "         [-0.3822,  0.5527,  0.1202, -0.4288,  0.5400]],\n",
      "\n",
      "        [[ 0.8191,  0.8202, -0.8649,  0.8079,  0.8321],\n",
      "         [ 0.8959,  0.8977, -0.9307,  0.8928,  0.8438]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0393 cost = 0.008188\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.4163, -0.0439, -0.4406,  0.5697],\n",
      "         [ 0.8550, -0.8166, -0.8923,  0.8289, -0.0212]],\n",
      "\n",
      "        [[ 0.8715,  0.9068, -0.9149,  0.8960,  0.8583],\n",
      "         [-0.6961, -0.3997,  0.7094, -0.0368, -0.8885]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0393 cost = 0.006093\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9430, -0.5042, -0.6159,  0.9237, -0.6185],\n",
      "         [ 0.8066, -0.9302, -0.9344,  0.6676,  0.0492]],\n",
      "\n",
      "        [[-0.3645, -0.4047,  0.5048, -0.0871, -0.6716],\n",
      "         [-0.6730, -0.4232,  0.6731, -0.0376, -0.8619]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0393 cost = 0.011894\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.5537,  0.1202, -0.4276,  0.5406],\n",
      "         [ 0.8068, -0.9302, -0.9344,  0.6678,  0.0491]],\n",
      "\n",
      "        [[ 0.8960,  0.8978, -0.9308,  0.8930,  0.8441],\n",
      "         [-0.6731, -0.4233,  0.6731, -0.0378, -0.8620]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0394 cost = 0.004951\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9430, -0.5048, -0.6162,  0.9237, -0.6188],\n",
      "         [-0.4748,  0.3889,  0.0953, -0.5062,  0.5717]],\n",
      "\n",
      "        [[-0.3647, -0.4049,  0.5050, -0.0872, -0.6717],\n",
      "         [ 0.8195,  0.8205, -0.8652,  0.8084,  0.8326]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0394 cost = 0.011286\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3785,  0.4180, -0.0436, -0.4404,  0.5705],\n",
      "         [ 0.8552, -0.8168, -0.8925,  0.8291, -0.0216]],\n",
      "\n",
      "        [[ 0.8717,  0.9069, -0.9150,  0.8962,  0.8585],\n",
      "         [-0.6963, -0.4002,  0.7097, -0.0375, -0.8886]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0394 cost = 0.006039\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9430, -0.5056, -0.6166,  0.9238, -0.6191],\n",
      "         [-0.3799,  0.5544,  0.1203, -0.4267,  0.5411]],\n",
      "\n",
      "        [[-0.3649, -0.4052,  0.5051, -0.0873, -0.6719],\n",
      "         [ 0.8962,  0.8979, -0.9308,  0.8931,  0.8442]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0395 cost = 0.007995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8554, -0.8170, -0.8926,  0.8293, -0.0219],\n",
      "         [ 0.8074, -0.9304, -0.9346,  0.6686,  0.0489]],\n",
      "\n",
      "        [[-0.6965, -0.4005,  0.7098, -0.0378, -0.8887],\n",
      "         [-0.6732, -0.4237,  0.6733, -0.0385, -0.8621]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0395 cost = 0.008334\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.4189, -0.0434, -0.4404,  0.5709],\n",
      "         [-0.4745,  0.3899,  0.0954, -0.5062,  0.5724]],\n",
      "\n",
      "        [[ 0.8718,  0.9069, -0.9151,  0.8963,  0.8587],\n",
      "         [ 0.8197,  0.8207, -0.8653,  0.8086,  0.8328]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0395 cost = 0.012350\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4745,  0.3902,  0.0954, -0.5063,  0.5726],\n",
      "         [-0.3789,  0.4193, -0.0433, -0.4405,  0.5711]],\n",
      "\n",
      "        [[ 0.8198,  0.8208, -0.8654,  0.8087,  0.8329],\n",
      "         [ 0.8718,  0.9070, -0.9151,  0.8964,  0.8588]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0396 cost = 0.010517\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8556, -0.8172, -0.8928,  0.8294, -0.0223],\n",
      "         [ 0.9431, -0.5070, -0.6175,  0.9238, -0.6197]],\n",
      "\n",
      "        [[-0.6967, -0.4009,  0.7100, -0.0383, -0.8888],\n",
      "         [-0.3653, -0.4057,  0.5054, -0.0876, -0.6721]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0396 cost = 0.009623\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8080, -0.9305, -0.9348,  0.6694,  0.0487],\n",
      "         [-0.3789,  0.5555,  0.1203, -0.4257,  0.5417]],\n",
      "\n",
      "        [[-0.6734, -0.4241,  0.6735, -0.0393, -0.8623],\n",
      "         [ 0.8963,  0.8980, -0.9309,  0.8933,  0.8446]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0396 cost = 0.003143\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4744,  0.3911,  0.0953, -0.5065,  0.5734],\n",
      "         [ 0.8081, -0.9306, -0.9349,  0.6695,  0.0487]],\n",
      "\n",
      "        [[ 0.8202,  0.8211, -0.8656,  0.8090,  0.8333],\n",
      "         [-0.6734, -0.4242,  0.6736, -0.0395, -0.8623]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0397 cost = 0.007451\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.4211, -0.0431, -0.4404,  0.5718],\n",
      "         [ 0.9431, -0.5077, -0.6180,  0.9239, -0.6200]],\n",
      "\n",
      "        [[ 0.8721,  0.9071, -0.9152,  0.8966,  0.8591],\n",
      "         [-0.3656, -0.4061,  0.5056, -0.0878, -0.6723]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0397 cost = 0.010077\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8559, -0.8174, -0.8931,  0.8297, -0.0227],\n",
      "         [-0.3785,  0.5560,  0.1204, -0.4251,  0.5421]],\n",
      "\n",
      "        [[-0.6970, -0.4015,  0.7102, -0.0390, -0.8890],\n",
      "         [ 0.8964,  0.8981, -0.9309,  0.8934,  0.8448]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0397 cost = 0.003187\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3795,  0.4219, -0.0429, -0.4405,  0.5720],\n",
      "         [-0.4747,  0.3917,  0.0954, -0.5068,  0.5740]],\n",
      "\n",
      "        [[ 0.8722,  0.9072, -0.9152,  0.8967,  0.8592],\n",
      "         [ 0.8204,  0.8213, -0.8657,  0.8092,  0.8335]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0398 cost = 0.012186\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.5563,  0.1205, -0.4249,  0.5423],\n",
      "         [ 0.8561, -0.8175, -0.8931,  0.8297, -0.0229]],\n",
      "\n",
      "        [[ 0.8965,  0.8981, -0.9310,  0.8935,  0.8450],\n",
      "         [-0.6971, -0.4017,  0.7104, -0.0393, -0.8891]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0398 cost = 0.004104\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9431, -0.5084, -0.6185,  0.9239, -0.6204],\n",
      "         [ 0.8087, -0.9307, -0.9350,  0.6703,  0.0485]],\n",
      "\n",
      "        [[-0.3660, -0.4065,  0.5059, -0.0879, -0.6725],\n",
      "         [-0.6736, -0.4246,  0.6738, -0.0403, -0.8624]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0398 cost = 0.011562\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9432, -0.5087, -0.6187,  0.9239, -0.6205],\n",
      "         [ 0.8089, -0.9307, -0.9351,  0.6705,  0.0485]],\n",
      "\n",
      "        [[-0.3660, -0.4067,  0.5060, -0.0880, -0.6726],\n",
      "         [-0.6736, -0.4247,  0.6739, -0.0404, -0.8625]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0399 cost = 0.011539\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.5567,  0.1205, -0.4246,  0.5425],\n",
      "         [-0.3803,  0.4235, -0.0426, -0.4407,  0.5726]],\n",
      "\n",
      "        [[ 0.8966,  0.8982, -0.9310,  0.8936,  0.8452],\n",
      "         [ 0.8724,  0.9073, -0.9154,  0.8970,  0.8595]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0399 cost = 0.006455\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8564, -0.8177, -0.8934,  0.8300, -0.0234],\n",
      "         [-0.4751,  0.3927,  0.0954, -0.5074,  0.5751]],\n",
      "\n",
      "        [[-0.6974, -0.4023,  0.7106, -0.0399, -0.8892],\n",
      "         [ 0.8208,  0.8216, -0.8660,  0.8096,  0.8340]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0399 cost = 0.005646\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8565, -0.8178, -0.8935,  0.8301, -0.0235],\n",
      "         [-0.3783,  0.5570,  0.1206, -0.4244,  0.5427]],\n",
      "\n",
      "        [[-0.6974, -0.4024,  0.7107, -0.0401, -0.8893],\n",
      "         [ 0.8967,  0.8982, -0.9311,  0.8937,  0.8454]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0400 cost = 0.003142\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4752,  0.3930,  0.0954, -0.5076,  0.5754],\n",
      "         [ 0.8095, -0.9309, -0.9353,  0.6715,  0.0482]],\n",
      "\n",
      "        [[ 0.8210,  0.8218, -0.8661,  0.8098,  0.8342],\n",
      "         [-0.6738, -0.4251,  0.6741, -0.0412, -0.8626]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0400 cost = 0.007282\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3804,  0.4247, -0.0424, -0.4409,  0.5732],\n",
      "         [ 0.9433, -0.5104, -0.6199,  0.9240, -0.6212]],\n",
      "\n",
      "        [[ 0.8726,  0.9075, -0.9154,  0.8972,  0.8597],\n",
      "         [-0.3665, -0.4074,  0.5063, -0.0884, -0.6729]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0400 cost = 0.009881\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8568, -0.8180, -0.8937,  0.8303, -0.0240],\n",
      "         [-0.3806,  0.4250, -0.0423, -0.4409,  0.5733]],\n",
      "\n",
      "        [[-0.6976, -0.4028,  0.7109, -0.0406, -0.8894],\n",
      "         [ 0.8726,  0.9075, -0.9155,  0.8972,  0.8598]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0401 cost = 0.004927\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4756,  0.3933,  0.0955, -0.5081,  0.5759],\n",
      "         [-0.3785,  0.5573,  0.1207, -0.4241,  0.5430]],\n",
      "\n",
      "        [[ 0.8212,  0.8219, -0.8662,  0.8100,  0.8344],\n",
      "         [ 0.8969,  0.8983, -0.9311,  0.8938,  0.8457]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0401 cost = 0.007676\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8101, -0.9311, -0.9355,  0.6722,  0.0481],\n",
      "         [ 0.9433, -0.5110, -0.6203,  0.9241, -0.6216]],\n",
      "\n",
      "        [[-0.6739, -0.4255,  0.6742, -0.0419, -0.8627],\n",
      "         [-0.3668, -0.4077,  0.5065, -0.0886, -0.6731]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0401 cost = 0.008747\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9434, -0.5113, -0.6205,  0.9241, -0.6217],\n",
      "         [-0.3811,  0.4261, -0.0420, -0.4410,  0.5737]],\n",
      "\n",
      "        [[-0.3669, -0.4079,  0.5066, -0.0887, -0.6732],\n",
      "         [ 0.8728,  0.9076, -0.9155,  0.8974,  0.8599]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0402 cost = 0.009566\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8104, -0.9311, -0.9356,  0.6726,  0.0480],\n",
      "         [ 0.8571, -0.8183, -0.8939,  0.8305, -0.0244]],\n",
      "\n",
      "        [[-0.6740, -0.4256,  0.6743, -0.0422, -0.8628],\n",
      "         [-0.6979, -0.4033,  0.7111, -0.0413, -0.8895]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0402 cost = 0.004552\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3779,  0.5579,  0.1208, -0.4234,  0.5435],\n",
      "         [-0.4757,  0.3941,  0.0955, -0.5083,  0.5766]],\n",
      "\n",
      "        [[ 0.8970,  0.8984, -0.9312,  0.8939,  0.8459],\n",
      "         [ 0.8215,  0.8221, -0.8663,  0.8102,  0.8346]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0402 cost = 0.011057\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.4271, -0.0418, -0.4410,  0.5741],\n",
      "         [-0.3778,  0.5581,  0.1208, -0.4232,  0.5436]],\n",
      "\n",
      "        [[ 0.8729,  0.9077, -0.9156,  0.8975,  0.8601],\n",
      "         [ 0.8970,  0.8984, -0.9312,  0.8940,  0.8460]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0403 cost = 0.005312\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8573, -0.8184, -0.8940,  0.8307, -0.0247],\n",
      "         [ 0.8107, -0.9312, -0.9357,  0.6730,  0.0479]],\n",
      "\n",
      "        [[-0.6981, -0.4036,  0.7113, -0.0417, -0.8896],\n",
      "         [-0.6740, -0.4259,  0.6744, -0.0426, -0.8629]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0403 cost = 0.007951\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4757,  0.3948,  0.0955, -0.5083,  0.5775],\n",
      "         [ 0.9434, -0.5126, -0.6213,  0.9242, -0.6223]],\n",
      "\n",
      "        [[ 0.8218,  0.8223, -0.8665,  0.8104,  0.8349],\n",
      "         [-0.3673, -0.4085,  0.5070, -0.0891, -0.6735]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0403 cost = 0.010035\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8110, -0.9313, -0.9357,  0.6734,  0.0478],\n",
      "         [ 0.9435, -0.5128, -0.6215,  0.9242, -0.6224]],\n",
      "\n",
      "        [[-0.6741, -0.4261,  0.6745, -0.0430, -0.8629],\n",
      "         [-0.3674, -0.4087,  0.5070, -0.0892, -0.6736]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0404 cost = 0.008637\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4757,  0.3951,  0.0955, -0.5084,  0.5779],\n",
      "         [-0.3818,  0.4285, -0.0415, -0.4411,  0.5747]],\n",
      "\n",
      "        [[ 0.8219,  0.8224, -0.8666,  0.8105,  0.8351],\n",
      "         [ 0.8731,  0.9078, -0.9157,  0.8977,  0.8604]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0404 cost = 0.009958\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3773,  0.5589,  0.1209, -0.4224,  0.5443],\n",
      "         [ 0.8576, -0.8186, -0.8942,  0.8309, -0.0251]],\n",
      "\n",
      "        [[ 0.8972,  0.8985, -0.9313,  0.8942,  0.8464],\n",
      "         [-0.6984, -0.4043,  0.7115, -0.0424, -0.8897]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0404 cost = 0.003953\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8577, -0.8187, -0.8943,  0.8310, -0.0252],\n",
      "         [-0.3772,  0.5591,  0.1209, -0.4222,  0.5444]],\n",
      "\n",
      "        [[-0.6984, -0.4044,  0.7116, -0.0425, -0.8898],\n",
      "         [ 0.8973,  0.8985, -0.9313,  0.8942,  0.8464]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0405 cost = 0.003052\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8115, -0.9314, -0.9359,  0.6741,  0.0476],\n",
      "         [-0.3818,  0.4298, -0.0414, -0.4410,  0.5752]],\n",
      "\n",
      "        [[-0.6743, -0.4265,  0.6747, -0.0436, -0.8631],\n",
      "         [ 0.8732,  0.9079, -0.9158,  0.8978,  0.8605]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0405 cost = 0.004703\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9436, -0.5137, -0.6224,  0.9243, -0.6228],\n",
      "         [-0.4756,  0.3959,  0.0956, -0.5084,  0.5787]],\n",
      "\n",
      "        [[-0.3680, -0.4094,  0.5074, -0.0896, -0.6740],\n",
      "         [ 0.8222,  0.8227, -0.8667,  0.8109,  0.8354]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0405 cost = 0.010627\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9436, -0.5139, -0.6225,  0.9243, -0.6230],\n",
      "         [-0.3769,  0.5595,  0.1210, -0.4217,  0.5447]],\n",
      "\n",
      "        [[-0.3681, -0.4095,  0.5075, -0.0897, -0.6741],\n",
      "         [ 0.8974,  0.8986, -0.9313,  0.8943,  0.8466]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0406 cost = 0.007527\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8118, -0.9315, -0.9360,  0.6745,  0.0475],\n",
      "         [ 0.8580, -0.8189, -0.8945,  0.8313, -0.0257]],\n",
      "\n",
      "        [[-0.6744, -0.4268,  0.6749, -0.0440, -0.8632],\n",
      "         [-0.6987, -0.4049,  0.7118, -0.0431, -0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0406 cost = 0.004451\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3819,  0.4309, -0.0412, -0.4409,  0.5757],\n",
      "         [-0.4756,  0.3963,  0.0956, -0.5085,  0.5791]],\n",
      "\n",
      "        [[ 0.8734,  0.9080, -0.9158,  0.8980,  0.8607],\n",
      "         [ 0.8223,  0.8228, -0.8668,  0.8110,  0.8356]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0406 cost = 0.011700\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8121, -0.9315, -0.9361,  0.6748,  0.0474],\n",
      "         [-0.4757,  0.3965,  0.0956, -0.5086,  0.5793]],\n",
      "\n",
      "        [[-0.6744, -0.4269,  0.6750, -0.0443, -0.8632],\n",
      "         [ 0.8224,  0.8229, -0.8669,  0.8111,  0.8357]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0407 cost = 0.005439\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9437, -0.5150, -0.6233,  0.9244, -0.6234],\n",
      "         [-0.3822,  0.4316, -0.0410, -0.4410,  0.5759]],\n",
      "\n",
      "        [[-0.3685, -0.4100,  0.5078, -0.0900, -0.6743],\n",
      "         [ 0.8735,  0.9080, -0.9159,  0.8981,  0.8609]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0407 cost = 0.009296\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8583, -0.8191, -0.8947,  0.8315, -0.0262],\n",
      "         [-0.3768,  0.5600,  0.1211, -0.4213,  0.5452]],\n",
      "\n",
      "        [[-0.6989, -0.4053,  0.7121, -0.0437, -0.8900],\n",
      "         [ 0.8975,  0.8987, -0.9314,  0.8945,  0.8470]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0407 cost = 0.003006\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3826,  0.4321, -0.0409, -0.4411,  0.5762],\n",
      "         [-0.4759,  0.3969,  0.0957, -0.5090,  0.5799]],\n",
      "\n",
      "        [[ 0.8736,  0.9081, -0.9159,  0.8982,  0.8610],\n",
      "         [ 0.8227,  0.8231, -0.8670,  0.8113,  0.8359]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0408 cost = 0.011619\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9437, -0.5157, -0.6238,  0.9244, -0.6237],\n",
      "         [ 0.8123, -0.9316, -0.9362,  0.6751,  0.0473]],\n",
      "\n",
      "        [[-0.3687, -0.4103,  0.5080, -0.0902, -0.6745],\n",
      "         [-0.6745, -0.4271,  0.6751, -0.0447, -0.8632]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0408 cost = 0.010971\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3769,  0.5604,  0.1212, -0.4212,  0.5455],\n",
      "         [ 0.8584, -0.8193, -0.8949,  0.8316, -0.0265]],\n",
      "\n",
      "        [[ 0.8977,  0.8988, -0.9314,  0.8946,  0.8473],\n",
      "         [-0.6991, -0.4055,  0.7122, -0.0441, -0.8901]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0408 cost = 0.003867\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8125, -0.9316, -0.9362,  0.6753,  0.0472],\n",
      "         [ 0.9437, -0.5163, -0.6241,  0.9245, -0.6240]],\n",
      "\n",
      "        [[-0.6746, -0.4272,  0.6752, -0.0450, -0.8633],\n",
      "         [-0.3689, -0.4105,  0.5081, -0.0904, -0.6746]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0409 cost = 0.008419\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4763,  0.3976,  0.0957, -0.5096,  0.5808],\n",
      "         [-0.3770,  0.5605,  0.1212, -0.4211,  0.5457]],\n",
      "\n",
      "        [[ 0.8230,  0.8234, -0.8672,  0.8115,  0.8363],\n",
      "         [ 0.8977,  0.8988, -0.9315,  0.8946,  0.8474]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0409 cost = 0.007272\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3835,  0.4336, -0.0405, -0.4414,  0.5768],\n",
      "         [ 0.8586, -0.8195, -0.8951,  0.8318, -0.0269]],\n",
      "\n",
      "        [[ 0.8739,  0.9083, -0.9160,  0.8984,  0.8614],\n",
      "         [-0.6993, -0.4058,  0.7124, -0.0445, -0.8902]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0409 cost = 0.005497\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9438, -0.5172, -0.6247,  0.9245, -0.6244],\n",
      "         [ 0.8128, -0.9317, -0.9363,  0.6757,  0.0471]],\n",
      "\n",
      "        [[-0.3692, -0.4109,  0.5083, -0.0906, -0.6748],\n",
      "         [-0.6747, -0.4274,  0.6754, -0.0454, -0.8633]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0410 cost = 0.010875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4763,  0.3983,  0.0957, -0.5097,  0.5814],\n",
      "         [-0.3837,  0.4343, -0.0404, -0.4414,  0.5771]],\n",
      "\n",
      "        [[ 0.8232,  0.8236, -0.8673,  0.8117,  0.8366],\n",
      "         [ 0.8739,  0.9084, -0.9161,  0.8985,  0.8615]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0410 cost = 0.009601\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8588, -0.8197, -0.8952,  0.8319, -0.0272],\n",
      "         [-0.3763,  0.5613,  0.1213, -0.4204,  0.5463]],\n",
      "\n",
      "        [[-0.6995, -0.4062,  0.7126, -0.0450, -0.8903],\n",
      "         [ 0.8979,  0.8989, -0.9315,  0.8948,  0.8477]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0410 cost = 0.002956\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.5615,  0.1213, -0.4202,  0.5464],\n",
      "         [-0.4762,  0.3988,  0.0957, -0.5097,  0.5818]],\n",
      "\n",
      "        [[ 0.8979,  0.8990, -0.9315,  0.8948,  0.8478],\n",
      "         [ 0.8234,  0.8237, -0.8674,  0.8119,  0.8368]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0411 cost = 0.010638\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9439, -0.5182, -0.6254,  0.9246, -0.6248],\n",
      "         [-0.3837,  0.4356, -0.0403, -0.4413,  0.5776]],\n",
      "\n",
      "        [[-0.3695, -0.4113,  0.5086, -0.0907, -0.6750],\n",
      "         [ 0.8741,  0.9085, -0.9161,  0.8987,  0.8617]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0411 cost = 0.009100\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8133, -0.9318, -0.9365,  0.6763,  0.0469],\n",
      "         [ 0.8590, -0.8197, -0.8954,  0.8320, -0.0275]],\n",
      "\n",
      "        [[-0.6748, -0.4277,  0.6755, -0.0461, -0.8634],\n",
      "         [-0.6996, -0.4064,  0.7127, -0.0453, -0.8904]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0411 cost = 0.004333\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3756,  0.5621,  0.1213, -0.4196,  0.5469],\n",
      "         [-0.4759,  0.3997,  0.0957, -0.5095,  0.5827]],\n",
      "\n",
      "        [[ 0.8981,  0.8991, -0.9316,  0.8950,  0.8481],\n",
      "         [ 0.8237,  0.8240, -0.8676,  0.8122,  0.8371]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0412 cost = 0.010572\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9439, -0.5190, -0.6260,  0.9246, -0.6251],\n",
      "         [ 0.8592, -0.8198, -0.8955,  0.8321, -0.0277]],\n",
      "\n",
      "        [[-0.3698, -0.4115,  0.5088, -0.0908, -0.6751],\n",
      "         [-0.6998, -0.4066,  0.7128, -0.0456, -0.8905]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0412 cost = 0.008766\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3836,  0.4372, -0.0403, -0.4411,  0.5784],\n",
      "         [ 0.8135, -0.9319, -0.9366,  0.6766,  0.0468]],\n",
      "\n",
      "        [[ 0.8743,  0.9086, -0.9163,  0.8989,  0.8621],\n",
      "         [-0.6749, -0.4279,  0.6756, -0.0464, -0.8635]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0412 cost = 0.005993\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3752,  0.5627,  0.1213, -0.4189,  0.5473],\n",
      "         [ 0.8593, -0.8199, -0.8957,  0.8323, -0.0279]],\n",
      "\n",
      "        [[ 0.8982,  0.8992, -0.9317,  0.8951,  0.8484],\n",
      "         [-0.6999, -0.4068,  0.7130, -0.0459, -0.8905]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0413 cost = 0.003774\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9440, -0.5199, -0.6266,  0.9246, -0.6254],\n",
      "         [-0.4754,  0.4008,  0.0956, -0.5092,  0.5838]],\n",
      "\n",
      "        [[-0.3700, -0.4118,  0.5090, -0.0909, -0.6752],\n",
      "         [ 0.8242,  0.8244, -0.8679,  0.8126,  0.8376]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0413 cost = 0.010213\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8138, -0.9319, -0.9367,  0.6770,  0.0467],\n",
      "         [-0.3837,  0.4383, -0.0402, -0.4410,  0.5789]],\n",
      "\n",
      "        [[-0.6749, -0.4281,  0.6757, -0.0468, -0.8635],\n",
      "         [ 0.8745,  0.9087, -0.9163,  0.8991,  0.8623]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0413 cost = 0.004491\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8596, -0.8201, -0.8959,  0.8325, -0.0283],\n",
      "         [ 0.8139, -0.9320, -0.9367,  0.6771,  0.0466]],\n",
      "\n",
      "        [[-0.7000, -0.4071,  0.7131, -0.0463, -0.8906],\n",
      "         [-0.6749, -0.4281,  0.6757, -0.0469, -0.8635]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0414 cost = 0.007534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4753,  0.4013,  0.0956, -0.5092,  0.5844],\n",
      "         [-0.3837,  0.4388, -0.0401, -0.4410,  0.5791]],\n",
      "\n",
      "        [[ 0.8244,  0.8246, -0.8681,  0.8128,  0.8378],\n",
      "         [ 0.8746,  0.9088, -0.9164,  0.8992,  0.8624]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0414 cost = 0.009336\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3748,  0.5633,  0.1213, -0.4183,  0.5480],\n",
      "         [ 0.9441, -0.5211, -0.6274,  0.9247, -0.6259]],\n",
      "\n",
      "        [[ 0.8984,  0.8993, -0.9318,  0.8953,  0.8488],\n",
      "         [-0.3703, -0.4122,  0.5092, -0.0911, -0.6754]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0414 cost = 0.007477\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3748,  0.5634,  0.1213, -0.4181,  0.5481],\n",
      "         [ 0.8143, -0.9320, -0.9368,  0.6776,  0.0465]],\n",
      "\n",
      "        [[ 0.8985,  0.8993, -0.9318,  0.8954,  0.8489],\n",
      "         [-0.6750, -0.4284,  0.6759, -0.0474, -0.8636]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0415 cost = 0.004360\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9441, -0.5216, -0.6278,  0.9247, -0.6261],\n",
      "         [-0.4751,  0.4019,  0.0956, -0.5091,  0.5850]],\n",
      "\n",
      "        [[-0.3705, -0.4124,  0.5093, -0.0912, -0.6755],\n",
      "         [ 0.8247,  0.8248, -0.8682,  0.8131,  0.8381]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0415 cost = 0.010109\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8600, -0.8204, -0.8962,  0.8328, -0.0290],\n",
      "         [-0.3834,  0.4402, -0.0401, -0.4409,  0.5797]],\n",
      "\n",
      "        [[-0.7004, -0.4076,  0.7134, -0.0471, -0.8908],\n",
      "         [ 0.8748,  0.9089, -0.9165,  0.8993,  0.8627]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0415 cost = 0.004532\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9442, -0.5223, -0.6282,  0.9248, -0.6264],\n",
      "         [ 0.8147, -0.9321, -0.9370,  0.6782,  0.0463]],\n",
      "\n",
      "        [[-0.3707, -0.4127,  0.5095, -0.0914, -0.6756],\n",
      "         [-0.6751, -0.4286,  0.6760, -0.0478, -0.8637]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0416 cost = 0.010545\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3746,  0.5637,  0.1214, -0.4176,  0.5485],\n",
      "         [-0.3833,  0.4406, -0.0400, -0.4409,  0.5799]],\n",
      "\n",
      "        [[ 0.8986,  0.8994, -0.9318,  0.8955,  0.8492],\n",
      "         [ 0.8749,  0.9089, -0.9165,  0.8994,  0.8628]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0416 cost = 0.005845\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4748,  0.4024,  0.0956, -0.5091,  0.5856],\n",
      "         [ 0.8603, -0.8207, -0.8964,  0.8331, -0.0296]],\n",
      "\n",
      "        [[ 0.8249,  0.8250, -0.8683,  0.8133,  0.8384],\n",
      "         [-0.7005, -0.4080,  0.7136, -0.0475, -0.8909]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0416 cost = 0.005595\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8152, -0.9322, -0.9371,  0.6789,  0.0461],\n",
      "         [-0.4748,  0.4025,  0.0956, -0.5092,  0.5857]],\n",
      "\n",
      "        [[-0.6752, -0.4288,  0.6761, -0.0483, -0.8638],\n",
      "         [ 0.8250,  0.8250, -0.8684,  0.8133,  0.8384]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0417 cost = 0.005149\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3746,  0.5639,  0.1214, -0.4173,  0.5488],\n",
      "         [ 0.9443, -0.5237, -0.6291,  0.9249, -0.6270]],\n",
      "\n",
      "        [[ 0.8987,  0.8995, -0.9319,  0.8956,  0.8494],\n",
      "         [-0.3710, -0.4131,  0.5097, -0.0918, -0.6759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0417 cost = 0.007362\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3831,  0.4417, -0.0399, -0.4409,  0.5804],\n",
      "         [ 0.8606, -0.8208, -0.8966,  0.8333, -0.0300]],\n",
      "\n",
      "        [[ 0.8750,  0.9090, -0.9166,  0.8996,  0.8630],\n",
      "         [-0.7007, -0.4083,  0.7138, -0.0480, -0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0417 cost = 0.005250\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3747,  0.5640,  0.1215, -0.4172,  0.5489],\n",
      "         [ 0.8607, -0.8209, -0.8967,  0.8334, -0.0302]],\n",
      "\n",
      "        [[ 0.8988,  0.8995, -0.9319,  0.8956,  0.8496],\n",
      "         [-0.7008, -0.4084,  0.7138, -0.0481, -0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0418 cost = 0.003664\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8157, -0.9323, -0.9372,  0.6796,  0.0459],\n",
      "         [-0.4750,  0.4028,  0.0957, -0.5095,  0.5862]],\n",
      "\n",
      "        [[-0.6754, -0.4291,  0.6763, -0.0489, -0.8639],\n",
      "         [ 0.8252,  0.8252, -0.8685,  0.8135,  0.8387]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0418 cost = 0.005114\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9444, -0.5247, -0.6298,  0.9251, -0.6275],\n",
      "         [-0.3833,  0.4424, -0.0397, -0.4410,  0.5806]],\n",
      "\n",
      "        [[-0.3714, -0.4136,  0.5100, -0.0922, -0.6761],\n",
      "         [ 0.8751,  0.9091, -0.9166,  0.8997,  0.8631]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0418 cost = 0.008755\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9445, -0.5251, -0.6300,  0.9251, -0.6276],\n",
      "         [-0.3834,  0.4426, -0.0396, -0.4411,  0.5807]],\n",
      "\n",
      "        [[-0.3715, -0.4137,  0.5100, -0.0923, -0.6761],\n",
      "         [ 0.8752,  0.9091, -0.9166,  0.8997,  0.8632]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0419 cost = 0.008739\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3750,  0.5640,  0.1216, -0.4171,  0.5492],\n",
      "         [ 0.8161, -0.9324, -0.9373,  0.6800,  0.0458]],\n",
      "\n",
      "        [[ 0.8989,  0.8996, -0.9319,  0.8957,  0.8498],\n",
      "         [-0.6755, -0.4292,  0.6765, -0.0494, -0.8639]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0419 cost = 0.004245\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8612, -0.8213, -0.8970,  0.8338, -0.0310],\n",
      "         [-0.4753,  0.4030,  0.0959, -0.5101,  0.5866]],\n",
      "\n",
      "        [[-0.7011, -0.4088,  0.7141, -0.0489, -0.8911],\n",
      "         [ 0.8254,  0.8254, -0.8686,  0.8136,  0.8389]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0419 cost = 0.005057\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8163, -0.9325, -0.9374,  0.6804,  0.0457],\n",
      "         [ 0.9446, -0.5262, -0.6306,  0.9252, -0.6281]],\n",
      "\n",
      "        [[-0.6756, -0.4293,  0.6766, -0.0497, -0.8639],\n",
      "         [-0.3717, -0.4140,  0.5102, -0.0927, -0.6763]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0420 cost = 0.007960\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3753,  0.5640,  0.1218, -0.4170,  0.5494],\n",
      "         [ 0.8614, -0.8215, -0.8971,  0.8340, -0.0314]],\n",
      "\n",
      "        [[ 0.8990,  0.8996, -0.9320,  0.8958,  0.8500],\n",
      "         [-0.7012, -0.4090,  0.7142, -0.0492, -0.8912]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0420 cost = 0.003616\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3838,  0.4432, -0.0393, -0.4414,  0.5810],\n",
      "         [-0.4755,  0.4030,  0.0960, -0.5104,  0.5867]],\n",
      "\n",
      "        [[ 0.8753,  0.9092, -0.9167,  0.8999,  0.8634],\n",
      "         [ 0.8255,  0.8254, -0.8686,  0.8137,  0.8390]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0420 cost = 0.010984\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9447, -0.5273, -0.6312,  0.9254, -0.6286],\n",
      "         [ 0.8615, -0.8217, -0.8972,  0.8341, -0.0318]],\n",
      "\n",
      "        [[-0.3720, -0.4144,  0.5104, -0.0931, -0.6766],\n",
      "         [-0.7014, -0.4092,  0.7144, -0.0496, -0.8912]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0421 cost = 0.008342\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4757,  0.4031,  0.0960, -0.5107,  0.5870],\n",
      "         [-0.3756,  0.5640,  0.1219, -0.4171,  0.5496]],\n",
      "\n",
      "        [[ 0.8256,  0.8255, -0.8686,  0.8138,  0.8391],\n",
      "         [ 0.8991,  0.8997, -0.9320,  0.8959,  0.8501]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0421 cost = 0.006758\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3842,  0.4439, -0.0390, -0.4416,  0.5814],\n",
      "         [ 0.8168, -0.9326, -0.9376,  0.6811,  0.0454]],\n",
      "\n",
      "        [[ 0.8755,  0.9093, -0.9168,  0.9000,  0.8635],\n",
      "         [-0.6758, -0.4296,  0.6768, -0.0503, -0.8640]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0421 cost = 0.005699\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8169, -0.9327, -0.9376,  0.6812,  0.0454],\n",
      "         [ 0.8617, -0.8219, -0.8973,  0.8343, -0.0322]],\n",
      "\n",
      "        [[-0.6758, -0.4297,  0.6768, -0.0505, -0.8640],\n",
      "         [-0.7016, -0.4094,  0.7145, -0.0500, -0.8913]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0422 cost = 0.004098\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9447, -0.5284, -0.6318,  0.9254, -0.6291],\n",
      "         [-0.3846,  0.4445, -0.0388, -0.4417,  0.5816]],\n",
      "\n",
      "        [[-0.3723, -0.4148,  0.5107, -0.0934, -0.6768],\n",
      "         [ 0.8756,  0.9094, -0.9168,  0.9001,  0.8637]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0422 cost = 0.008576\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3753,  0.5645,  0.1220, -0.4166,  0.5501],\n",
      "         [-0.4759,  0.4037,  0.0961, -0.5111,  0.5877]],\n",
      "\n",
      "        [[ 0.8992,  0.8998, -0.9320,  0.8960,  0.8504],\n",
      "         [ 0.8259,  0.8258, -0.8688,  0.8140,  0.8394]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0422 cost = 0.010111\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4759,  0.4039,  0.0961, -0.5111,  0.5880],\n",
      "         [-0.3849,  0.4451, -0.0386, -0.4417,  0.5819]],\n",
      "\n",
      "        [[ 0.8260,  0.8258, -0.8689,  0.8141,  0.8395],\n",
      "         [ 0.8757,  0.9095, -0.9168,  0.9002,  0.8638]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0423 cost = 0.008931\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3751,  0.5648,  0.1220, -0.4162,  0.5504],\n",
      "         [ 0.9448, -0.5291, -0.6323,  0.9254, -0.6294]],\n",
      "\n",
      "        [[ 0.8993,  0.8998, -0.9321,  0.8961,  0.8506],\n",
      "         [-0.3726, -0.4150,  0.5108, -0.0935, -0.6769]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0423 cost = 0.007120\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8173, -0.9327, -0.9377,  0.6817,  0.0453],\n",
      "         [ 0.8620, -0.8220, -0.8976,  0.8345, -0.0326]],\n",
      "\n",
      "        [[-0.6759, -0.4299,  0.6770, -0.0510, -0.8641],\n",
      "         [-0.7018, -0.4098,  0.7148, -0.0507, -0.8915]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0423 cost = 0.004066\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3850,  0.4463, -0.0385, -0.4416,  0.5824],\n",
      "         [-0.4758,  0.4047,  0.0961, -0.5111,  0.5888]],\n",
      "\n",
      "        [[ 0.8758,  0.9096, -0.9169,  0.9003,  0.8640],\n",
      "         [ 0.8263,  0.8261, -0.8690,  0.8144,  0.8398]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0424 cost = 0.010805\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8174, -0.9327, -0.9378,  0.6819,  0.0452],\n",
      "         [ 0.9448, -0.5295, -0.6327,  0.9255, -0.6295]],\n",
      "\n",
      "        [[-0.6759, -0.4300,  0.6770, -0.0512, -0.8641],\n",
      "         [-0.3728, -0.4153,  0.5110, -0.0936, -0.6770]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0424 cost = 0.007799\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8622, -0.8220, -0.8977,  0.8346, -0.0328],\n",
      "         [-0.3748,  0.5654,  0.1220, -0.4156,  0.5509]],\n",
      "\n",
      "        [[-0.7020, -0.4101,  0.7150, -0.0511, -0.8915],\n",
      "         [ 0.8995,  0.8999, -0.9322,  0.8963,  0.8510]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0424 cost = 0.002732\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3748,  0.5655,  0.1220, -0.4154,  0.5510],\n",
      "         [ 0.8176, -0.9328, -0.9378,  0.6820,  0.0452]],\n",
      "\n",
      "        [[ 0.8996,  0.9000, -0.9322,  0.8963,  0.8511],\n",
      "         [-0.6760, -0.4301,  0.6771, -0.0515, -0.8641]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0425 cost = 0.004115\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8623, -0.8221, -0.8978,  0.8347, -0.0330],\n",
      "         [ 0.9448, -0.5300, -0.6331,  0.9255, -0.6297]],\n",
      "\n",
      "        [[-0.7021, -0.4103,  0.7151, -0.0514, -0.8916],\n",
      "         [-0.3731, -0.4156,  0.5112, -0.0937, -0.6771]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0425 cost = 0.008305\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4759,  0.4056,  0.0960, -0.5113,  0.5900],\n",
      "         [-0.3856,  0.4482, -0.0382, -0.4416,  0.5832]],\n",
      "\n",
      "        [[ 0.8268,  0.8265, -0.8693,  0.8148,  0.8403],\n",
      "         [ 0.8761,  0.9098, -0.9171,  0.9006,  0.8644]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0425 cost = 0.008760\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4759,  0.4058,  0.0960, -0.5113,  0.5902],\n",
      "         [-0.3747,  0.5659,  0.1221, -0.4150,  0.5513]],\n",
      "\n",
      "        [[ 0.8269,  0.8266, -0.8694,  0.8149,  0.8404],\n",
      "         [ 0.8997,  0.9000, -0.9322,  0.8964,  0.8513]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0426 cost = 0.006530\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9449, -0.5304, -0.6335,  0.9255, -0.6299],\n",
      "         [ 0.8625, -0.8221, -0.8979,  0.8348, -0.0332]],\n",
      "\n",
      "        [[-0.3733, -0.4159,  0.5114, -0.0938, -0.6773],\n",
      "         [-0.7023, -0.4106,  0.7152, -0.0518, -0.8917]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0426 cost = 0.008144\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8181, -0.9328, -0.9379,  0.6826,  0.0450],\n",
      "         [-0.3854,  0.4495, -0.0381, -0.4414,  0.5837]],\n",
      "\n",
      "        [[-0.6761, -0.4304,  0.6773, -0.0520, -0.8642],\n",
      "         [ 0.8763,  0.9098, -0.9171,  0.9007,  0.8646]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0426 cost = 0.004194\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3738,  0.5665,  0.1221, -0.4142,  0.5518],\n",
      "         [ 0.8626, -0.8222, -0.8980,  0.8349, -0.0334]],\n",
      "\n",
      "        [[ 0.8998,  0.9001, -0.9323,  0.8965,  0.8516],\n",
      "         [-0.7024, -0.4108,  0.7153, -0.0521, -0.8918]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0427 cost = 0.003488\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9449, -0.5309, -0.6340,  0.9255, -0.6302],\n",
      "         [-0.4755,  0.4069,  0.0960, -0.5110,  0.5911]],\n",
      "\n",
      "        [[-0.3736, -0.4162,  0.5116, -0.0939, -0.6775],\n",
      "         [ 0.8272,  0.8269, -0.8696,  0.8153,  0.8408]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0427 cost = 0.009518\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8183, -0.9329, -0.9380,  0.6829,  0.0449],\n",
      "         [-0.3854,  0.4506, -0.0380, -0.4412,  0.5841]],\n",
      "\n",
      "        [[-0.6761, -0.4306,  0.6774, -0.0523, -0.8643],\n",
      "         [ 0.8764,  0.9099, -0.9172,  0.9009,  0.8648]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0427 cost = 0.004173\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3733,  0.5670,  0.1221, -0.4136,  0.5522],\n",
      "         [ 0.8184, -0.9329, -0.9380,  0.6830,  0.0449]],\n",
      "\n",
      "        [[ 0.8999,  0.9002, -0.9323,  0.8966,  0.8518],\n",
      "         [-0.6761, -0.4307,  0.6774, -0.0525, -0.8643]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0428 cost = 0.004050\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4753,  0.4074,  0.0960, -0.5110,  0.5916],\n",
      "         [ 0.9450, -0.5316, -0.6344,  0.9256, -0.6305]],\n",
      "\n",
      "        [[ 0.8274,  0.8271, -0.8698,  0.8155,  0.8410],\n",
      "         [-0.3738, -0.4165,  0.5118, -0.0941, -0.6776]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0428 cost = 0.008790\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8630, -0.8224, -0.8982,  0.8352, -0.0339],\n",
      "         [-0.3855,  0.4515, -0.0379, -0.4411,  0.5845]],\n",
      "\n",
      "        [[-0.7027, -0.4113,  0.7156, -0.0528, -0.8919],\n",
      "         [ 0.8765,  0.9100, -0.9172,  0.9010,  0.8649]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0428 cost = 0.004228\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3731,  0.5672,  0.1222, -0.4132,  0.5524],\n",
      "         [ 0.8630, -0.8224, -0.8983,  0.8353, -0.0340]],\n",
      "\n",
      "        [[ 0.9000,  0.9003, -0.9324,  0.8967,  0.8520],\n",
      "         [-0.7027, -0.4114,  0.7157, -0.0529, -0.8919]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0429 cost = 0.003451\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8188, -0.9329, -0.9382,  0.6835,  0.0448],\n",
      "         [-0.4755,  0.4077,  0.0961, -0.5112,  0.5920]],\n",
      "\n",
      "        [[-0.6762, -0.4309,  0.6776, -0.0529, -0.8644],\n",
      "         [ 0.8276,  0.8273, -0.8698,  0.8156,  0.8412]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0429 cost = 0.004833\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9450, -0.5324, -0.6350,  0.9257, -0.6309],\n",
      "         [-0.3858,  0.4522, -0.0376, -0.4411,  0.5848]],\n",
      "\n",
      "        [[-0.3742, -0.4169,  0.5120, -0.0944, -0.6778],\n",
      "         [ 0.8766,  0.9101, -0.9173,  0.9011,  0.8651]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0429 cost = 0.008283\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8633, -0.8226, -0.8984,  0.8355, -0.0344],\n",
      "         [-0.3859,  0.4524, -0.0375, -0.4412,  0.5848]],\n",
      "\n",
      "        [[-0.7029, -0.4116,  0.7158, -0.0533, -0.8920],\n",
      "         [ 0.8767,  0.9101, -0.9173,  0.9011,  0.8651]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0430 cost = 0.004199\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3731,  0.5674,  0.1223, -0.4129,  0.5527],\n",
      "         [ 0.8191, -0.9330, -0.9382,  0.6839,  0.0446]],\n",
      "\n",
      "        [[ 0.9001,  0.9003, -0.9324,  0.8968,  0.8522],\n",
      "         [-0.6764, -0.4310,  0.6777, -0.0532, -0.8644]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0430 cost = 0.004001\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4758,  0.4079,  0.0962, -0.5117,  0.5925],\n",
      "         [ 0.9451, -0.5331, -0.6354,  0.9257, -0.6313]],\n",
      "\n",
      "        [[ 0.8278,  0.8274, -0.8700,  0.8158,  0.8415],\n",
      "         [-0.3744, -0.4171,  0.5122, -0.0947, -0.6780]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0430 cost = 0.008685\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8634, -0.8228, -0.8985,  0.8356, -0.0348],\n",
      "         [ 0.8193, -0.9331, -0.9383,  0.6842,  0.0446]],\n",
      "\n",
      "        [[-0.7031, -0.4118,  0.7160, -0.0537, -0.8921],\n",
      "         [-0.6764, -0.4311,  0.6778, -0.0535, -0.8644]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0431 cost = 0.006929\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3864,  0.4531, -0.0371, -0.4413,  0.5851],\n",
      "         [-0.3732,  0.5674,  0.1224, -0.4127,  0.5529]],\n",
      "\n",
      "        [[ 0.8768,  0.9102, -0.9174,  0.9013,  0.8653],\n",
      "         [ 0.9002,  0.9004, -0.9325,  0.8969,  0.8523]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0431 cost = 0.004475\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4761,  0.4079,  0.0963, -0.5120,  0.5927],\n",
      "         [ 0.9451, -0.5337, -0.6359,  0.9258, -0.6316]],\n",
      "\n",
      "        [[ 0.8280,  0.8275, -0.8700,  0.8159,  0.8416],\n",
      "         [-0.3747, -0.4175,  0.5124, -0.0950, -0.6782]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0431 cost = 0.008639\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3734,  0.5674,  0.1225, -0.4126,  0.5530],\n",
      "         [ 0.9452, -0.5338, -0.6360,  0.9259, -0.6317]],\n",
      "\n",
      "        [[ 0.9002,  0.9004, -0.9325,  0.8969,  0.8524],\n",
      "         [-0.3748, -0.4177,  0.5125, -0.0952, -0.6783]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0432 cost = 0.006820\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4764,  0.4080,  0.0964, -0.5123,  0.5929],\n",
      "         [-0.3869,  0.4537, -0.0368, -0.4414,  0.5853]],\n",
      "\n",
      "        [[ 0.8280,  0.8275, -0.8700,  0.8159,  0.8416],\n",
      "         [ 0.8769,  0.9102, -0.9174,  0.9013,  0.8654]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0432 cost = 0.008436\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8199, -0.9332, -0.9385,  0.6850,  0.0443],\n",
      "         [ 0.8638, -0.8230, -0.8988,  0.8360, -0.0354]],\n",
      "\n",
      "        [[-0.6766, -0.4315,  0.6780, -0.0542, -0.8645],\n",
      "         [-0.7034, -0.4125,  0.7163, -0.0545, -0.8922]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0432 cost = 0.003889\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8200, -0.9332, -0.9385,  0.6852,  0.0443],\n",
      "         [-0.3735,  0.5676,  0.1226, -0.4123,  0.5532]],\n",
      "\n",
      "        [[-0.6767, -0.4316,  0.6781, -0.0543, -0.8646],\n",
      "         [ 0.9003,  0.9004, -0.9325,  0.8970,  0.8525]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0433 cost = 0.002568\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8640, -0.8230, -0.8988,  0.8361, -0.0356],\n",
      "         [ 0.9453, -0.5343, -0.6367,  0.9260, -0.6321]],\n",
      "\n",
      "        [[-0.7035, -0.4127,  0.7164, -0.0548, -0.8923],\n",
      "         [-0.3751, -0.4182,  0.5127, -0.0956, -0.6786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0433 cost = 0.007985\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3872,  0.4549, -0.0366, -0.4414,  0.5857],\n",
      "         [-0.4768,  0.4082,  0.0965, -0.5126,  0.5933]],\n",
      "\n",
      "        [[ 0.8770,  0.9103, -0.9174,  0.9015,  0.8655],\n",
      "         [ 0.8282,  0.8276, -0.8701,  0.8160,  0.8418]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0433 cost = 0.010376\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8203, -0.9333, -0.9386,  0.6857,  0.0441],\n",
      "         [-0.3873,  0.4552, -0.0365, -0.4415,  0.5858]],\n",
      "\n",
      "        [[-0.6768, -0.4319,  0.6782, -0.0547, -0.8646],\n",
      "         [ 0.8770,  0.9103, -0.9175,  0.9015,  0.8656]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0434 cost = 0.004043\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9453, -0.5346, -0.6371,  0.9260, -0.6324],\n",
      "         [-0.4770,  0.4084,  0.0965, -0.5129,  0.5937]],\n",
      "\n",
      "        [[-0.3754, -0.4186,  0.5129, -0.0959, -0.6788],\n",
      "         [ 0.8283,  0.8278, -0.8702,  0.8162,  0.8420]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0434 cost = 0.009211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3736,  0.5678,  0.1228, -0.4120,  0.5535],\n",
      "         [ 0.8642, -0.8232, -0.8990,  0.8363, -0.0360]],\n",
      "\n",
      "        [[ 0.9004,  0.9005, -0.9325,  0.8971,  0.8528],\n",
      "         [-0.7038, -0.4132,  0.7167, -0.0554, -0.8924]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0434 cost = 0.003350\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8206, -0.9333, -0.9387,  0.6860,  0.0440],\n",
      "         [ 0.8643, -0.8232, -0.8990,  0.8363, -0.0361]],\n",
      "\n",
      "        [[-0.6769, -0.4321,  0.6784, -0.0551, -0.8647],\n",
      "         [-0.7038, -0.4133,  0.7167, -0.0555, -0.8924]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0435 cost = 0.003842\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3880,  0.4563, -0.0361, -0.4416,  0.5861],\n",
      "         [-0.4774,  0.4086,  0.0965, -0.5133,  0.5941]],\n",
      "\n",
      "        [[ 0.8772,  0.9104, -0.9175,  0.9017,  0.8658],\n",
      "         [ 0.8285,  0.8279, -0.8702,  0.8163,  0.8422]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0435 cost = 0.010300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3738,  0.5680,  0.1229, -0.4118,  0.5538],\n",
      "         [ 0.9454, -0.5352, -0.6376,  0.9261, -0.6327]],\n",
      "\n",
      "        [[ 0.9005,  0.9005, -0.9326,  0.8972,  0.8530],\n",
      "         [-0.3758, -0.4191,  0.5132, -0.0962, -0.6791]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0435 cost = 0.006688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3739,  0.5680,  0.1229, -0.4118,  0.5538],\n",
      "         [ 0.8645, -0.8233, -0.8992,  0.8365, -0.0364]],\n",
      "\n",
      "        [[ 0.9006,  0.9006, -0.9326,  0.8972,  0.8531],\n",
      "         [-0.7040, -0.4136,  0.7169, -0.0560, -0.8925]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0436 cost = 0.003327\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9454, -0.5354, -0.6378,  0.9261, -0.6328],\n",
      "         [-0.4779,  0.4089,  0.0965, -0.5138,  0.5946]],\n",
      "\n",
      "        [[-0.3760, -0.4193,  0.5133, -0.0964, -0.6792],\n",
      "         [ 0.8287,  0.8281, -0.8704,  0.8165,  0.8424]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0436 cost = 0.009129\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3889,  0.4573, -0.0357, -0.4418,  0.5865],\n",
      "         [ 0.8209, -0.9334, -0.9388,  0.6864,  0.0439]],\n",
      "\n",
      "        [[ 0.8773,  0.9106, -0.9176,  0.9018,  0.8660],\n",
      "         [-0.6771, -0.4324,  0.6786, -0.0556, -0.8647]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0436 cost = 0.005233\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9454, -0.5358, -0.6381,  0.9261, -0.6330],\n",
      "         [ 0.8210, -0.9334, -0.9388,  0.6865,  0.0438]],\n",
      "\n",
      "        [[-0.3762, -0.4195,  0.5134, -0.0966, -0.6793],\n",
      "         [-0.6771, -0.4325,  0.6786, -0.0557, -0.8647]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0437 cost = 0.009523\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3892,  0.4577, -0.0356, -0.4420,  0.5867],\n",
      "         [-0.4783,  0.4090,  0.0966, -0.5142,  0.5950]],\n",
      "\n",
      "        [[ 0.8774,  0.9106, -0.9176,  0.9019,  0.8661],\n",
      "         [ 0.8289,  0.8282, -0.8705,  0.8166,  0.8426]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0437 cost = 0.010207\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3745,  0.5682,  0.1230, -0.4118,  0.5542],\n",
      "         [ 0.8648, -0.8236, -0.8994,  0.8367, -0.0370]],\n",
      "\n",
      "        [[ 0.9007,  0.9007, -0.9327,  0.8974,  0.8534],\n",
      "         [-0.7043, -0.4141,  0.7172, -0.0567, -0.8926]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0437 cost = 0.003297\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4787,  0.4093,  0.0966, -0.5146,  0.5954],\n",
      "         [ 0.9455, -0.5365, -0.6386,  0.9262, -0.6333]],\n",
      "\n",
      "        [[ 0.8291,  0.8284, -0.8706,  0.8168,  0.8427],\n",
      "         [-0.3764, -0.4199,  0.5136, -0.0969, -0.6795]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0438 cost = 0.008346\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8649, -0.8236, -0.8995,  0.8368, -0.0372],\n",
      "         [ 0.8214, -0.9335, -0.9389,  0.6870,  0.0437]],\n",
      "\n",
      "        [[-0.7044, -0.4143,  0.7173, -0.0570, -0.8927],\n",
      "         [-0.6772, -0.4327,  0.6788, -0.0563, -0.8648]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0438 cost = 0.006688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3749,  0.5682,  0.1231, -0.4118,  0.5544],\n",
      "         [-0.3901,  0.4587, -0.0352, -0.4423,  0.5871]],\n",
      "\n",
      "        [[ 0.9008,  0.9007, -0.9327,  0.8974,  0.8536],\n",
      "         [ 0.8776,  0.9108, -0.9177,  0.9021,  0.8664]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0438 cost = 0.005200\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8216, -0.9336, -0.9390,  0.6874,  0.0436],\n",
      "         [ 0.9455, -0.5371, -0.6391,  0.9263, -0.6336]],\n",
      "\n",
      "        [[-0.6773, -0.4329,  0.6789, -0.0566, -0.8648],\n",
      "         [-0.3767, -0.4203,  0.5138, -0.0972, -0.6796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0439 cost = 0.007267\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3903,  0.4592, -0.0351, -0.4424,  0.5873],\n",
      "         [-0.3752,  0.5683,  0.1232, -0.4117,  0.5545]],\n",
      "\n",
      "        [[ 0.8777,  0.9108, -0.9178,  0.9021,  0.8665],\n",
      "         [ 0.9009,  0.9007, -0.9327,  0.8975,  0.8537]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0439 cost = 0.004271\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8652, -0.8238, -0.8997,  0.8371, -0.0378],\n",
      "         [-0.4795,  0.4096,  0.0966, -0.5154,  0.5961]],\n",
      "\n",
      "        [[-0.7047, -0.4149,  0.7176, -0.0576, -0.8928],\n",
      "         [ 0.8293,  0.8286, -0.8707,  0.8170,  0.8431]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0439 cost = 0.004561\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3754,  0.5684,  0.1232, -0.4117,  0.5547],\n",
      "         [-0.4796,  0.4097,  0.0966, -0.5155,  0.5962]],\n",
      "\n",
      "        [[ 0.9010,  0.9008, -0.9327,  0.8975,  0.8539],\n",
      "         [ 0.8294,  0.8286, -0.8707,  0.8170,  0.8431]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0440 cost = 0.009404\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8220, -0.9337, -0.9391,  0.6879,  0.0434],\n",
      "         [ 0.9456, -0.5377, -0.6396,  0.9263, -0.6339]],\n",
      "\n",
      "        [[-0.6774, -0.4333,  0.6791, -0.0571, -0.8649],\n",
      "         [-0.3771, -0.4208,  0.5141, -0.0975, -0.6799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0440 cost = 0.007220\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8654, -0.8239, -0.8998,  0.8372, -0.0380],\n",
      "         [-0.3910,  0.4603, -0.0347, -0.4425,  0.5878]],\n",
      "\n",
      "        [[-0.7049, -0.4152,  0.7177, -0.0580, -0.8929],\n",
      "         [ 0.8778,  0.9109, -0.9178,  0.9023,  0.8667]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0440 cost = 0.003967\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8222, -0.9337, -0.9392,  0.6882,  0.0433],\n",
      "         [ 0.8654, -0.8240, -0.8998,  0.8372, -0.0382]],\n",
      "\n",
      "        [[-0.6775, -0.4334,  0.6792, -0.0574, -0.8650],\n",
      "         [-0.7049, -0.4153,  0.7178, -0.0582, -0.8929]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0441 cost = 0.003727\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9457, -0.5381, -0.6400,  0.9264, -0.6342],\n",
      "         [-0.4799,  0.4101,  0.0966, -0.5158,  0.5971]],\n",
      "\n",
      "        [[-0.3773, -0.4212,  0.5143, -0.0977, -0.6801],\n",
      "         [ 0.8297,  0.8289, -0.8709,  0.8173,  0.8435]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0441 cost = 0.008916\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3913,  0.4610, -0.0345, -0.4426,  0.5881],\n",
      "         [-0.3757,  0.5687,  0.1233, -0.4114,  0.5552]],\n",
      "\n",
      "        [[ 0.8780,  0.9110, -0.9179,  0.9024,  0.8669],\n",
      "         [ 0.9012,  0.9009, -0.9328,  0.8977,  0.8542]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0441 cost = 0.004211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9457, -0.5385, -0.6403,  0.9264, -0.6344],\n",
      "         [-0.4800,  0.4103,  0.0966, -0.5160,  0.5975]],\n",
      "\n",
      "        [[-0.3775, -0.4214,  0.5144, -0.0978, -0.6802],\n",
      "         [ 0.8298,  0.8290, -0.8710,  0.8174,  0.8436]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0442 cost = 0.008888\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8657, -0.8242, -0.9000,  0.8374, -0.0386],\n",
      "         [ 0.8225, -0.9338, -0.9393,  0.6886,  0.0431]],\n",
      "\n",
      "        [[-0.7052, -0.4157,  0.7180, -0.0588, -0.8930],\n",
      "         [-0.6776, -0.4337,  0.6793, -0.0579, -0.8650]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0442 cost = 0.006553\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3758,  0.5689,  0.1233, -0.4113,  0.5555],\n",
      "         [-0.3918,  0.4616, -0.0342, -0.4428,  0.5884]],\n",
      "\n",
      "        [[ 0.9012,  0.9009, -0.9329,  0.8978,  0.8544],\n",
      "         [ 0.8781,  0.9111, -0.9179,  0.9025,  0.8670]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0442 cost = 0.005090\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9457, -0.5393, -0.6407,  0.9265, -0.6347],\n",
      "         [-0.3919,  0.4618, -0.0341, -0.4428,  0.5885]],\n",
      "\n",
      "        [[-0.3778, -0.4218,  0.5146, -0.0981, -0.6804],\n",
      "         [ 0.8781,  0.9111, -0.9179,  0.9026,  0.8671]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0443 cost = 0.007753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8659, -0.8245, -0.9002,  0.8376, -0.0391],\n",
      "         [-0.4803,  0.4107,  0.0967, -0.5164,  0.5981]],\n",
      "\n",
      "        [[-0.7054, -0.4161,  0.7182, -0.0592, -0.8931],\n",
      "         [ 0.8301,  0.8291, -0.8711,  0.8176,  0.8438]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0443 cost = 0.004476\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8229, -0.9339, -0.9394,  0.6891,  0.0429],\n",
      "         [-0.3760,  0.5691,  0.1234, -0.4112,  0.5559]],\n",
      "\n",
      "        [[-0.6778, -0.4340,  0.6795, -0.0585, -0.8651],\n",
      "         [ 0.9013,  0.9010, -0.9329,  0.8979,  0.8546]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0443 cost = 0.002430\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.5691,  0.1234, -0.4112,  0.5560],\n",
      "         [-0.4804,  0.4108,  0.0967, -0.5166,  0.5985]],\n",
      "\n",
      "        [[ 0.9014,  0.9010, -0.9329,  0.8979,  0.8547],\n",
      "         [ 0.8302,  0.8292, -0.8712,  0.8177,  0.8440]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0444 cost = 0.009249\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8230, -0.9340, -0.9394,  0.6893,  0.0428],\n",
      "         [-0.3923,  0.4627, -0.0339, -0.4429,  0.5890]],\n",
      "\n",
      "        [[-0.6778, -0.4341,  0.6796, -0.0587, -0.8652],\n",
      "         [ 0.8783,  0.9112, -0.9180,  0.9027,  0.8673]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0444 cost = 0.003828\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9458, -0.5405, -0.6414,  0.9266, -0.6352],\n",
      "         [ 0.8661, -0.8248, -0.9004,  0.8378, -0.0396]],\n",
      "\n",
      "        [[-0.3782, -0.4222,  0.5149, -0.0984, -0.6806],\n",
      "         [-0.7056, -0.4164,  0.7185, -0.0597, -0.8932]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0444 cost = 0.007452\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8232, -0.9340, -0.9395,  0.6895,  0.0427],\n",
      "         [ 0.9458, -0.5408, -0.6416,  0.9266, -0.6353]],\n",
      "\n",
      "        [[-0.6779, -0.4341,  0.6797, -0.0589, -0.8652],\n",
      "         [-0.3783, -0.4223,  0.5150, -0.0985, -0.6806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0445 cost = 0.007067\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.5694,  0.1235, -0.4110,  0.5565],\n",
      "         [ 0.8662, -0.8249, -0.9005,  0.8380, -0.0400]],\n",
      "\n",
      "        [[ 0.9015,  0.9011, -0.9330,  0.8980,  0.8550],\n",
      "         [-0.7057, -0.4165,  0.7186, -0.0600, -0.8932]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0445 cost = 0.003165\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4802,  0.4114,  0.0967, -0.5167,  0.5994],\n",
      "         [-0.3925,  0.4634, -0.0337, -0.4430,  0.5894]],\n",
      "\n",
      "        [[ 0.8305,  0.8295, -0.8713,  0.8179,  0.8443],\n",
      "         [ 0.8785,  0.9113, -0.9181,  0.9029,  0.8675]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0445 cost = 0.007865\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.5695,  0.1235, -0.4108,  0.5567],\n",
      "         [ 0.8664, -0.8251, -0.9006,  0.8381, -0.0403]],\n",
      "\n",
      "        [[ 0.9016,  0.9012, -0.9330,  0.8981,  0.8551],\n",
      "         [-0.7058, -0.4167,  0.7187, -0.0603, -0.8933]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0446 cost = 0.003153\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4802,  0.4117,  0.0967, -0.5167,  0.5997],\n",
      "         [-0.3924,  0.4640, -0.0336, -0.4430,  0.5897]],\n",
      "\n",
      "        [[ 0.8306,  0.8296, -0.8714,  0.8180,  0.8445],\n",
      "         [ 0.8785,  0.9114, -0.9181,  0.9029,  0.8677]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0446 cost = 0.007834\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9460, -0.5420, -0.6424,  0.9267, -0.6359],\n",
      "         [ 0.8236, -0.9341, -0.9396,  0.6901,  0.0425]],\n",
      "\n",
      "        [[-0.3787, -0.4228,  0.5153, -0.0988, -0.6809],\n",
      "         [-0.6780, -0.4345,  0.6799, -0.0595, -0.8653]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0446 cost = 0.009091\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3758,  0.5698,  0.1235, -0.4103,  0.5570],\n",
      "         [ 0.8237, -0.9341, -0.9396,  0.6903,  0.0424]],\n",
      "\n",
      "        [[ 0.9017,  0.9012, -0.9330,  0.8982,  0.8554],\n",
      "         [-0.6780, -0.4345,  0.6799, -0.0596, -0.8653]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0447 cost = 0.003655\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3921,  0.4651, -0.0336, -0.4428,  0.5901],\n",
      "         [ 0.9460, -0.5424, -0.6428,  0.9267, -0.6361]],\n",
      "\n",
      "        [[ 0.8787,  0.9114, -0.9182,  0.9031,  0.8678],\n",
      "         [-0.3788, -0.4230,  0.5154, -0.0989, -0.6810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0447 cost = 0.007671\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8668, -0.8252, -0.9009,  0.8384, -0.0409],\n",
      "         [-0.4799,  0.4123,  0.0967, -0.5166,  0.6004]],\n",
      "\n",
      "        [[-0.7061, -0.4172,  0.7189, -0.0609, -0.8934],\n",
      "         [ 0.8309,  0.8298, -0.8716,  0.8183,  0.8448]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0447 cost = 0.004380\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8669, -0.8253, -0.9009,  0.8385, -0.0410],\n",
      "         [ 0.9461, -0.5429, -0.6431,  0.9268, -0.6363]],\n",
      "\n",
      "        [[-0.7062, -0.4173,  0.7190, -0.0611, -0.8935],\n",
      "         [-0.3790, -0.4232,  0.5155, -0.0991, -0.6811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0448 cost = 0.007447\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8242, -0.9342, -0.9398,  0.6910,  0.0422],\n",
      "         [-0.4798,  0.4125,  0.0967, -0.5166,  0.6006]],\n",
      "\n",
      "        [[-0.6781, -0.4348,  0.6801, -0.0602, -0.8654],\n",
      "         [ 0.8310,  0.8299, -0.8716,  0.8184,  0.8449]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0448 cost = 0.004407\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3756,  0.5701,  0.1235, -0.4097,  0.5574],\n",
      "         [-0.3920,  0.4661, -0.0335, -0.4427,  0.5905]],\n",
      "\n",
      "        [[ 0.9018,  0.9013, -0.9331,  0.8983,  0.8556],\n",
      "         [ 0.8788,  0.9115, -0.9182,  0.9032,  0.8680]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0448 cost = 0.004932\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8244, -0.9343, -0.9398,  0.6913,  0.0421],\n",
      "         [ 0.8671, -0.8255, -0.9011,  0.8387, -0.0415]],\n",
      "\n",
      "        [[-0.6782, -0.4350,  0.6802, -0.0604, -0.8654],\n",
      "         [-0.7064, -0.4176,  0.7192, -0.0616, -0.8936]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0449 cost = 0.003582\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3755,  0.5702,  0.1236, -0.4095,  0.5576],\n",
      "         [ 0.9462, -0.5438, -0.6438,  0.9269, -0.6367]],\n",
      "\n",
      "        [[ 0.9019,  0.9013, -0.9331,  0.8984,  0.8558],\n",
      "         [-0.3794, -0.4237,  0.5158, -0.0994, -0.6813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0449 cost = 0.006237\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4797,  0.4128,  0.0968, -0.5168,  0.6011],\n",
      "         [-0.3918,  0.4668, -0.0334, -0.4427,  0.5908]],\n",
      "\n",
      "        [[ 0.8312,  0.8301, -0.8717,  0.8185,  0.8451],\n",
      "         [ 0.8789,  0.9116, -0.9183,  0.9033,  0.8681]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0449 cost = 0.007691\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8674, -0.8256, -0.9012,  0.8389, -0.0419],\n",
      "         [-0.3755,  0.5703,  0.1236, -0.4093,  0.5578]],\n",
      "\n",
      "        [[-0.7066, -0.4179,  0.7194, -0.0621, -0.8937],\n",
      "         [ 0.9019,  0.9014, -0.9331,  0.8984,  0.8559]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0450 cost = 0.002389\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8249, -0.9343, -0.9400,  0.6919,  0.0419],\n",
      "         [-0.3917,  0.4674, -0.0334, -0.4426,  0.5910]],\n",
      "\n",
      "        [[-0.6784, -0.4353,  0.6803, -0.0610, -0.8655],\n",
      "         [ 0.8789,  0.9116, -0.9183,  0.9033,  0.8682]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0450 cost = 0.003720\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4796,  0.4131,  0.0968, -0.5168,  0.6016],\n",
      "         [ 0.9463, -0.5444, -0.6444,  0.9270, -0.6371]],\n",
      "\n",
      "        [[ 0.8314,  0.8302, -0.8718,  0.8187,  0.8453],\n",
      "         [-0.3797, -0.4241,  0.5160, -0.0997, -0.6816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0450 cost = 0.007834\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8676, -0.8256, -0.9014,  0.8391, -0.0422],\n",
      "         [ 0.8251, -0.9344, -0.9400,  0.6921,  0.0419]],\n",
      "\n",
      "        [[-0.7067, -0.4182,  0.7195, -0.0625, -0.8937],\n",
      "         [-0.6784, -0.4354,  0.6804, -0.0612, -0.8656]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0451 cost = 0.006277\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4796,  0.4132,  0.0969, -0.5169,  0.6018],\n",
      "         [-0.3753,  0.5705,  0.1236, -0.4088,  0.5581]],\n",
      "\n",
      "        [[ 0.8315,  0.8303, -0.8719,  0.8188,  0.8454],\n",
      "         [ 0.9020,  0.9014, -0.9332,  0.8985,  0.8561]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0451 cost = 0.005649\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3915,  0.4686, -0.0332, -0.4424,  0.5914],\n",
      "         [ 0.9463, -0.5447, -0.6448,  0.9271, -0.6373]],\n",
      "\n",
      "        [[ 0.8791,  0.9117, -0.9183,  0.9035,  0.8683],\n",
      "         [-0.3799, -0.4245,  0.5162, -0.0999, -0.6817]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0451 cost = 0.007505\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8254, -0.9344, -0.9401,  0.6926,  0.0417],\n",
      "         [ 0.8678, -0.8257, -0.9015,  0.8392, -0.0425]],\n",
      "\n",
      "        [[-0.6786, -0.4357,  0.6806, -0.0616, -0.8656],\n",
      "         [-0.7069, -0.4186,  0.7197, -0.0630, -0.8938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0452 cost = 0.003527\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9464, -0.5449, -0.6450,  0.9271, -0.6374],\n",
      "         [-0.3914,  0.4692, -0.0331, -0.4423,  0.5916]],\n",
      "\n",
      "        [[-0.3801, -0.4248,  0.5164, -0.1001, -0.6819],\n",
      "         [ 0.8791,  0.9117, -0.9184,  0.9035,  0.8684]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0452 cost = 0.007422\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4794,  0.4138,  0.0969, -0.5168,  0.6024],\n",
      "         [-0.3746,  0.5709,  0.1237, -0.4080,  0.5585]],\n",
      "\n",
      "        [[ 0.8316,  0.8305, -0.8719,  0.8190,  0.8455],\n",
      "         [ 0.9021,  0.9015, -0.9332,  0.8986,  0.8563]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0452 cost = 0.005600\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8680, -0.8258, -0.9016,  0.8394, -0.0428],\n",
      "         [ 0.9464, -0.5452, -0.6453,  0.9271, -0.6376]],\n",
      "\n",
      "        [[-0.7071, -0.4190,  0.7199, -0.0634, -0.8939],\n",
      "         [-0.3803, -0.4250,  0.5165, -0.1003, -0.6820]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0453 cost = 0.007265\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4791,  0.4142,  0.0969, -0.5166,  0.6027],\n",
      "         [ 0.8259, -0.9345, -0.9402,  0.6932,  0.0415]],\n",
      "\n",
      "        [[ 0.8317,  0.8305, -0.8720,  0.8191,  0.8456],\n",
      "         [-0.6787, -0.4361,  0.6808, -0.0622, -0.8658]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0453 cost = 0.005359\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3738,  0.5714,  0.1237, -0.4072,  0.5589],\n",
      "         [-0.3913,  0.4704, -0.0329, -0.4419,  0.5921]],\n",
      "\n",
      "        [[ 0.9022,  0.9015, -0.9332,  0.8987,  0.8564],\n",
      "         [ 0.8792,  0.9118, -0.9184,  0.9037,  0.8685]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0453 cost = 0.004812\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3736,  0.5716,  0.1238, -0.4069,  0.5590],\n",
      "         [-0.4790,  0.4145,  0.0969, -0.5165,  0.6030]],\n",
      "\n",
      "        [[ 0.9022,  0.9015, -0.9332,  0.8987,  0.8564],\n",
      "         [ 0.8318,  0.8306, -0.8720,  0.8192,  0.8457]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0454 cost = 0.008896\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9465, -0.5457, -0.6458,  0.9272, -0.6380],\n",
      "         [-0.3912,  0.4712, -0.0328, -0.4417,  0.5924]],\n",
      "\n",
      "        [[-0.3807, -0.4255,  0.5167, -0.1005, -0.6823],\n",
      "         [ 0.8793,  0.9118, -0.9184,  0.9037,  0.8686]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0454 cost = 0.007351\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8683, -0.8259, -0.9018,  0.8396, -0.0432],\n",
      "         [ 0.8263, -0.9346, -0.9403,  0.6937,  0.0414]],\n",
      "\n",
      "        [[-0.7074, -0.4196,  0.7202, -0.0641, -0.8941],\n",
      "         [-0.6788, -0.4364,  0.6809, -0.0626, -0.8659]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0454 cost = 0.006164\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8264, -0.9346, -0.9404,  0.6939,  0.0413],\n",
      "         [ 0.8684, -0.8259, -0.9018,  0.8397, -0.0433]],\n",
      "\n",
      "        [[-0.6789, -0.4364,  0.6810, -0.0628, -0.8659],\n",
      "         [-0.7074, -0.4197,  0.7202, -0.0643, -0.8941]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0455 cost = 0.003474\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9465, -0.5463, -0.6463,  0.9273, -0.6382],\n",
      "         [-0.3729,  0.5721,  0.1238, -0.4060,  0.5594]],\n",
      "\n",
      "        [[-0.3809, -0.4259,  0.5169, -0.1007, -0.6825],\n",
      "         [ 0.9023,  0.9016, -0.9333,  0.8989,  0.8567]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0455 cost = 0.005862\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3909,  0.4723, -0.0327, -0.4415,  0.5929],\n",
      "         [-0.4784,  0.4155,  0.0969, -0.5161,  0.6039]],\n",
      "\n",
      "        [[ 0.8794,  0.9119, -0.9185,  0.9039,  0.8688],\n",
      "         [ 0.8321,  0.8309, -0.8722,  0.8195,  0.8461]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0455 cost = 0.009476\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3727,  0.5724,  0.1237, -0.4057,  0.5596],\n",
      "         [ 0.8267, -0.9347, -0.9405,  0.6944,  0.0411]],\n",
      "\n",
      "        [[ 0.9024,  0.9016, -0.9333,  0.8989,  0.8568],\n",
      "         [-0.6789, -0.4367,  0.6811, -0.0632, -0.8660]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0456 cost = 0.003477\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3910,  0.4729, -0.0326, -0.4414,  0.5932],\n",
      "         [-0.4784,  0.4158,  0.0969, -0.5161,  0.6042]],\n",
      "\n",
      "        [[ 0.8795,  0.9120, -0.9185,  0.9040,  0.8689],\n",
      "         [ 0.8323,  0.8310, -0.8723,  0.8196,  0.8463]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0456 cost = 0.009445\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9466, -0.5469, -0.6468,  0.9273, -0.6385],\n",
      "         [ 0.8688, -0.8261, -0.9021,  0.8400, -0.0439]],\n",
      "\n",
      "        [[-0.3812, -0.4263,  0.5172, -0.1009, -0.6826],\n",
      "         [-0.7077, -0.4203,  0.7205, -0.0649, -0.8942]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0456 cost = 0.007039\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3912,  0.4736, -0.0325, -0.4414,  0.5935],\n",
      "         [ 0.8270, -0.9348, -0.9405,  0.6948,  0.0410]],\n",
      "\n",
      "        [[ 0.8796,  0.9120, -0.9186,  0.9041,  0.8691],\n",
      "         [-0.6790, -0.4369,  0.6812, -0.0636, -0.8660]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0457 cost = 0.004675\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9466, -0.5473, -0.6471,  0.9273, -0.6387],\n",
      "         [ 0.8689, -0.8262, -0.9022,  0.8400, -0.0441]],\n",
      "\n",
      "        [[-0.3814, -0.4265,  0.5173, -0.1010, -0.6827],\n",
      "         [-0.7078, -0.4205,  0.7206, -0.0652, -0.8943]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0457 cost = 0.007019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4787,  0.4165,  0.0968, -0.5164,  0.6051],\n",
      "         [-0.3727,  0.5729,  0.1237, -0.4052,  0.5601]],\n",
      "\n",
      "        [[ 0.8326,  0.8313, -0.8725,  0.8199,  0.8466],\n",
      "         [ 0.9026,  0.9018, -0.9334,  0.8991,  0.8572]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0457 cost = 0.005409\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3916,  0.4746, -0.0323, -0.4413,  0.5939],\n",
      "         [ 0.9466, -0.5478, -0.6474,  0.9273, -0.6389]],\n",
      "\n",
      "        [[ 0.8798,  0.9121, -0.9187,  0.9042,  0.8693],\n",
      "         [-0.3815, -0.4267,  0.5174, -0.1011, -0.6828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0458 cost = 0.007263\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8273, -0.9348, -0.9407,  0.6952,  0.0409],\n",
      "         [ 0.8690, -0.8263, -0.9023,  0.8402, -0.0444]],\n",
      "\n",
      "        [[-0.6791, -0.4372,  0.6813, -0.0641, -0.8661],\n",
      "         [-0.7080, -0.4208,  0.7207, -0.0656, -0.8944]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0458 cost = 0.003417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4787,  0.4172,  0.0968, -0.5164,  0.6057],\n",
      "         [-0.3723,  0.5734,  0.1238, -0.4047,  0.5605]],\n",
      "\n",
      "        [[ 0.8328,  0.8314, -0.8726,  0.8201,  0.8468],\n",
      "         [ 0.9027,  0.9018, -0.9334,  0.8992,  0.8574]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0458 cost = 0.005371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4786,  0.4175,  0.0968, -0.5163,  0.6060],\n",
      "         [ 0.8275, -0.9349, -0.9407,  0.6955,  0.0408]],\n",
      "\n",
      "        [[ 0.8329,  0.8315, -0.8727,  0.8201,  0.8469],\n",
      "         [-0.6791, -0.4374,  0.6814, -0.0643, -0.8661]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0459 cost = 0.005181\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3717,  0.5739,  0.1238, -0.4041,  0.5608],\n",
      "         [ 0.9466, -0.5484, -0.6479,  0.9273, -0.6392]],\n",
      "\n",
      "        [[ 0.9028,  0.9019, -0.9335,  0.8993,  0.8575],\n",
      "         [-0.3818, -0.4272,  0.5176, -0.1013, -0.6830]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0459 cost = 0.005940\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8692, -0.8264, -0.9024,  0.8403, -0.0446],\n",
      "         [-0.3921,  0.4763, -0.0320, -0.4410,  0.5946]],\n",
      "\n",
      "        [[-0.7082, -0.4213,  0.7209, -0.0662, -0.8945],\n",
      "         [ 0.8800,  0.9122, -0.9187,  0.9044,  0.8695]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0459 cost = 0.003600\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4786,  0.4181,  0.0968, -0.5162,  0.6065],\n",
      "         [ 0.8278, -0.9349, -0.9408,  0.6958,  0.0407]],\n",
      "\n",
      "        [[ 0.8331,  0.8317, -0.8728,  0.8203,  0.8471],\n",
      "         [-0.6792, -0.4376,  0.6815, -0.0646, -0.8662]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0460 cost = 0.005149\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8693, -0.8264, -0.9025,  0.8404, -0.0448],\n",
      "         [-0.3924,  0.4769, -0.0318, -0.4409,  0.5949]],\n",
      "\n",
      "        [[-0.7083, -0.4215,  0.7210, -0.0664, -0.8946],\n",
      "         [ 0.8800,  0.9123, -0.9188,  0.9045,  0.8696]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0460 cost = 0.003588\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9467, -0.5489, -0.6483,  0.9274, -0.6394],\n",
      "         [-0.3712,  0.5744,  0.1238, -0.4033,  0.5611]],\n",
      "\n",
      "        [[-0.3821, -0.4276,  0.5178, -0.1015, -0.6832],\n",
      "         [ 0.9029,  0.9019, -0.9335,  0.8994,  0.8577]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0460 cost = 0.005724\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8694, -0.8265, -0.9025,  0.8404, -0.0449],\n",
      "         [-0.3711,  0.5746,  0.1239, -0.4032,  0.5612]],\n",
      "\n",
      "        [[-0.7084, -0.4217,  0.7211, -0.0667, -0.8946],\n",
      "         [ 0.9029,  0.9019, -0.9335,  0.8994,  0.8578]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0461 cost = 0.002259\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3928,  0.4777, -0.0315, -0.4407,  0.5951],\n",
      "         [-0.4789,  0.4187,  0.0968, -0.5164,  0.6071]],\n",
      "\n",
      "        [[ 0.8801,  0.9124, -0.9188,  0.9046,  0.8698],\n",
      "         [ 0.8334,  0.8319, -0.8730,  0.8206,  0.8474]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0461 cost = 0.009222\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8282, -0.9350, -0.9409,  0.6965,  0.0406],\n",
      "         [ 0.9467, -0.5494, -0.6486,  0.9274, -0.6396]],\n",
      "\n",
      "        [[-0.6793, -0.4379,  0.6816, -0.0652, -0.8663],\n",
      "         [-0.3823, -0.4278,  0.5180, -0.1017, -0.6833]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0461 cost = 0.006540\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8695, -0.8266, -0.9026,  0.8405, -0.0452],\n",
      "         [-0.3710,  0.5749,  0.1239, -0.4028,  0.5614]],\n",
      "\n",
      "        [[-0.7086, -0.4220,  0.7213, -0.0670, -0.8947],\n",
      "         [ 0.9030,  0.9020, -0.9335,  0.8995,  0.8580]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0462 cost = 0.002248\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8284, -0.9351, -0.9410,  0.6967,  0.0405],\n",
      "         [-0.3934,  0.4785, -0.0312, -0.4407,  0.5954]],\n",
      "\n",
      "        [[-0.6793, -0.4380,  0.6817, -0.0654, -0.8663],\n",
      "         [ 0.8802,  0.9124, -0.9189,  0.9047,  0.8699]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0462 cost = 0.003497\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4793,  0.4192,  0.0968, -0.5166,  0.6077],\n",
      "         [ 0.9467, -0.5499, -0.6489,  0.9274, -0.6399]],\n",
      "\n",
      "        [[ 0.8336,  0.8321, -0.8731,  0.8208,  0.8476],\n",
      "         [-0.3825, -0.4281,  0.5181, -0.1019, -0.6835]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0462 cost = 0.007368\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9467, -0.5500, -0.6490,  0.9274, -0.6399],\n",
      "         [ 0.8285, -0.9351, -0.9410,  0.6969,  0.0405]],\n",
      "\n",
      "        [[-0.3826, -0.4283,  0.5182, -0.1020, -0.6835],\n",
      "         [-0.6794, -0.4381,  0.6818, -0.0656, -0.8663]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0463 cost = 0.008434\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3710,  0.5752,  0.1240, -0.4025,  0.5616],\n",
      "         [-0.3938,  0.4791, -0.0309, -0.4407,  0.5957]],\n",
      "\n",
      "        [[ 0.9031,  0.9020, -0.9336,  0.8996,  0.8581],\n",
      "         [ 0.8803,  0.9125, -0.9189,  0.9048,  0.8700]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0463 cost = 0.004571\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4796,  0.4195,  0.0968, -0.5168,  0.6080],\n",
      "         [ 0.8698, -0.8268, -0.9028,  0.8408, -0.0457]],\n",
      "\n",
      "        [[ 0.8337,  0.8321, -0.8732,  0.8209,  0.8478],\n",
      "         [-0.7088, -0.4225,  0.7215, -0.0676, -0.8948]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0463 cost = 0.004311\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8699, -0.8268, -0.9029,  0.8408, -0.0458],\n",
      "         [-0.4797,  0.4196,  0.0969, -0.5169,  0.6081]],\n",
      "\n",
      "        [[-0.7089, -0.4226,  0.7216, -0.0678, -0.8948],\n",
      "         [ 0.8338,  0.8322, -0.8732,  0.8209,  0.8478]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0464 cost = 0.004040\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8289, -0.9352, -0.9412,  0.6975,  0.0403],\n",
      "         [-0.3942,  0.4797, -0.0307, -0.4407,  0.5959]],\n",
      "\n",
      "        [[-0.6795, -0.4384,  0.6819, -0.0661, -0.8664],\n",
      "         [ 0.8804,  0.9126, -0.9190,  0.9049,  0.8702]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0464 cost = 0.003464\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3711,  0.5754,  0.1241, -0.4022,  0.5618],\n",
      "         [ 0.9468, -0.5509, -0.6497,  0.9276, -0.6404]],\n",
      "\n",
      "        [[ 0.9032,  0.9021, -0.9336,  0.8997,  0.8583],\n",
      "         [-0.3830, -0.4289,  0.5185, -0.1024, -0.6838]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0464 cost = 0.005794\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8700, -0.8270, -0.9030,  0.8410, -0.0461],\n",
      "         [-0.3712,  0.5754,  0.1242, -0.4022,  0.5619]],\n",
      "\n",
      "        [[-0.7090, -0.4229,  0.7218, -0.0682, -0.8949],\n",
      "         [ 0.9032,  0.9021, -0.9336,  0.8997,  0.8584]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0465 cost = 0.002216\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9468, -0.5512, -0.6500,  0.9276, -0.6406],\n",
      "         [-0.3946,  0.4803, -0.0305, -0.4407,  0.5961]],\n",
      "\n",
      "        [[-0.3831, -0.4291,  0.5186, -0.1026, -0.6840],\n",
      "         [ 0.8805,  0.9126, -0.9190,  0.9050,  0.8703]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0465 cost = 0.006987\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4803,  0.4198,  0.0970, -0.5173,  0.6086],\n",
      "         [ 0.8293, -0.9353, -0.9413,  0.6980,  0.0401]],\n",
      "\n",
      "        [[ 0.8340,  0.8323, -0.8733,  0.8211,  0.8480],\n",
      "         [-0.6796, -0.4386,  0.6820, -0.0665, -0.8665]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0465 cost = 0.004995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3949,  0.4806, -0.0303, -0.4407,  0.5962],\n",
      "         [ 0.8294, -0.9353, -0.9413,  0.6981,  0.0401]],\n",
      "\n",
      "        [[ 0.8806,  0.9127, -0.9190,  0.9050,  0.8703],\n",
      "         [-0.6796, -0.4387,  0.6821, -0.0666, -0.8665]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0466 cost = 0.004444\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9469, -0.5519, -0.6504,  0.9277, -0.6409],\n",
      "         [-0.3716,  0.5755,  0.1243, -0.4020,  0.5620]],\n",
      "\n",
      "        [[-0.3833, -0.4294,  0.5187, -0.1030, -0.6841],\n",
      "         [ 0.9033,  0.9021, -0.9337,  0.8997,  0.8585]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0466 cost = 0.005574\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4808,  0.4199,  0.0970, -0.5176,  0.6087],\n",
      "         [ 0.8703, -0.8272, -0.9032,  0.8413, -0.0467]],\n",
      "\n",
      "        [[ 0.8341,  0.8324, -0.8733,  0.8211,  0.8481],\n",
      "         [-0.7093, -0.4233,  0.7220, -0.0688, -0.8950]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0466 cost = 0.004248\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8704, -0.8273, -0.9032,  0.8413, -0.0468],\n",
      "         [-0.3718,  0.5755,  0.1244, -0.4020,  0.5620]],\n",
      "\n",
      "        [[-0.7093, -0.4234,  0.7220, -0.0689, -0.8950],\n",
      "         [ 0.9033,  0.9022, -0.9337,  0.8998,  0.8586]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0467 cost = 0.002195\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8298, -0.9354, -0.9414,  0.6988,  0.0399],\n",
      "         [-0.3956,  0.4812, -0.0300, -0.4409,  0.5964]],\n",
      "\n",
      "        [[-0.6797, -0.4389,  0.6822, -0.0670, -0.8666],\n",
      "         [ 0.8807,  0.9127, -0.9191,  0.9051,  0.8705]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0467 cost = 0.003417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9470, -0.5528, -0.6509,  0.9278, -0.6413],\n",
      "         [-0.4813,  0.4199,  0.0971, -0.5180,  0.6089]],\n",
      "\n",
      "        [[-0.3836, -0.4298,  0.5189, -0.1034, -0.6844],\n",
      "         [ 0.8342,  0.8324, -0.8733,  0.8212,  0.8482]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0467 cost = 0.007922\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8706, -0.8275, -0.9033,  0.8415, -0.0472],\n",
      "         [ 0.8300, -0.9355, -0.9415,  0.6990,  0.0398]],\n",
      "\n",
      "        [[-0.7095, -0.4236,  0.7222, -0.0693, -0.8951],\n",
      "         [-0.6798, -0.4390,  0.6823, -0.0672, -0.8666]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0468 cost = 0.005800\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9470, -0.5533, -0.6512,  0.9279, -0.6415],\n",
      "         [-0.3961,  0.4815, -0.0297, -0.4410,  0.5965]],\n",
      "\n",
      "        [[-0.3837, -0.4300,  0.5190, -0.1036, -0.6845],\n",
      "         [ 0.8808,  0.9128, -0.9191,  0.9052,  0.8705]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0468 cost = 0.006885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3724,  0.5754,  0.1246, -0.4019,  0.5621],\n",
      "         [-0.4817,  0.4199,  0.0972, -0.5183,  0.6089]],\n",
      "\n",
      "        [[ 0.9034,  0.9022, -0.9337,  0.8998,  0.8587],\n",
      "         [ 0.8343,  0.8324, -0.8734,  0.8212,  0.8483]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0468 cost = 0.008436\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8303, -0.9356, -0.9416,  0.6996,  0.0396],\n",
      "         [ 0.8708, -0.8277, -0.9035,  0.8417, -0.0478]],\n",
      "\n",
      "        [[-0.6799, -0.4392,  0.6824, -0.0676, -0.8666],\n",
      "         [-0.7096, -0.4239,  0.7224, -0.0697, -0.8951]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0469 cost = 0.003250\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9471, -0.5542, -0.6517,  0.9279, -0.6419],\n",
      "         [-0.4818,  0.4200,  0.0972, -0.5184,  0.6091]],\n",
      "\n",
      "        [[-0.3840, -0.4303,  0.5191, -0.1039, -0.6846],\n",
      "         [ 0.8344,  0.8325, -0.8734,  0.8213,  0.8484]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0469 cost = 0.007861\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3725,  0.5755,  0.1246, -0.4017,  0.5622],\n",
      "         [-0.3963,  0.4820, -0.0295, -0.4410,  0.5967]],\n",
      "\n",
      "        [[ 0.9035,  0.9022, -0.9337,  0.8999,  0.8589],\n",
      "         [ 0.8809,  0.9128, -0.9192,  0.9053,  0.8707]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0469 cost = 0.004458\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3964,  0.4822, -0.0294, -0.4411,  0.5968],\n",
      "         [ 0.8306, -0.9357, -0.9417,  0.7000,  0.0394]],\n",
      "\n",
      "        [[ 0.8809,  0.9129, -0.9192,  0.9053,  0.8707],\n",
      "         [-0.6800, -0.4394,  0.6825, -0.0680, -0.8667]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0470 cost = 0.004356\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4820,  0.4202,  0.0973, -0.5186,  0.6095],\n",
      "         [ 0.9471, -0.5550, -0.6522,  0.9280, -0.6422]],\n",
      "\n",
      "        [[ 0.8345,  0.8326, -0.8735,  0.8214,  0.8486],\n",
      "         [-0.3842, -0.4306,  0.5193, -0.1042, -0.6847]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0470 cost = 0.007095\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8711, -0.8280, -0.9037,  0.8420, -0.0485],\n",
      "         [-0.3728,  0.5756,  0.1247, -0.4016,  0.5624]],\n",
      "\n",
      "        [[-0.7099, -0.4243,  0.7226, -0.0703, -0.8952],\n",
      "         [ 0.9036,  0.9023, -0.9338,  0.9000,  0.8591]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0470 cost = 0.002156\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4822,  0.4203,  0.0973, -0.5188,  0.6097],\n",
      "         [ 0.8712, -0.8281, -0.9037,  0.8421, -0.0486]],\n",
      "\n",
      "        [[ 0.8346,  0.8327, -0.8735,  0.8214,  0.8486],\n",
      "         [-0.7100, -0.4244,  0.7227, -0.0704, -0.8953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0471 cost = 0.004162\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3730,  0.5756,  0.1247, -0.4015,  0.5625],\n",
      "         [ 0.9472, -0.5556, -0.6526,  0.9281, -0.6425]],\n",
      "\n",
      "        [[ 0.9036,  0.9023, -0.9338,  0.9000,  0.8592],\n",
      "         [-0.3844, -0.4309,  0.5194, -0.1044, -0.6849]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0471 cost = 0.005598\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8310, -0.9358, -0.9418,  0.7007,  0.0392],\n",
      "         [-0.3970,  0.4831, -0.0291, -0.4412,  0.5971]],\n",
      "\n",
      "        [[-0.6801, -0.4396,  0.6827, -0.0685, -0.8668],\n",
      "         [ 0.8811,  0.9130, -0.9192,  0.9055,  0.8709]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0471 cost = 0.003351\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3972,  0.4832, -0.0290, -0.4412,  0.5971],\n",
      "         [ 0.8311, -0.9358, -0.9418,  0.7008,  0.0392]],\n",
      "\n",
      "        [[ 0.8811,  0.9130, -0.9193,  0.9055,  0.8710],\n",
      "         [-0.6802, -0.4397,  0.6827, -0.0686, -0.8668]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0472 cost = 0.004312\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3734,  0.5756,  0.1248, -0.4015,  0.5625],\n",
      "         [ 0.8715, -0.8283, -0.9039,  0.8423, -0.0491]],\n",
      "\n",
      "        [[ 0.9037,  0.9023, -0.9338,  0.9001,  0.8593],\n",
      "         [-0.7102, -0.4248,  0.7229, -0.0709, -0.8953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0472 cost = 0.002763\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9473, -0.5562, -0.6531,  0.9282, -0.6428],\n",
      "         [-0.4830,  0.4204,  0.0974, -0.5193,  0.6101]],\n",
      "\n",
      "        [[-0.3847, -0.4313,  0.5196, -0.1048, -0.6851],\n",
      "         [ 0.8348,  0.8328, -0.8736,  0.8216,  0.8488]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0472 cost = 0.007747\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4832,  0.4204,  0.0975, -0.5195,  0.6101],\n",
      "         [ 0.8716, -0.8283, -0.9040,  0.8424, -0.0493]],\n",
      "\n",
      "        [[ 0.8349,  0.8329, -0.8736,  0.8216,  0.8489],\n",
      "         [-0.7103, -0.4249,  0.7230, -0.0711, -0.8954]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0473 cost = 0.004122\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3979,  0.4838, -0.0286, -0.4414,  0.5973],\n",
      "         [-0.3739,  0.5755,  0.1250, -0.4014,  0.5626]],\n",
      "\n",
      "        [[ 0.8812,  0.9131, -0.9193,  0.9056,  0.8711],\n",
      "         [ 0.9038,  0.9024, -0.9338,  0.9001,  0.8594]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0473 cost = 0.003548\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9473, -0.5568, -0.6535,  0.9283, -0.6431],\n",
      "         [ 0.8315, -0.9359, -0.9419,  0.7014,  0.0390]],\n",
      "\n",
      "        [[-0.3849, -0.4315,  0.5198, -0.1051, -0.6852],\n",
      "         [-0.6803, -0.4399,  0.6829, -0.0691, -0.8668]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0473 cost = 0.008013\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8316, -0.9359, -0.9420,  0.7016,  0.0389],\n",
      "         [-0.3984,  0.4841, -0.0284, -0.4415,  0.5974]],\n",
      "\n",
      "        [[-0.6803, -0.4400,  0.6829, -0.0692, -0.8669],\n",
      "         [ 0.8813,  0.9131, -0.9193,  0.9057,  0.8712]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0474 cost = 0.003315\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4839,  0.4205,  0.0976, -0.5200,  0.6105],\n",
      "         [ 0.9474, -0.5573, -0.6537,  0.9283, -0.6433]],\n",
      "\n",
      "        [[ 0.8350,  0.8329, -0.8737,  0.8217,  0.8490],\n",
      "         [-0.3850, -0.4317,  0.5199, -0.1053, -0.6853]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0474 cost = 0.006958\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3745,  0.5755,  0.1251, -0.4014,  0.5628],\n",
      "         [ 0.8719, -0.8285, -0.9041,  0.8427, -0.0499]],\n",
      "\n",
      "        [[ 0.9038,  0.9024, -0.9339,  0.9002,  0.8596],\n",
      "         [-0.7105, -0.4253,  0.7232, -0.0718, -0.8955]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0474 cost = 0.002732\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8319, -0.9360, -0.9421,  0.7020,  0.0388],\n",
      "         [-0.3746,  0.5755,  0.1252, -0.4014,  0.5628]],\n",
      "\n",
      "        [[-0.6805, -0.4402,  0.6831, -0.0696, -0.8669],\n",
      "         [ 0.9039,  0.9024, -0.9339,  0.9002,  0.8596]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0475 cost = 0.002078\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8720, -0.8286, -0.9042,  0.8428, -0.0501],\n",
      "         [ 0.9474, -0.5579, -0.6541,  0.9284, -0.6436]],\n",
      "\n",
      "        [[-0.7106, -0.4255,  0.7233, -0.0720, -0.8955],\n",
      "         [-0.3852, -0.4321,  0.5200, -0.1057, -0.6855]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0475 cost = 0.006555\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3993,  0.4848, -0.0279, -0.4417,  0.5976],\n",
      "         [-0.4846,  0.4205,  0.0977, -0.5205,  0.6106]],\n",
      "\n",
      "        [[ 0.8814,  0.9132, -0.9194,  0.9058,  0.8713],\n",
      "         [ 0.8351,  0.8330, -0.8737,  0.8217,  0.8491]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0475 cost = 0.008825\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8721, -0.8287, -0.9043,  0.8429, -0.0504],\n",
      "         [-0.3995,  0.4850, -0.0278, -0.4417,  0.5977]],\n",
      "\n",
      "        [[-0.7107, -0.4258,  0.7234, -0.0723, -0.8956],\n",
      "         [ 0.8814,  0.9132, -0.9194,  0.9058,  0.8714]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0476 cost = 0.003332\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8323, -0.9361, -0.9422,  0.7025,  0.0386],\n",
      "         [-0.4850,  0.4205,  0.0977, -0.5208,  0.6108]],\n",
      "\n",
      "        [[-0.6806, -0.4405,  0.6832, -0.0700, -0.8670],\n",
      "         [ 0.8352,  0.8330, -0.8738,  0.8218,  0.8492]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0476 cost = 0.003880\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9475, -0.5585, -0.6546,  0.9285, -0.6439],\n",
      "         [-0.3752,  0.5755,  0.1253, -0.4014,  0.5630]],\n",
      "\n",
      "        [[-0.3855, -0.4325,  0.5202, -0.1060, -0.6857],\n",
      "         [ 0.9040,  0.9025, -0.9339,  0.9003,  0.8598]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0476 cost = 0.005293\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9475, -0.5587, -0.6547,  0.9285, -0.6440],\n",
      "         [-0.4002,  0.4854, -0.0275, -0.4419,  0.5979]],\n",
      "\n",
      "        [[-0.3856, -0.4326,  0.5203, -0.1061, -0.6858],\n",
      "         [ 0.8815,  0.9133, -0.9194,  0.9059,  0.8715]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0477 cost = 0.006606\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8723, -0.8289, -0.9044,  0.8431, -0.0509],\n",
      "         [ 0.8325, -0.9362, -0.9422,  0.7028,  0.0385]],\n",
      "\n",
      "        [[-0.7109, -0.4260,  0.7236, -0.0727, -0.8956],\n",
      "         [-0.6807, -0.4406,  0.6834, -0.0703, -0.8670]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0477 cost = 0.005557\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3755,  0.5755,  0.1254, -0.4014,  0.5631],\n",
      "         [-0.4855,  0.4206,  0.0978, -0.5213,  0.6112]],\n",
      "\n",
      "        [[ 0.9040,  0.9025, -0.9339,  0.9003,  0.8599],\n",
      "         [ 0.8353,  0.8332, -0.8738,  0.8219,  0.8494]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0477 cost = 0.008228\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8724, -0.8291, -0.9045,  0.8432, -0.0513],\n",
      "         [-0.4856,  0.4207,  0.0978, -0.5214,  0.6113]],\n",
      "\n",
      "        [[-0.7110, -0.4262,  0.7238, -0.0730, -0.8957],\n",
      "         [ 0.8354,  0.8332, -0.8739,  0.8219,  0.8495]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0478 cost = 0.003806\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4007,  0.4860, -0.0272, -0.4420,  0.5981],\n",
      "         [-0.3756,  0.5756,  0.1254, -0.4013,  0.5633]],\n",
      "\n",
      "        [[ 0.8816,  0.9133, -0.9195,  0.9060,  0.8717],\n",
      "         [ 0.9041,  0.9025, -0.9339,  0.9004,  0.8600]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0478 cost = 0.003462\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9476, -0.5598, -0.6553,  0.9286, -0.6445],\n",
      "         [ 0.8328, -0.9363, -0.9423,  0.7032,  0.0382]],\n",
      "\n",
      "        [[-0.3859, -0.4331,  0.5205, -0.1065, -0.6860],\n",
      "         [-0.6809, -0.4408,  0.6835, -0.0708, -0.8671]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0478 cost = 0.007828\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8726, -0.8293, -0.9046,  0.8433, -0.0517],\n",
      "         [-0.3757,  0.5757,  0.1255, -0.4013,  0.5634]],\n",
      "\n",
      "        [[-0.7112, -0.4265,  0.7239, -0.0734, -0.8957],\n",
      "         [ 0.9041,  0.9026, -0.9340,  0.9004,  0.8602]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0479 cost = 0.002072\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8329, -0.9363, -0.9424,  0.7035,  0.0381],\n",
      "         [-0.4010,  0.4865, -0.0269, -0.4420,  0.5984]],\n",
      "\n",
      "        [[-0.6809, -0.4409,  0.6836, -0.0711, -0.8671],\n",
      "         [ 0.8817,  0.9134, -0.9195,  0.9061,  0.8718]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0479 cost = 0.003234\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9476, -0.5605, -0.6557,  0.9286, -0.6448],\n",
      "         [-0.4859,  0.4211,  0.0978, -0.5218,  0.6121]],\n",
      "\n",
      "        [[-0.3861, -0.4334,  0.5207, -0.1067, -0.6861],\n",
      "         [ 0.8357,  0.8334, -0.8740,  0.8222,  0.8498]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0479 cost = 0.007517\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4859,  0.4212,  0.0978, -0.5218,  0.6122],\n",
      "         [-0.4013,  0.4867, -0.0268, -0.4421,  0.5985]],\n",
      "\n",
      "        [[ 0.8357,  0.8335, -0.8740,  0.8222,  0.8498],\n",
      "         [ 0.8818,  0.9135, -0.9195,  0.9062,  0.8719]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0480 cost = 0.006623\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8332, -0.9364, -0.9425,  0.7039,  0.0380],\n",
      "         [-0.3758,  0.5759,  0.1256, -0.4011,  0.5638]],\n",
      "\n",
      "        [[-0.6810, -0.4410,  0.6838, -0.0714, -0.8671],\n",
      "         [ 0.9042,  0.9026, -0.9340,  0.9005,  0.8604]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0480 cost = 0.002027\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9477, -0.5612, -0.6561,  0.9287, -0.6451],\n",
      "         [ 0.8728, -0.8296, -0.9048,  0.8435, -0.0523]],\n",
      "\n",
      "        [[-0.3863, -0.4336,  0.5208, -0.1069, -0.6862],\n",
      "         [-0.7115, -0.4268,  0.7242, -0.0740, -0.8958]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0480 cost = 0.006290\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4859,  0.4216,  0.0979, -0.5219,  0.6126],\n",
      "         [ 0.8333, -0.9364, -0.9425,  0.7041,  0.0379]],\n",
      "\n",
      "        [[ 0.8359,  0.8336, -0.8741,  0.8223,  0.8500],\n",
      "         [-0.6811, -0.4411,  0.6838, -0.0716, -0.8672]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0481 cost = 0.004630\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4014,  0.4876, -0.0266, -0.4420,  0.5989],\n",
      "         [ 0.8729, -0.8297, -0.9049,  0.8437, -0.0526]],\n",
      "\n",
      "        [[ 0.8819,  0.9135, -0.9196,  0.9063,  0.8721],\n",
      "         [-0.7116, -0.4269,  0.7243, -0.0742, -0.8959]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0481 cost = 0.003727\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3757,  0.5762,  0.1256, -0.4007,  0.5640],\n",
      "         [ 0.9477, -0.5619, -0.6565,  0.9287, -0.6454]],\n",
      "\n",
      "        [[ 0.9043,  0.9027, -0.9340,  0.9006,  0.8606],\n",
      "         [-0.3865, -0.4338,  0.5210, -0.1071, -0.6863]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0481 cost = 0.005329\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8730, -0.8298, -0.9050,  0.8438, -0.0529],\n",
      "         [-0.4862,  0.4218,  0.0979, -0.5221,  0.6129]],\n",
      "\n",
      "        [[-0.7117, -0.4271,  0.7244, -0.0745, -0.8959],\n",
      "         [ 0.8360,  0.8338, -0.8742,  0.8224,  0.8501]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0482 cost = 0.003736\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8337, -0.9365, -0.9426,  0.7047,  0.0377],\n",
      "         [-0.4017,  0.4881, -0.0264, -0.4420,  0.5991]],\n",
      "\n",
      "        [[-0.6812, -0.4413,  0.6840, -0.0721, -0.8672],\n",
      "         [ 0.8820,  0.9136, -0.9196,  0.9064,  0.8722]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0482 cost = 0.003190\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9478, -0.5625, -0.6569,  0.9288, -0.6457],\n",
      "         [-0.3758,  0.5762,  0.1257, -0.4006,  0.5642]],\n",
      "\n",
      "        [[-0.3867, -0.4341,  0.5211, -0.1073, -0.6865],\n",
      "         [ 0.9044,  0.9028, -0.9341,  0.9007,  0.8607]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0482 cost = 0.005147\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8732, -0.8300, -0.9051,  0.8439, -0.0533],\n",
      "         [-0.4864,  0.4219,  0.0980, -0.5224,  0.6132]],\n",
      "\n",
      "        [[-0.7118, -0.4273,  0.7245, -0.0749, -0.8960],\n",
      "         [ 0.8361,  0.8338, -0.8742,  0.8225,  0.8503]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0483 cost = 0.003720\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3759,  0.5763,  0.1258, -0.4005,  0.5642],\n",
      "         [-0.4021,  0.4885, -0.0261, -0.4420,  0.5992]],\n",
      "\n",
      "        [[ 0.9044,  0.9028, -0.9341,  0.9007,  0.8608],\n",
      "         [ 0.8820,  0.9136, -0.9196,  0.9065,  0.8723]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0483 cost = 0.004209\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8340, -0.9366, -0.9427,  0.7051,  0.0375],\n",
      "         [ 0.9478, -0.5632, -0.6573,  0.9289, -0.6460]],\n",
      "\n",
      "        [[-0.6813, -0.4414,  0.6841, -0.0725, -0.8673],\n",
      "         [-0.3869, -0.4343,  0.5213, -0.1076, -0.6866]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0483 cost = 0.005912\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8341, -0.9366, -0.9427,  0.7052,  0.0374],\n",
      "         [-0.4866,  0.4220,  0.0981, -0.5226,  0.6135]],\n",
      "\n",
      "        [[-0.6814, -0.4415,  0.6842, -0.0726, -0.8673],\n",
      "         [ 0.8362,  0.8339, -0.8743,  0.8226,  0.8504]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0484 cost = 0.003752\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4023,  0.4889, -0.0259, -0.4420,  0.5994],\n",
      "         [ 0.9478, -0.5636, -0.6575,  0.9289, -0.6462]],\n",
      "\n",
      "        [[ 0.8821,  0.9137, -0.9197,  0.9065,  0.8724],\n",
      "         [-0.3871, -0.4345,  0.5214, -0.1078, -0.6867]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0484 cost = 0.006366\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.5763,  0.1259, -0.4004,  0.5645],\n",
      "         [ 0.8735, -0.8303, -0.9053,  0.8442, -0.0540]],\n",
      "\n",
      "        [[ 0.9045,  0.9028, -0.9341,  0.9008,  0.8609],\n",
      "         [-0.7121, -0.4276,  0.7248, -0.0755, -0.8961]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0484 cost = 0.002606\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8735, -0.8304, -0.9053,  0.8442, -0.0541],\n",
      "         [-0.4025,  0.4892, -0.0258, -0.4421,  0.5995]],\n",
      "\n",
      "        [[-0.7122, -0.4276,  0.7248, -0.0757, -0.8961],\n",
      "         [ 0.8822,  0.9137, -0.9197,  0.9066,  0.8724]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0485 cost = 0.003194\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8344, -0.9367, -0.9428,  0.7057,  0.0372],\n",
      "         [ 0.9479, -0.5643, -0.6579,  0.9290, -0.6465]],\n",
      "\n",
      "        [[-0.6815, -0.4416,  0.6843, -0.0731, -0.8674],\n",
      "         [-0.3873, -0.4348,  0.5215, -0.1081, -0.6869]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0485 cost = 0.005866\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3763,  0.5763,  0.1260, -0.4004,  0.5647],\n",
      "         [-0.4870,  0.4220,  0.0982, -0.5231,  0.6139]],\n",
      "\n",
      "        [[ 0.9046,  0.9029, -0.9341,  0.9008,  0.8610],\n",
      "         [ 0.8364,  0.8340, -0.8743,  0.8227,  0.8505]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0485 cost = 0.008030\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8737, -0.8306, -0.9055,  0.8444, -0.0546],\n",
      "         [-0.4870,  0.4221,  0.0982, -0.5231,  0.6141]],\n",
      "\n",
      "        [[-0.7123, -0.4279,  0.7250, -0.0761, -0.8962],\n",
      "         [ 0.8364,  0.8341, -0.8744,  0.8227,  0.8506]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0486 cost = 0.003673\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4029,  0.4898, -0.0254, -0.4422,  0.5998],\n",
      "         [-0.3764,  0.5764,  0.1260, -0.4002,  0.5648]],\n",
      "\n",
      "        [[ 0.8823,  0.9138, -0.9197,  0.9067,  0.8725],\n",
      "         [ 0.9046,  0.9029, -0.9341,  0.9009,  0.8611]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0486 cost = 0.003331\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9480, -0.5649, -0.6583,  0.9291, -0.6468],\n",
      "         [ 0.8346, -0.9368, -0.9429,  0.7060,  0.0371]],\n",
      "\n",
      "        [[-0.3876, -0.4351,  0.5217, -0.1084, -0.6871],\n",
      "         [-0.6816, -0.4418,  0.6845, -0.0735, -0.8674]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0486 cost = 0.007555\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8738, -0.8307, -0.9055,  0.8445, -0.0549],\n",
      "         [-0.4872,  0.4223,  0.0982, -0.5233,  0.6145]],\n",
      "\n",
      "        [[-0.7125, -0.4280,  0.7252, -0.0764, -0.8962],\n",
      "         [ 0.8366,  0.8342, -0.8745,  0.8229,  0.8507]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0487 cost = 0.003656\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4033,  0.4903, -0.0252, -0.4422,  0.6000],\n",
      "         [-0.3764,  0.5765,  0.1261, -0.4001,  0.5651]],\n",
      "\n",
      "        [[ 0.8824,  0.9139, -0.9198,  0.9068,  0.8727],\n",
      "         [ 0.9047,  0.9029, -0.9342,  0.9009,  0.8613]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0487 cost = 0.003314\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9480, -0.5655, -0.6586,  0.9291, -0.6471],\n",
      "         [ 0.8348, -0.9369, -0.9430,  0.7063,  0.0369]],\n",
      "\n",
      "        [[-0.3878, -0.4354,  0.5219, -0.1086, -0.6872],\n",
      "         [-0.6818, -0.4419,  0.6846, -0.0739, -0.8675]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0487 cost = 0.007524\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4036,  0.4906, -0.0250, -0.4422,  0.6002],\n",
      "         [-0.3765,  0.5767,  0.1261, -0.4000,  0.5653]],\n",
      "\n",
      "        [[ 0.8824,  0.9139, -0.9198,  0.9068,  0.8727],\n",
      "         [ 0.9047,  0.9030, -0.9342,  0.9010,  0.8614]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0488 cost = 0.003302\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4874,  0.4227,  0.0983, -0.5236,  0.6152],\n",
      "         [ 0.8350, -0.9369, -0.9430,  0.7066,  0.0368]],\n",
      "\n",
      "        [[ 0.8368,  0.8344, -0.8745,  0.8230,  0.8509],\n",
      "         [-0.6818, -0.4420,  0.6847, -0.0741, -0.8675]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0488 cost = 0.004474\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8740, -0.8310, -0.9057,  0.8447, -0.0555],\n",
      "         [ 0.9480, -0.5662, -0.6589,  0.9292, -0.6473]],\n",
      "\n",
      "        [[-0.7128, -0.4284,  0.7254, -0.0771, -0.8964],\n",
      "         [-0.3880, -0.4357,  0.5221, -0.1088, -0.6873]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0488 cost = 0.006181\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8741, -0.8311, -0.9057,  0.8448, -0.0556],\n",
      "         [ 0.8352, -0.9370, -0.9431,  0.7069,  0.0367]],\n",
      "\n",
      "        [[-0.7128, -0.4285,  0.7255, -0.0773, -0.8964],\n",
      "         [-0.6819, -0.4421,  0.6848, -0.0744, -0.8676]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0489 cost = 0.005277\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4041,  0.4913, -0.0246, -0.4422,  0.6005],\n",
      "         [ 0.9481, -0.5667, -0.6591,  0.9292, -0.6476]],\n",
      "\n",
      "        [[ 0.8825,  0.9140, -0.9198,  0.9069,  0.8729],\n",
      "         [-0.3882, -0.4360,  0.5222, -0.1090, -0.6874]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0489 cost = 0.006214\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.5770,  0.1263, -0.3999,  0.5657],\n",
      "         [-0.4878,  0.4229,  0.0983, -0.5239,  0.6157]],\n",
      "\n",
      "        [[ 0.9048,  0.9030, -0.9342,  0.9011,  0.8615],\n",
      "         [ 0.8369,  0.8345, -0.8746,  0.8231,  0.8510]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0489 cost = 0.007932\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8355, -0.9371, -0.9432,  0.7075,  0.0365],\n",
      "         [-0.3767,  0.5770,  0.1263, -0.3998,  0.5658]],\n",
      "\n",
      "        [[-0.6820, -0.4424,  0.6850, -0.0749, -0.8677],\n",
      "         [ 0.9048,  0.9030, -0.9342,  0.9011,  0.8616]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0490 cost = 0.001938\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8744, -0.8313, -0.9059,  0.8450, -0.0562],\n",
      "         [-0.4044,  0.4918, -0.0243, -0.4423,  0.6008]],\n",
      "\n",
      "        [[-0.7131, -0.4290,  0.7257, -0.0779, -0.8965],\n",
      "         [ 0.8826,  0.9140, -0.9198,  0.9070,  0.8730]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0490 cost = 0.003113\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9481, -0.5674, -0.6595,  0.9293, -0.6479],\n",
      "         [-0.4879,  0.4233,  0.0983, -0.5240,  0.6162]],\n",
      "\n",
      "        [[-0.3885, -0.4365,  0.5224, -0.1093, -0.6877],\n",
      "         [ 0.8370,  0.8346, -0.8747,  0.8232,  0.8512]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0490 cost = 0.007175\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4879,  0.4233,  0.0983, -0.5241,  0.6163],\n",
      "         [ 0.8745, -0.8315, -0.9060,  0.8451, -0.0565]],\n",
      "\n",
      "        [[ 0.8371,  0.8346, -0.8747,  0.8233,  0.8512],\n",
      "         [-0.7132, -0.4292,  0.7258, -0.0781, -0.8965]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0491 cost = 0.003788\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3768,  0.5772,  0.1263, -0.3996,  0.5661],\n",
      "         [-0.4047,  0.4923, -0.0240, -0.4423,  0.6010]],\n",
      "\n",
      "        [[ 0.9049,  0.9031, -0.9342,  0.9012,  0.8618],\n",
      "         [ 0.8827,  0.9141, -0.9199,  0.9071,  0.8731]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0491 cost = 0.004072\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8359, -0.9372, -0.9433,  0.7082,  0.0362],\n",
      "         [ 0.9482, -0.5679, -0.6599,  0.9293, -0.6482]],\n",
      "\n",
      "        [[-0.6822, -0.4428,  0.6852, -0.0755, -0.8678],\n",
      "         [-0.3887, -0.4367,  0.5226, -0.1095, -0.6878]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0491 cost = 0.005705\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8746, -0.8316, -0.9061,  0.8452, -0.0568],\n",
      "         [-0.3769,  0.5773,  0.1264, -0.3995,  0.5663]],\n",
      "\n",
      "        [[-0.7134, -0.4295,  0.7260, -0.0785, -0.8966],\n",
      "         [ 0.9050,  0.9031, -0.9343,  0.9012,  0.8619]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0492 cost = 0.001947\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8361, -0.9372, -0.9434,  0.7084,  0.0361],\n",
      "         [-0.4050,  0.4929, -0.0238, -0.4423,  0.6013]],\n",
      "\n",
      "        [[-0.6823, -0.4429,  0.6853, -0.0757, -0.8678],\n",
      "         [ 0.8828,  0.9142, -0.9199,  0.9072,  0.8732]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0492 cost = 0.003047\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9482, -0.5685, -0.6602,  0.9294, -0.6484],\n",
      "         [-0.4883,  0.4237,  0.0984, -0.5244,  0.6169]],\n",
      "\n",
      "        [[-0.3889, -0.4371,  0.5227, -0.1097, -0.6880],\n",
      "         [ 0.8373,  0.8348, -0.8748,  0.8235,  0.8515]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0492 cost = 0.007117\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3770,  0.5774,  0.1264, -0.3994,  0.5665],\n",
      "         [ 0.9482, -0.5687, -0.6603,  0.9294, -0.6485]],\n",
      "\n",
      "        [[ 0.9051,  0.9032, -0.9343,  0.9013,  0.8620],\n",
      "         [-0.3890, -0.4371,  0.5228, -0.1098, -0.6880]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0493 cost = 0.005053\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4883,  0.4238,  0.0984, -0.5245,  0.6171],\n",
      "         [ 0.8749, -0.8319, -0.9062,  0.8455, -0.0574]],\n",
      "\n",
      "        [[ 0.8374,  0.8348, -0.8748,  0.8235,  0.8516],\n",
      "         [-0.7136, -0.4298,  0.7262, -0.0790, -0.8967]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0493 cost = 0.003748\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4053,  0.4934, -0.0236, -0.4424,  0.6015],\n",
      "         [ 0.8364, -0.9373, -0.9435,  0.7089,  0.0359]],\n",
      "\n",
      "        [[ 0.8829,  0.9142, -0.9200,  0.9073,  0.8733],\n",
      "         [-0.6824, -0.4431,  0.6854, -0.0761, -0.8679]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0493 cost = 0.003866\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3773,  0.5774,  0.1265, -0.3993,  0.5666],\n",
      "         [-0.4886,  0.4238,  0.0985, -0.5247,  0.6173]],\n",
      "\n",
      "        [[ 0.9051,  0.9032, -0.9343,  0.9014,  0.8621],\n",
      "         [ 0.8374,  0.8349, -0.8748,  0.8235,  0.8516]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0494 cost = 0.007826\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8750, -0.8320, -0.9063,  0.8456, -0.0577],\n",
      "         [ 0.9483, -0.5695, -0.6608,  0.9295, -0.6489]],\n",
      "\n",
      "        [[-0.7137, -0.4301,  0.7263, -0.0793, -0.8968],\n",
      "         [-0.3893, -0.4375,  0.5230, -0.1101, -0.6882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0494 cost = 0.006021\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4057,  0.4939, -0.0233, -0.4424,  0.6017],\n",
      "         [ 0.8366, -0.9374, -0.9435,  0.7092,  0.0357]],\n",
      "\n",
      "        [[ 0.8830,  0.9143, -0.9200,  0.9074,  0.8735],\n",
      "         [-0.6825, -0.4433,  0.6856, -0.0764, -0.8679]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0494 cost = 0.003847\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.5775,  0.1265, -0.3991,  0.5668],\n",
      "         [ 0.9484, -0.5698, -0.6610,  0.9296, -0.6490]],\n",
      "\n",
      "        [[ 0.9052,  0.9032, -0.9343,  0.9014,  0.8623],\n",
      "         [-0.3895, -0.4378,  0.5231, -0.1103, -0.6883]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0495 cost = 0.005004\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4059,  0.4942, -0.0231, -0.4424,  0.6019],\n",
      "         [ 0.8368, -0.9374, -0.9436,  0.7095,  0.0356]],\n",
      "\n",
      "        [[ 0.8830,  0.9143, -0.9200,  0.9074,  0.8735],\n",
      "         [-0.6826, -0.4434,  0.6856, -0.0767, -0.8679]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0495 cost = 0.003834\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4890,  0.4242,  0.0985, -0.5249,  0.6179],\n",
      "         [ 0.8753, -0.8321, -0.9065,  0.8458, -0.0582]],\n",
      "\n",
      "        [[ 0.8377,  0.8350, -0.8750,  0.8237,  0.8519],\n",
      "         [-0.7139, -0.4305,  0.7266, -0.0798, -0.8968]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0495 cost = 0.003706\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4892,  0.4242,  0.0985, -0.5250,  0.6180],\n",
      "         [ 0.8370, -0.9375, -0.9436,  0.7098,  0.0356]],\n",
      "\n",
      "        [[ 0.8377,  0.8351, -0.8750,  0.8238,  0.8519],\n",
      "         [-0.6826, -0.4435,  0.6857, -0.0769, -0.8680]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0496 cost = 0.004307\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3781,  0.5776,  0.1266, -0.3990,  0.5669],\n",
      "         [ 0.8754, -0.8322, -0.9065,  0.8459, -0.0584]],\n",
      "\n",
      "        [[ 0.9053,  0.9033, -0.9344,  0.9015,  0.8625],\n",
      "         [-0.7140, -0.4307,  0.7267, -0.0801, -0.8969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0496 cost = 0.002466\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9485, -0.5705, -0.6615,  0.9297, -0.6494],\n",
      "         [-0.4066,  0.4949, -0.0227, -0.4425,  0.6021]],\n",
      "\n",
      "        [[-0.3898, -0.4383,  0.5234, -0.1107, -0.6886],\n",
      "         [ 0.8831,  0.9144, -0.9201,  0.9075,  0.8737]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0496 cost = 0.006046\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8373, -0.9375, -0.9437,  0.7102,  0.0354],\n",
      "         [ 0.8755, -0.8322, -0.9066,  0.8460, -0.0586]],\n",
      "\n",
      "        [[-0.6827, -0.4437,  0.6858, -0.0772, -0.8680],\n",
      "         [-0.7141, -0.4309,  0.7268, -0.0803, -0.8969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0497 cost = 0.002859\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4899,  0.4243,  0.0986, -0.5255,  0.6183],\n",
      "         [ 0.9485, -0.5708, -0.6617,  0.9297, -0.6495]],\n",
      "\n",
      "        [[ 0.8379,  0.8352, -0.8751,  0.8239,  0.8521],\n",
      "         [-0.3900, -0.4385,  0.5235, -0.1108, -0.6887]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0497 cost = 0.006261\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4071,  0.4953, -0.0224, -0.4426,  0.6022],\n",
      "         [-0.3787,  0.5775,  0.1267, -0.3989,  0.5669]],\n",
      "\n",
      "        [[ 0.8832,  0.9144, -0.9201,  0.9076,  0.8738],\n",
      "         [ 0.9054,  0.9033, -0.9344,  0.9016,  0.8627]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0497 cost = 0.003149\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9485, -0.5711, -0.6619,  0.9298, -0.6497],\n",
      "         [ 0.8376, -0.9376, -0.9438,  0.7107,  0.0353]],\n",
      "\n",
      "        [[-0.3901, -0.4387,  0.5236, -0.1110, -0.6888],\n",
      "         [-0.6828, -0.4440,  0.6859, -0.0776, -0.8681]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0498 cost = 0.007178\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8758, -0.8324, -0.9067,  0.8463, -0.0591],\n",
      "         [-0.4075,  0.4957, -0.0223, -0.4426,  0.6023]],\n",
      "\n",
      "        [[-0.7144, -0.4313,  0.7270, -0.0809, -0.8970],\n",
      "         [ 0.8832,  0.9145, -0.9201,  0.9077,  0.8738]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0498 cost = 0.003000\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4906,  0.4243,  0.0986, -0.5259,  0.6185],\n",
      "         [-0.3791,  0.5775,  0.1268, -0.3988,  0.5670]],\n",
      "\n",
      "        [[ 0.8380,  0.8352, -0.8751,  0.8240,  0.8522],\n",
      "         [ 0.9054,  0.9033, -0.9344,  0.9016,  0.8627]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0498 cost = 0.004457\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8759, -0.8324, -0.9068,  0.8463, -0.0593],\n",
      "         [ 0.8379, -0.9377, -0.9439,  0.7111,  0.0351]],\n",
      "\n",
      "        [[-0.7145, -0.4315,  0.7271, -0.0811, -0.8970],\n",
      "         [-0.6829, -0.4442,  0.6861, -0.0779, -0.8681]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0499 cost = 0.005032\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3789,  0.5777,  0.1269, -0.3985,  0.5671],\n",
      "         [-0.4906,  0.4245,  0.0987, -0.5259,  0.6187]],\n",
      "\n",
      "        [[ 0.9055,  0.9034, -0.9344,  0.9017,  0.8628],\n",
      "         [ 0.8381,  0.8353, -0.8752,  0.8240,  0.8523]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0499 cost = 0.007710\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9486, -0.5719, -0.6625,  0.9299, -0.6501],\n",
      "         [-0.4078,  0.4964, -0.0219, -0.4425,  0.6026]],\n",
      "\n",
      "        [[-0.3905, -0.4393,  0.5238, -0.1115, -0.6891],\n",
      "         [ 0.8833,  0.9145, -0.9202,  0.9078,  0.8739]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0499 cost = 0.005966\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4078,  0.4967, -0.0218, -0.4424,  0.6027],\n",
      "         [-0.3787,  0.5779,  0.1269, -0.3982,  0.5673]],\n",
      "\n",
      "        [[ 0.8834,  0.9145, -0.9202,  0.9078,  0.8740],\n",
      "         [ 0.9055,  0.9034, -0.9345,  0.9017,  0.8629]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0500 cost = 0.003112\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8761, -0.8326, -0.9069,  0.8465, -0.0597],\n",
      "         [-0.4906,  0.4250,  0.0986, -0.5258,  0.6192]],\n",
      "\n",
      "        [[-0.7147, -0.4320,  0.7273, -0.0816, -0.8971],\n",
      "         [ 0.8383,  0.8354, -0.8753,  0.8242,  0.8525]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0500 cost = 0.003449\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9486, -0.5723, -0.6628,  0.9299, -0.6503],\n",
      "         [ 0.8383, -0.9378, -0.9440,  0.7118,  0.0349]],\n",
      "\n",
      "        [[-0.3906, -0.4396,  0.5240, -0.1116, -0.6892],\n",
      "         [-0.6831, -0.4446,  0.6862, -0.0785, -0.8683]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0500 cost = 0.007092\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8761, -0.8326, -0.9070,  0.8466, -0.0599],\n",
      "         [ 0.9486, -0.5726, -0.6629,  0.9299, -0.6504]],\n",
      "\n",
      "        [[-0.7148, -0.4322,  0.7274, -0.0819, -0.8972],\n",
      "         [-0.3907, -0.4397,  0.5240, -0.1117, -0.6892]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0501 cost = 0.005846\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.5783,  0.1269, -0.3977,  0.5677],\n",
      "         [ 0.8385, -0.9378, -0.9441,  0.7121,  0.0347]],\n",
      "\n",
      "        [[ 0.9056,  0.9034, -0.9345,  0.9018,  0.8631],\n",
      "         [-0.6831, -0.4447,  0.6863, -0.0788, -0.8683]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0501 cost = 0.002769\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4082,  0.4977, -0.0215, -0.4422,  0.6031],\n",
      "         [-0.4906,  0.4255,  0.0987, -0.5258,  0.6197]],\n",
      "\n",
      "        [[ 0.8835,  0.9146, -0.9202,  0.9079,  0.8742],\n",
      "         [ 0.8384,  0.8355, -0.8753,  0.8243,  0.8526]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0501 cost = 0.008106\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8387, -0.9379, -0.9442,  0.7125,  0.0346],\n",
      "         [-0.4906,  0.4256,  0.0987, -0.5259,  0.6199]],\n",
      "\n",
      "        [[-0.6832, -0.4449,  0.6864, -0.0791, -0.8684],\n",
      "         [ 0.8385,  0.8356, -0.8754,  0.8244,  0.8527]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0502 cost = 0.003474\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8764, -0.8329, -0.9071,  0.8468, -0.0604],\n",
      "         [ 0.9487, -0.5734, -0.6634,  0.9300, -0.6507]],\n",
      "\n",
      "        [[-0.7150, -0.4326,  0.7276, -0.0824, -0.8973],\n",
      "         [-0.3910, -0.4402,  0.5243, -0.1119, -0.6894]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0502 cost = 0.005810\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.5786,  0.1270, -0.3974,  0.5680],\n",
      "         [-0.4084,  0.4982, -0.0212, -0.4422,  0.6034]],\n",
      "\n",
      "        [[ 0.9057,  0.9035, -0.9345,  0.9019,  0.8633],\n",
      "         [ 0.8836,  0.9147, -0.9203,  0.9080,  0.8743]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0502 cost = 0.003880\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4907,  0.4259,  0.0987, -0.5260,  0.6204],\n",
      "         [ 0.8390, -0.9380, -0.9442,  0.7129,  0.0344]],\n",
      "\n",
      "        [[ 0.8386,  0.8357, -0.8755,  0.8245,  0.8529],\n",
      "         [-0.6833, -0.4451,  0.6866, -0.0795, -0.8684]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0503 cost = 0.004149\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9487, -0.5739, -0.6637,  0.9300, -0.6510],\n",
      "         [-0.3784,  0.5787,  0.1270, -0.3972,  0.5682]],\n",
      "\n",
      "        [[-0.3912, -0.4405,  0.5244, -0.1121, -0.6895],\n",
      "         [ 0.9058,  0.9035, -0.9346,  0.9020,  0.8634]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0503 cost = 0.004687\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4086,  0.4988, -0.0210, -0.4421,  0.6037],\n",
      "         [ 0.8766, -0.8331, -0.9073,  0.8470, -0.0609]],\n",
      "\n",
      "        [[ 0.8837,  0.9147, -0.9203,  0.9081,  0.8745],\n",
      "         [-0.7152, -0.4330,  0.7278, -0.0829, -0.8974]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0503 cost = 0.003336\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8393, -0.9380, -0.9443,  0.7133,  0.0343],\n",
      "         [-0.3785,  0.5788,  0.1270, -0.3971,  0.5683]],\n",
      "\n",
      "        [[-0.6834, -0.4452,  0.6867, -0.0799, -0.8685],\n",
      "         [ 0.9058,  0.9036, -0.9346,  0.9020,  0.8635]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0504 cost = 0.001812\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4911,  0.4262,  0.0987, -0.5262,  0.6208],\n",
      "         [ 0.8767, -0.8331, -0.9073,  0.8471, -0.0612]],\n",
      "\n",
      "        [[ 0.8388,  0.8359, -0.8756,  0.8247,  0.8531],\n",
      "         [-0.7153, -0.4331,  0.7279, -0.0832, -0.8974]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0504 cost = 0.003551\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9488, -0.5745, -0.6641,  0.9301, -0.6513],\n",
      "         [-0.4090,  0.4994, -0.0208, -0.4421,  0.6039]],\n",
      "\n",
      "        [[-0.3915, -0.4408,  0.5247, -0.1123, -0.6897],\n",
      "         [ 0.8838,  0.9148, -0.9203,  0.9082,  0.8746]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0504 cost = 0.005839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8768, -0.8332, -0.9074,  0.8472, -0.0614],\n",
      "         [-0.4091,  0.4995, -0.0207, -0.4422,  0.6039]],\n",
      "\n",
      "        [[-0.7154, -0.4333,  0.7280, -0.0834, -0.8975],\n",
      "         [ 0.8838,  0.9148, -0.9203,  0.9082,  0.8746]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0505 cost = 0.002907\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8396, -0.9381, -0.9444,  0.7138,  0.0341],\n",
      "         [-0.3787,  0.5789,  0.1271, -0.3969,  0.5684]],\n",
      "\n",
      "        [[-0.6835, -0.4455,  0.6868, -0.0803, -0.8685],\n",
      "         [ 0.9059,  0.9036, -0.9346,  0.9021,  0.8637]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0505 cost = 0.001801\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9488, -0.5751, -0.6644,  0.9301, -0.6515],\n",
      "         [-0.4915,  0.4264,  0.0987, -0.5266,  0.6211]],\n",
      "\n",
      "        [[-0.3917, -0.4411,  0.5248, -0.1125, -0.6898],\n",
      "         [ 0.8390,  0.8360, -0.8757,  0.8248,  0.8533]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0505 cost = 0.006749\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8397, -0.9382, -0.9445,  0.7140,  0.0340],\n",
      "         [-0.3789,  0.5789,  0.1271, -0.3969,  0.5685]],\n",
      "\n",
      "        [[-0.6836, -0.4455,  0.6869, -0.0805, -0.8685],\n",
      "         [ 0.9060,  0.9036, -0.9346,  0.9021,  0.8638]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0506 cost = 0.001796\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9489, -0.5755, -0.6646,  0.9302, -0.6517],\n",
      "         [-0.4097,  0.5000, -0.0204, -0.4422,  0.6041]],\n",
      "\n",
      "        [[-0.3918, -0.4412,  0.5249, -0.1127, -0.6899],\n",
      "         [ 0.8839,  0.9149, -0.9204,  0.9083,  0.8747]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0506 cost = 0.005796\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8770, -0.8335, -0.9076,  0.8474, -0.0621],\n",
      "         [-0.4918,  0.4265,  0.0988, -0.5268,  0.6213]],\n",
      "\n",
      "        [[-0.7156, -0.4336,  0.7282, -0.0839, -0.8975],\n",
      "         [ 0.8391,  0.8361, -0.8757,  0.8249,  0.8534]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0506 cost = 0.003355\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4100,  0.5002, -0.0202, -0.4422,  0.6042],\n",
      "         [ 0.8399, -0.9382, -0.9445,  0.7143,  0.0338]],\n",
      "\n",
      "        [[ 0.8839,  0.9149, -0.9204,  0.9084,  0.8748],\n",
      "         [-0.6837, -0.4456,  0.6870, -0.0807, -0.8686]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0507 cost = 0.003607\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9489, -0.5763, -0.6649,  0.9302, -0.6520],\n",
      "         [-0.3791,  0.5790,  0.1273, -0.3969,  0.5687]],\n",
      "\n",
      "        [[-0.3920, -0.4414,  0.5250, -0.1130, -0.6900],\n",
      "         [ 0.9060,  0.9037, -0.9346,  0.9022,  0.8639]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0507 cost = 0.004604\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4921,  0.4265,  0.0989, -0.5271,  0.6215],\n",
      "         [ 0.8772, -0.8337, -0.9077,  0.8475, -0.0625]],\n",
      "\n",
      "        [[ 0.8392,  0.8361, -0.8757,  0.8249,  0.8535],\n",
      "         [-0.7158, -0.4337,  0.7284, -0.0842, -0.8976]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0507 cost = 0.003501\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4922,  0.4265,  0.0989, -0.5272,  0.6216],\n",
      "         [-0.4105,  0.5005, -0.0199, -0.4423,  0.6043]],\n",
      "\n",
      "        [[ 0.8392,  0.8361, -0.8757,  0.8249,  0.8535],\n",
      "         [ 0.8840,  0.9150, -0.9204,  0.9084,  0.8749]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0508 cost = 0.005877\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9489, -0.5770, -0.6653,  0.9303, -0.6523],\n",
      "         [ 0.8773, -0.8338, -0.9077,  0.8476, -0.0627]],\n",
      "\n",
      "        [[-0.3921, -0.4416,  0.5251, -0.1133, -0.6901],\n",
      "         [-0.7159, -0.4338,  0.7285, -0.0844, -0.8976]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0508 cost = 0.005557\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3793,  0.5791,  0.1274, -0.3967,  0.5688],\n",
      "         [ 0.8402, -0.9383, -0.9446,  0.7148,  0.0336]],\n",
      "\n",
      "        [[ 0.9061,  0.9037, -0.9347,  0.9022,  0.8640],\n",
      "         [-0.6838, -0.4458,  0.6872, -0.0812, -0.8686]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0508 cost = 0.002674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9490, -0.5775, -0.6656,  0.9303, -0.6525],\n",
      "         [-0.4108,  0.5010, -0.0198, -0.4422,  0.6045]],\n",
      "\n",
      "        [[-0.3922, -0.4418,  0.5252, -0.1134, -0.6902],\n",
      "         [ 0.8841,  0.9150, -0.9205,  0.9085,  0.8750]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0509 cost = 0.005724\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8404, -0.9384, -0.9447,  0.7151,  0.0335],\n",
      "         [-0.4925,  0.4268,  0.0990, -0.5274,  0.6218]],\n",
      "\n",
      "        [[-0.6839, -0.4458,  0.6872, -0.0814, -0.8686],\n",
      "         [ 0.8393,  0.8362, -0.8758,  0.8250,  0.8536]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0509 cost = 0.003371\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5792,  0.1275, -0.3965,  0.5689],\n",
      "         [ 0.8775, -0.8340, -0.9079,  0.8478, -0.0633]],\n",
      "\n",
      "        [[ 0.9061,  0.9037, -0.9347,  0.9023,  0.8641],\n",
      "         [-0.7161, -0.4340,  0.7287, -0.0849, -0.8977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0509 cost = 0.002318\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4111,  0.5014, -0.0196, -0.4422,  0.6046],\n",
      "         [ 0.9490, -0.5783, -0.6660,  0.9304, -0.6528]],\n",
      "\n",
      "        [[ 0.8841,  0.9151, -0.9205,  0.9086,  0.8751],\n",
      "         [-0.3924, -0.4420,  0.5253, -0.1137, -0.6903]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0510 cost = 0.005628\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4928,  0.4269,  0.0990, -0.5276,  0.6220],\n",
      "         [ 0.8776, -0.8341, -0.9080,  0.8479, -0.0636]],\n",
      "\n",
      "        [[ 0.8394,  0.8363, -0.8758,  0.8250,  0.8537],\n",
      "         [-0.7161, -0.4341,  0.7288, -0.0851, -0.8977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0510 cost = 0.003459\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8407, -0.9384, -0.9448,  0.7155,  0.0334],\n",
      "         [-0.3796,  0.5792,  0.1276, -0.3964,  0.5690]],\n",
      "\n",
      "        [[-0.6840, -0.4459,  0.6874, -0.0817, -0.8687],\n",
      "         [ 0.9062,  0.9038, -0.9347,  0.9023,  0.8642]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0510 cost = 0.001761\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4930,  0.4269,  0.0991, -0.5278,  0.6221],\n",
      "         [ 0.9491, -0.5789, -0.6664,  0.9305, -0.6531]],\n",
      "\n",
      "        [[ 0.8395,  0.8364, -0.8758,  0.8251,  0.8538],\n",
      "         [-0.3926, -0.4422,  0.5254, -0.1140, -0.6904]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0511 cost = 0.005886\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4116,  0.5020, -0.0193, -0.4422,  0.6048],\n",
      "         [ 0.8408, -0.9385, -0.9448,  0.7157,  0.0333]],\n",
      "\n",
      "        [[ 0.8842,  0.9151, -0.9205,  0.9087,  0.8751],\n",
      "         [-0.6841, -0.4460,  0.6874, -0.0819, -0.8687]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0511 cost = 0.003536\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8778, -0.8342, -0.9081,  0.8481, -0.0640],\n",
      "         [-0.3799,  0.5791,  0.1277, -0.3963,  0.5690]],\n",
      "\n",
      "        [[-0.7163, -0.4344,  0.7289, -0.0856, -0.8978],\n",
      "         [ 0.9062,  0.9038, -0.9347,  0.9023,  0.8643]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0511 cost = 0.001779\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4120,  0.5022, -0.0191, -0.4422,  0.6048],\n",
      "         [ 0.8778, -0.8343, -0.9081,  0.8482, -0.0641]],\n",
      "\n",
      "        [[ 0.8843,  0.9152, -0.9205,  0.9087,  0.8752],\n",
      "         [-0.7164, -0.4344,  0.7290, -0.0857, -0.8978]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0512 cost = 0.003209\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.5791,  0.1277, -0.3963,  0.5690],\n",
      "         [ 0.8411, -0.9385, -0.9449,  0.7161,  0.0332]],\n",
      "\n",
      "        [[ 0.9063,  0.9038, -0.9347,  0.9024,  0.8644],\n",
      "         [-0.6841, -0.4462,  0.6875, -0.0822, -0.8688]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0512 cost = 0.002631\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9492, -0.5796, -0.6669,  0.9306, -0.6534],\n",
      "         [-0.4939,  0.4269,  0.0992, -0.5284,  0.6223]],\n",
      "\n",
      "        [[-0.3929, -0.4426,  0.5256, -0.1145, -0.6906],\n",
      "         [ 0.8396,  0.8364, -0.8759,  0.8251,  0.8539]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0512 cost = 0.006558\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3804,  0.5791,  0.1278, -0.3963,  0.5691],\n",
      "         [ 0.8780, -0.8344, -0.9082,  0.8483, -0.0644]],\n",
      "\n",
      "        [[ 0.9063,  0.9038, -0.9347,  0.9024,  0.8644],\n",
      "         [-0.7165, -0.4347,  0.7291, -0.0860, -0.8979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0513 cost = 0.002285\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4942,  0.4268,  0.0993, -0.5286,  0.6224],\n",
      "         [-0.4127,  0.5027, -0.0187, -0.4423,  0.6049]],\n",
      "\n",
      "        [[ 0.8397,  0.8365, -0.8759,  0.8252,  0.8539],\n",
      "         [ 0.8843,  0.9152, -0.9206,  0.9088,  0.8753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0513 cost = 0.005760\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9492, -0.5801, -0.6672,  0.9306, -0.6536],\n",
      "         [ 0.8414, -0.9386, -0.9449,  0.7165,  0.0330]],\n",
      "\n",
      "        [[-0.3930, -0.4428,  0.5257, -0.1147, -0.6907],\n",
      "         [-0.6843, -0.4463,  0.6877, -0.0826, -0.8688]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0513 cost = 0.006699\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4129,  0.5030, -0.0186, -0.4423,  0.6050],\n",
      "         [ 0.9492, -0.5803, -0.6673,  0.9307, -0.6537]],\n",
      "\n",
      "        [[ 0.8844,  0.9153, -0.9206,  0.9088,  0.8753],\n",
      "         [-0.3931, -0.4429,  0.5258, -0.1148, -0.6908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0514 cost = 0.005523\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8416, -0.9386, -0.9450,  0.7168,  0.0329],\n",
      "         [ 0.8782, -0.8345, -0.9083,  0.8485, -0.0649]],\n",
      "\n",
      "        [[-0.6843, -0.4465,  0.6877, -0.0828, -0.8688],\n",
      "         [-0.7167, -0.4349,  0.7293, -0.0865, -0.8979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0514 cost = 0.002648\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.5791,  0.1279, -0.3960,  0.5692],\n",
      "         [-0.4946,  0.4270,  0.0993, -0.5288,  0.6226]],\n",
      "\n",
      "        [[ 0.9064,  0.9039, -0.9348,  0.9024,  0.8646],\n",
      "         [ 0.8398,  0.8366, -0.8759,  0.8253,  0.8540]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0514 cost = 0.007400\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8417, -0.9387, -0.9451,  0.7171,  0.0328],\n",
      "         [-0.4131,  0.5036, -0.0184, -0.4422,  0.6052]],\n",
      "\n",
      "        [[-0.6844, -0.4466,  0.6878, -0.0830, -0.8689],\n",
      "         [ 0.8845,  0.9153, -0.9206,  0.9089,  0.8754]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0515 cost = 0.002754\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.5792,  0.1279, -0.3958,  0.5693],\n",
      "         [ 0.8784, -0.8346, -0.9084,  0.8486, -0.0652]],\n",
      "\n",
      "        [[ 0.9064,  0.9039, -0.9348,  0.9025,  0.8647],\n",
      "         [-0.7168, -0.4352,  0.7294, -0.0869, -0.8980]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0515 cost = 0.002261\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4947,  0.4273,  0.0993, -0.5289,  0.6230],\n",
      "         [ 0.9493, -0.5810, -0.6679,  0.9307, -0.6541]],\n",
      "\n",
      "        [[ 0.8399,  0.8367, -0.8760,  0.8254,  0.8542],\n",
      "         [-0.3934, -0.4434,  0.5260, -0.1151, -0.6910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0515 cost = 0.005765\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8420, -0.9387, -0.9451,  0.7174,  0.0327],\n",
      "         [ 0.9493, -0.5811, -0.6681,  0.9308, -0.6541]],\n",
      "\n",
      "        [[-0.6844, -0.4467,  0.6879, -0.0833, -0.8689],\n",
      "         [-0.3935, -0.4435,  0.5260, -0.1152, -0.6910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0516 cost = 0.005131\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3810,  0.5793,  0.1280, -0.3956,  0.5694],\n",
      "         [ 0.8786, -0.8346, -0.9085,  0.8488, -0.0655]],\n",
      "\n",
      "        [[ 0.9065,  0.9039, -0.9348,  0.9026,  0.8648],\n",
      "         [-0.7170, -0.4355,  0.7296, -0.0872, -0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0516 cost = 0.002250\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4949,  0.4274,  0.0993, -0.5290,  0.6232],\n",
      "         [-0.4134,  0.5045, -0.0181, -0.4422,  0.6055]],\n",
      "\n",
      "        [[ 0.8400,  0.8368, -0.8761,  0.8255,  0.8543],\n",
      "         [ 0.8846,  0.9154, -0.9207,  0.9090,  0.8756]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0516 cost = 0.005676\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8787, -0.8346, -0.9086,  0.8489, -0.0657],\n",
      "         [-0.4950,  0.4275,  0.0994, -0.5290,  0.6233]],\n",
      "\n",
      "        [[-0.7171, -0.4357,  0.7297, -0.0875, -0.8981],\n",
      "         [ 0.8401,  0.8368, -0.8761,  0.8255,  0.8544]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0517 cost = 0.003220\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9494, -0.5816, -0.6685,  0.9308, -0.6544],\n",
      "         [-0.3811,  0.5793,  0.1280, -0.3953,  0.5696]],\n",
      "\n",
      "        [[-0.3937, -0.4439,  0.5262, -0.1155, -0.6912],\n",
      "         [ 0.9066,  0.9040, -0.9348,  0.9026,  0.8649]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0517 cost = 0.004399\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4135,  0.5052, -0.0180, -0.4420,  0.6057],\n",
      "         [ 0.8423, -0.9388, -0.9452,  0.7180,  0.0325]],\n",
      "\n",
      "        [[ 0.8847,  0.9154, -0.9207,  0.9091,  0.8757],\n",
      "         [-0.6846, -0.4470,  0.6881, -0.0838, -0.8690]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0517 cost = 0.003431\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9494, -0.5818, -0.6688,  0.9309, -0.6545],\n",
      "         [ 0.8788, -0.8347, -0.9087,  0.8490, -0.0660]],\n",
      "\n",
      "        [[-0.3939, -0.4440,  0.5263, -0.1156, -0.6913],\n",
      "         [-0.7172, -0.4359,  0.7298, -0.0878, -0.8982]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0518 cost = 0.005324\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8425, -0.9388, -0.9453,  0.7182,  0.0324],\n",
      "         [-0.4951,  0.4278,  0.0994, -0.5290,  0.6237]],\n",
      "\n",
      "        [[-0.6846, -0.4472,  0.6881, -0.0840, -0.8690],\n",
      "         [ 0.8403,  0.8369, -0.8762,  0.8257,  0.8545]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0518 cost = 0.003255\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4136,  0.5057, -0.0179, -0.4419,  0.6059],\n",
      "         [-0.3811,  0.5795,  0.1281, -0.3949,  0.5697]],\n",
      "\n",
      "        [[ 0.8847,  0.9155, -0.9207,  0.9092,  0.8757],\n",
      "         [ 0.9066,  0.9040, -0.9349,  0.9027,  0.8651]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0518 cost = 0.002850\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8426, -0.9388, -0.9453,  0.7184,  0.0323],\n",
      "         [-0.3811,  0.5795,  0.1281, -0.3949,  0.5698]],\n",
      "\n",
      "        [[-0.6847, -0.4472,  0.6882, -0.0842, -0.8691],\n",
      "         [ 0.9066,  0.9040, -0.9349,  0.9027,  0.8651]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0519 cost = 0.001698\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4953,  0.4279,  0.0994, -0.5292,  0.6240],\n",
      "         [-0.4139,  0.5060, -0.0177, -0.4418,  0.6061]],\n",
      "\n",
      "        [[ 0.8404,  0.8370, -0.8762,  0.8257,  0.8546],\n",
      "         [ 0.8848,  0.9155, -0.9207,  0.9092,  0.8758]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0519 cost = 0.005605\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9495, -0.5825, -0.6693,  0.9309, -0.6549],\n",
      "         [ 0.8791, -0.8348, -0.9089,  0.8492, -0.0665]],\n",
      "\n",
      "        [[-0.3941, -0.4444,  0.5265, -0.1159, -0.6915],\n",
      "         [-0.7174, -0.4362,  0.7300, -0.0884, -0.8983]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0519 cost = 0.005288\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4954,  0.4282,  0.0994, -0.5292,  0.6242],\n",
      "         [ 0.8791, -0.8348, -0.9089,  0.8492, -0.0666]],\n",
      "\n",
      "        [[ 0.8405,  0.8371, -0.8763,  0.8258,  0.8547],\n",
      "         [-0.7175, -0.4363,  0.7301, -0.0885, -0.8983]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0520 cost = 0.003307\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9495, -0.5828, -0.6695,  0.9310, -0.6550],\n",
      "         [-0.3810,  0.5798,  0.1282, -0.3944,  0.5700]],\n",
      "\n",
      "        [[-0.3942, -0.4445,  0.5266, -0.1160, -0.6916],\n",
      "         [ 0.9067,  0.9041, -0.9349,  0.9028,  0.8653]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0520 cost = 0.004345\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8429, -0.9389, -0.9454,  0.7189,  0.0321],\n",
      "         [-0.4141,  0.5070, -0.0176, -0.4416,  0.6064]],\n",
      "\n",
      "        [[-0.6848, -0.4474,  0.6884, -0.0846, -0.8691],\n",
      "         [ 0.8849,  0.9156, -0.9208,  0.9093,  0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0520 cost = 0.002689\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9495, -0.5832, -0.6698,  0.9310, -0.6552],\n",
      "         [-0.4142,  0.5071, -0.0175, -0.4415,  0.6064]],\n",
      "\n",
      "        [[-0.3944, -0.4447,  0.5267, -0.1162, -0.6916],\n",
      "         [ 0.8849,  0.9156, -0.9208,  0.9093,  0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0521 cost = 0.005438\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8431, -0.9389, -0.9454,  0.7190,  0.0320],\n",
      "         [ 0.8793, -0.8349, -0.9090,  0.8494, -0.0670]],\n",
      "\n",
      "        [[-0.6849, -0.4475,  0.6884, -0.0848, -0.8691],\n",
      "         [-0.7176, -0.4365,  0.7302, -0.0889, -0.8983]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0521 cost = 0.002572\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3810,  0.5800,  0.1283, -0.3941,  0.5702],\n",
      "         [-0.4956,  0.4286,  0.0995, -0.5293,  0.6246]],\n",
      "\n",
      "        [[ 0.9068,  0.9041, -0.9349,  0.9028,  0.8654],\n",
      "         [ 0.8406,  0.8373, -0.8764,  0.8259,  0.8549]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0521 cost = 0.007250\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4956,  0.4287,  0.0995, -0.5292,  0.6248],\n",
      "         [-0.4144,  0.5077, -0.0174, -0.4414,  0.6066]],\n",
      "\n",
      "        [[ 0.8407,  0.8373, -0.8764,  0.8260,  0.8549],\n",
      "         [ 0.8850,  0.9156, -0.9208,  0.9094,  0.8760]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0522 cost = 0.005534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3809,  0.5802,  0.1283, -0.3938,  0.5704],\n",
      "         [ 0.8794, -0.8350, -0.9091,  0.8495, -0.0673]],\n",
      "\n",
      "        [[ 0.9068,  0.9042, -0.9349,  0.9029,  0.8655],\n",
      "         [-0.7178, -0.4367,  0.7304, -0.0892, -0.8984]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0522 cost = 0.002193\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9496, -0.5840, -0.6704,  0.9310, -0.6555],\n",
      "         [ 0.8433, -0.9389, -0.9455,  0.7193,  0.0319]],\n",
      "\n",
      "        [[-0.3946, -0.4450,  0.5269, -0.1164, -0.6918],\n",
      "         [-0.6849, -0.4476,  0.6885, -0.0850, -0.8692]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0522 cost = 0.006462\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.5803,  0.1283, -0.3935,  0.5706],\n",
      "         [-0.4955,  0.4292,  0.0995, -0.5291,  0.6252]],\n",
      "\n",
      "        [[ 0.9069,  0.9042, -0.9350,  0.9029,  0.8656],\n",
      "         [ 0.8409,  0.8375, -0.8765,  0.8262,  0.8552]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0523 cost = 0.007208\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9496, -0.5843, -0.6707,  0.9310, -0.6556],\n",
      "         [ 0.8795, -0.8350, -0.9092,  0.8496, -0.0675]],\n",
      "\n",
      "        [[-0.3947, -0.4451,  0.5270, -0.1165, -0.6918],\n",
      "         [-0.7179, -0.4368,  0.7305, -0.0895, -0.8984]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0523 cost = 0.005212\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4143,  0.5091, -0.0174, -0.4410,  0.6072],\n",
      "         [ 0.8434, -0.9390, -0.9455,  0.7195,  0.0318]],\n",
      "\n",
      "        [[ 0.8851,  0.9157, -0.9209,  0.9096,  0.8763],\n",
      "         [-0.6850, -0.4477,  0.6886, -0.0853, -0.8692]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0523 cost = 0.003337\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3806,  0.5806,  0.1282, -0.3930,  0.5708],\n",
      "         [-0.4143,  0.5094, -0.0174, -0.4409,  0.6073]],\n",
      "\n",
      "        [[ 0.9070,  0.9043, -0.9350,  0.9030,  0.8659],\n",
      "         [ 0.8852,  0.9158, -0.9209,  0.9096,  0.8764]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0524 cost = 0.003558\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9496, -0.5848, -0.6711,  0.9311, -0.6558],\n",
      "         [-0.4953,  0.4299,  0.0994, -0.5288,  0.6259]],\n",
      "\n",
      "        [[-0.3949, -0.4453,  0.5271, -0.1166, -0.6919],\n",
      "         [ 0.8411,  0.8377, -0.8767,  0.8264,  0.8555]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0524 cost = 0.006266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8797, -0.8350, -0.9094,  0.8497, -0.0679],\n",
      "         [ 0.8436, -0.9390, -0.9456,  0.7198,  0.0317]],\n",
      "\n",
      "        [[-0.7180, -0.4371,  0.7306, -0.0899, -0.8985],\n",
      "         [-0.6850, -0.4479,  0.6886, -0.0856, -0.8692]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0524 cost = 0.004527\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8437, -0.9390, -0.9456,  0.7200,  0.0316],\n",
      "         [-0.4952,  0.4302,  0.0994, -0.5288,  0.6262]],\n",
      "\n",
      "        [[-0.6851, -0.4479,  0.6887, -0.0857, -0.8692],\n",
      "         [ 0.8412,  0.8378, -0.8768,  0.8265,  0.8556]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0525 cost = 0.003166\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4142,  0.5103, -0.0173, -0.4407,  0.6077],\n",
      "         [ 0.8799, -0.8351, -0.9095,  0.8498, -0.0682]],\n",
      "\n",
      "        [[ 0.8853,  0.9158, -0.9210,  0.9097,  0.8765],\n",
      "         [-0.7181, -0.4373,  0.7307, -0.0901, -0.8986]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0525 cost = 0.003016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3805,  0.5809,  0.1282, -0.3925,  0.5712],\n",
      "         [ 0.9497, -0.5856, -0.6716,  0.9311, -0.6560]],\n",
      "\n",
      "        [[ 0.9071,  0.9044, -0.9351,  0.9032,  0.8662],\n",
      "         [-0.3951, -0.4456,  0.5273, -0.1168, -0.6920]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0525 cost = 0.004366\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4142,  0.5107, -0.0172, -0.4407,  0.6079],\n",
      "         [ 0.9497, -0.5857, -0.6717,  0.9311, -0.6561]],\n",
      "\n",
      "        [[ 0.8854,  0.9159, -0.9210,  0.9098,  0.8766],\n",
      "         [-0.3951, -0.4457,  0.5273, -0.1168, -0.6920]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0526 cost = 0.005240\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8800, -0.8352, -0.9096,  0.8500, -0.0685],\n",
      "         [ 0.8441, -0.9391, -0.9457,  0.7205,  0.0314]],\n",
      "\n",
      "        [[-0.7183, -0.4375,  0.7309, -0.0905, -0.8986],\n",
      "         [-0.6852, -0.4481,  0.6889, -0.0862, -0.8693]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0526 cost = 0.004491\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4953,  0.4305,  0.0994, -0.5289,  0.6268],\n",
      "         [-0.3806,  0.5810,  0.1282, -0.3923,  0.5714]],\n",
      "\n",
      "        [[ 0.8414,  0.8380, -0.8769,  0.8267,  0.8558],\n",
      "         [ 0.9072,  0.9044, -0.9351,  0.9032,  0.8663]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0526 cost = 0.003891\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4952,  0.4307,  0.0994, -0.5289,  0.6269],\n",
      "         [-0.4142,  0.5112, -0.0171, -0.4406,  0.6081]],\n",
      "\n",
      "        [[ 0.8415,  0.8381, -0.8769,  0.8267,  0.8558],\n",
      "         [ 0.8854,  0.9159, -0.9210,  0.9098,  0.8767]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0527 cost = 0.005386\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9498, -0.5864, -0.6722,  0.9312, -0.6564],\n",
      "         [-0.3802,  0.5813,  0.1282, -0.3919,  0.5716]],\n",
      "\n",
      "        [[-0.3954, -0.4461,  0.5275, -0.1171, -0.6922],\n",
      "         [ 0.9072,  0.9044, -0.9351,  0.9033,  0.8664]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0527 cost = 0.004219\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8445, -0.9392, -0.9459,  0.7211,  0.0311],\n",
      "         [ 0.8802, -0.8353, -0.9098,  0.8501, -0.0690]],\n",
      "\n",
      "        [[-0.6853, -0.4484,  0.6890, -0.0866, -0.8694],\n",
      "         [-0.7185, -0.4379,  0.7311, -0.0910, -0.8987]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0527 cost = 0.002504\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9498, -0.5867, -0.6725,  0.9312, -0.6566],\n",
      "         [-0.4140,  0.5121, -0.0170, -0.4403,  0.6085]],\n",
      "\n",
      "        [[-0.3956, -0.4463,  0.5276, -0.1172, -0.6923],\n",
      "         [ 0.8855,  0.9160, -0.9211,  0.9099,  0.8768]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0528 cost = 0.005283\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3797,  0.5816,  0.1282, -0.3913,  0.5719],\n",
      "         [-0.4948,  0.4314,  0.0994, -0.5285,  0.6274]],\n",
      "\n",
      "        [[ 0.9073,  0.9045, -0.9351,  0.9033,  0.8665],\n",
      "         [ 0.8416,  0.8382, -0.8770,  0.8268,  0.8560]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0528 cost = 0.007063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8447, -0.9392, -0.9459,  0.7214,  0.0310],\n",
      "         [ 0.8804, -0.8354, -0.9099,  0.8503, -0.0693]],\n",
      "\n",
      "        [[-0.6854, -0.4486,  0.6891, -0.0869, -0.8694],\n",
      "         [-0.7186, -0.4382,  0.7312, -0.0914, -0.8988]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0528 cost = 0.002493\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4946,  0.4317,  0.0994, -0.5283,  0.6278],\n",
      "         [ 0.8805, -0.8354, -0.9099,  0.8503, -0.0694]],\n",
      "\n",
      "        [[ 0.8418,  0.8383, -0.8770,  0.8270,  0.8562],\n",
      "         [-0.7187, -0.4383,  0.7313, -0.0915, -0.8988]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0529 cost = 0.003168\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8448, -0.9393, -0.9460,  0.7216,  0.0309],\n",
      "         [-0.4138,  0.5131, -0.0169, -0.4399,  0.6089]],\n",
      "\n",
      "        [[-0.6854, -0.4488,  0.6892, -0.0871, -0.8695],\n",
      "         [ 0.8856,  0.9160, -0.9211,  0.9100,  0.8770]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0529 cost = 0.002589\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3793,  0.5820,  0.1282, -0.3907,  0.5723],\n",
      "         [ 0.9499, -0.5874, -0.6731,  0.9313, -0.6569]],\n",
      "\n",
      "        [[ 0.9074,  0.9045, -0.9352,  0.9034,  0.8667],\n",
      "         [-0.3958, -0.4467,  0.5279, -0.1174, -0.6925]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0529 cost = 0.004291\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8806, -0.8355, -0.9100,  0.8504, -0.0697],\n",
      "         [ 0.8450, -0.9393, -0.9460,  0.7218,  0.0308]],\n",
      "\n",
      "        [[-0.7188, -0.4386,  0.7314, -0.0918, -0.8989],\n",
      "         [-0.6855, -0.4489,  0.6893, -0.0873, -0.8695]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0530 cost = 0.004417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4138,  0.5137, -0.0168, -0.4398,  0.6091],\n",
      "         [-0.4945,  0.4322,  0.0993, -0.5282,  0.6283]],\n",
      "\n",
      "        [[ 0.8857,  0.9161, -0.9211,  0.9101,  0.8771],\n",
      "         [ 0.8419,  0.8385, -0.8772,  0.8271,  0.8564]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0530 cost = 0.007397\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5822,  0.1282, -0.3903,  0.5725],\n",
      "         [ 0.9499, -0.5877, -0.6733,  0.9313, -0.6571]],\n",
      "\n",
      "        [[ 0.9075,  0.9046, -0.9352,  0.9035,  0.8669],\n",
      "         [-0.3960, -0.4470,  0.5280, -0.1176, -0.6926]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0530 cost = 0.004273\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5823,  0.1282, -0.3902,  0.5725],\n",
      "         [-0.4138,  0.5141, -0.0167, -0.4397,  0.6093]],\n",
      "\n",
      "        [[ 0.9075,  0.9046, -0.9352,  0.9035,  0.8669],\n",
      "         [ 0.8857,  0.9161, -0.9212,  0.9102,  0.8772]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0531 cost = 0.003441\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8808, -0.8356, -0.9102,  0.8506, -0.0701],\n",
      "         [ 0.8453, -0.9394, -0.9461,  0.7223,  0.0306]],\n",
      "\n",
      "        [[-0.7190, -0.4390,  0.7316, -0.0923, -0.8990],\n",
      "         [-0.6856, -0.4492,  0.6894, -0.0878, -0.8696]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0531 cost = 0.004389\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4945,  0.4326,  0.0993, -0.5282,  0.6288],\n",
      "         [ 0.9500, -0.5879, -0.6737,  0.9313, -0.6572]],\n",
      "\n",
      "        [[ 0.8422,  0.8387, -0.8773,  0.8273,  0.8566],\n",
      "         [-0.3962, -0.4474,  0.5282, -0.1177, -0.6927]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0531 cost = 0.005378\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5824,  0.1282, -0.3899,  0.5727],\n",
      "         [-0.4946,  0.4327,  0.0993, -0.5282,  0.6290]],\n",
      "\n",
      "        [[ 0.9076,  0.9046, -0.9352,  0.9036,  0.8671],\n",
      "         [ 0.8422,  0.8387, -0.8773,  0.8274,  0.8567]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0532 cost = 0.006961\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8456, -0.9394, -0.9462,  0.7227,  0.0304],\n",
      "         [-0.4138,  0.5150, -0.0166, -0.4395,  0.6097]],\n",
      "\n",
      "        [[-0.6857, -0.4494,  0.6896, -0.0881, -0.8696],\n",
      "         [ 0.8858,  0.9162, -0.9212,  0.9103,  0.8774]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0532 cost = 0.002554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8810, -0.8356, -0.9103,  0.8508, -0.0705],\n",
      "         [ 0.9500, -0.5880, -0.6740,  0.9314, -0.6573]],\n",
      "\n",
      "        [[-0.7192, -0.4395,  0.7318, -0.0928, -0.8991],\n",
      "         [-0.3965, -0.4477,  0.5283, -0.1178, -0.6928]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0532 cost = 0.005139\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8457, -0.9394, -0.9462,  0.7230,  0.0303],\n",
      "         [ 0.9500, -0.5881, -0.6741,  0.9314, -0.6574]],\n",
      "\n",
      "        [[-0.6857, -0.4496,  0.6896, -0.0883, -0.8697],\n",
      "         [-0.3965, -0.4478,  0.5284, -0.1179, -0.6929]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0533 cost = 0.004804\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5826,  0.1282, -0.3895,  0.5730],\n",
      "         [-0.4945,  0.4331,  0.0992, -0.5281,  0.6295]],\n",
      "\n",
      "        [[ 0.9077,  0.9047, -0.9353,  0.9037,  0.8673],\n",
      "         [ 0.8424,  0.8389, -0.8775,  0.8276,  0.8569]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0533 cost = 0.006920\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4137,  0.5159, -0.0165, -0.4394,  0.6101],\n",
      "         [ 0.8812, -0.8357, -0.9105,  0.8509, -0.0708]],\n",
      "\n",
      "        [[ 0.8860,  0.9163, -0.9212,  0.9104,  0.8775],\n",
      "         [-0.7194, -0.4399,  0.7320, -0.0931, -0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0533 cost = 0.002899\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5827,  0.1281, -0.3893,  0.5732],\n",
      "         [ 0.8813, -0.8357, -0.9105,  0.8510, -0.0709]],\n",
      "\n",
      "        [[ 0.9077,  0.9047, -0.9353,  0.9037,  0.8675],\n",
      "         [-0.7194, -0.4400,  0.7320, -0.0932, -0.8992]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0534 cost = 0.002078\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8460, -0.9395, -0.9463,  0.7234,  0.0300],\n",
      "         [-0.4944,  0.4335,  0.0991, -0.5280,  0.6300]],\n",
      "\n",
      "        [[-0.6859, -0.4499,  0.6898, -0.0887, -0.8697],\n",
      "         [ 0.8426,  0.8391, -0.8776,  0.8278,  0.8572]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0534 cost = 0.003037\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9501, -0.5885, -0.6746,  0.9315, -0.6576],\n",
      "         [-0.4138,  0.5165, -0.0164, -0.4393,  0.6103]],\n",
      "\n",
      "        [[-0.3969, -0.4484,  0.5287, -0.1181, -0.6931],\n",
      "         [ 0.8860,  0.9163, -0.9213,  0.9105,  0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0534 cost = 0.005153\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9501, -0.5886, -0.6746,  0.9315, -0.6577],\n",
      "         [-0.3792,  0.5829,  0.1281, -0.3890,  0.5734]],\n",
      "\n",
      "        [[-0.3970, -0.4485,  0.5288, -0.1181, -0.6931],\n",
      "         [ 0.9078,  0.9048, -0.9353,  0.9038,  0.8677]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0535 cost = 0.004101\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4138,  0.5168, -0.0163, -0.4393,  0.6105],\n",
      "         [ 0.8815, -0.8358, -0.9106,  0.8511, -0.0714]],\n",
      "\n",
      "        [[ 0.8861,  0.9164, -0.9213,  0.9105,  0.8778],\n",
      "         [-0.7196, -0.4403,  0.7322, -0.0937, -0.8992]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0535 cost = 0.002877\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8462, -0.9395, -0.9464,  0.7237,  0.0298],\n",
      "         [-0.4944,  0.4339,  0.0991, -0.5281,  0.6305]],\n",
      "\n",
      "        [[-0.6860, -0.4501,  0.6900, -0.0891, -0.8698],\n",
      "         [ 0.8428,  0.8393, -0.8777,  0.8279,  0.8574]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0535 cost = 0.003021\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4140,  0.5171, -0.0162, -0.4393,  0.6106],\n",
      "         [ 0.8463, -0.9396, -0.9464,  0.7238,  0.0298]],\n",
      "\n",
      "        [[ 0.8862,  0.9164, -0.9213,  0.9106,  0.8779],\n",
      "         [-0.6860, -0.4501,  0.6900, -0.0892, -0.8698]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0536 cost = 0.003140\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5830,  0.1281, -0.3889,  0.5736],\n",
      "         [ 0.8816, -0.8360, -0.9107,  0.8513, -0.0718]],\n",
      "\n",
      "        [[ 0.9079,  0.9048, -0.9354,  0.9039,  0.8679],\n",
      "         [-0.7198, -0.4405,  0.7324, -0.0940, -0.8993]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0536 cost = 0.002057\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9502, -0.5894, -0.6751,  0.9315, -0.6581],\n",
      "         [-0.4947,  0.4340,  0.0991, -0.5283,  0.6309]],\n",
      "\n",
      "        [[-0.3973, -0.4489,  0.5290, -0.1184, -0.6933],\n",
      "         [ 0.8430,  0.8394, -0.8778,  0.8280,  0.8575]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0536 cost = 0.005976\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.5830,  0.1282, -0.3888,  0.5737],\n",
      "         [-0.4144,  0.5175, -0.0159, -0.4393,  0.6108]],\n",
      "\n",
      "        [[ 0.9080,  0.9049, -0.9354,  0.9039,  0.8680],\n",
      "         [ 0.8862,  0.9165, -0.9214,  0.9106,  0.8780]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0537 cost = 0.003344\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8817, -0.8361, -0.9108,  0.8514, -0.0721],\n",
      "         [ 0.8465, -0.9396, -0.9465,  0.7242,  0.0296]],\n",
      "\n",
      "        [[-0.7199, -0.4407,  0.7325, -0.0943, -0.8993],\n",
      "         [-0.6862, -0.4502,  0.6902, -0.0896, -0.8698]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0537 cost = 0.004274\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4949,  0.4341,  0.0991, -0.5285,  0.6312],\n",
      "         [ 0.9502, -0.5899, -0.6754,  0.9316, -0.6583]],\n",
      "\n",
      "        [[ 0.8431,  0.8395, -0.8778,  0.8281,  0.8577],\n",
      "         [-0.3975, -0.4491,  0.5292, -0.1186, -0.6934]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0537 cost = 0.005246\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9503, -0.5900, -0.6755,  0.9316, -0.6584],\n",
      "         [ 0.8818, -0.8362, -0.9109,  0.8515, -0.0724]],\n",
      "\n",
      "        [[-0.3976, -0.4492,  0.5292, -0.1187, -0.6935],\n",
      "         [-0.7200, -0.4409,  0.7326, -0.0946, -0.8994]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0538 cost = 0.004921\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8468, -0.9397, -0.9465,  0.7246,  0.0294],\n",
      "         [-0.4950,  0.4342,  0.0992, -0.5286,  0.6313]],\n",
      "\n",
      "        [[-0.6863, -0.4504,  0.6904, -0.0900, -0.8699],\n",
      "         [ 0.8431,  0.8395, -0.8778,  0.8282,  0.8577]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0538 cost = 0.002989\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4146,  0.5181, -0.0156, -0.4394,  0.6111],\n",
      "         [-0.3799,  0.5830,  0.1282, -0.3886,  0.5740]],\n",
      "\n",
      "        [[ 0.8863,  0.9165, -0.9214,  0.9107,  0.8781],\n",
      "         [ 0.9080,  0.9049, -0.9354,  0.9040,  0.8681]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0538 cost = 0.002584\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4951,  0.4342,  0.0992, -0.5287,  0.6315],\n",
      "         [ 0.9503, -0.5906, -0.6759,  0.9317, -0.6587]],\n",
      "\n",
      "        [[ 0.8432,  0.8396, -0.8779,  0.8282,  0.8578],\n",
      "         [-0.3978, -0.4495,  0.5294, -0.1190, -0.6936]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0539 cost = 0.005215\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8470, -0.9398, -0.9466,  0.7250,  0.0292],\n",
      "         [-0.3801,  0.5830,  0.1283, -0.3886,  0.5741]],\n",
      "\n",
      "        [[-0.6864, -0.4506,  0.6905, -0.0903, -0.8699],\n",
      "         [ 0.9081,  0.9049, -0.9354,  0.9040,  0.8682]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0539 cost = 0.001551\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4150,  0.5185, -0.0154, -0.4394,  0.6112],\n",
      "         [ 0.8821, -0.8364, -0.9111,  0.8517, -0.0731]],\n",
      "\n",
      "        [[ 0.8864,  0.9166, -0.9214,  0.9108,  0.8782],\n",
      "         [-0.7203, -0.4413,  0.7329, -0.0952, -0.8995]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0539 cost = 0.002820\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4954,  0.4342,  0.0992, -0.5289,  0.6317],\n",
      "         [ 0.8471, -0.9398, -0.9467,  0.7252,  0.0291]],\n",
      "\n",
      "        [[ 0.8432,  0.8396, -0.8779,  0.8282,  0.8579],\n",
      "         [-0.6865, -0.4507,  0.6906, -0.0905, -0.8699]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0540 cost = 0.003475\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8822, -0.8365, -0.9111,  0.8518, -0.0733],\n",
      "         [ 0.9504, -0.5911, -0.6763,  0.9318, -0.6589]],\n",
      "\n",
      "        [[-0.7204, -0.4415,  0.7330, -0.0954, -0.8995],\n",
      "         [-0.3980, -0.4499,  0.5295, -0.1193, -0.6938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0540 cost = 0.004981\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4154,  0.5188, -0.0151, -0.4394,  0.6113],\n",
      "         [-0.3804,  0.5830,  0.1284, -0.3885,  0.5742]],\n",
      "\n",
      "        [[ 0.8864,  0.9166, -0.9214,  0.9109,  0.8783],\n",
      "         [ 0.9081,  0.9050, -0.9354,  0.9041,  0.8683]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0540 cost = 0.002562\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4156,  0.5189, -0.0150, -0.4394,  0.6113],\n",
      "         [ 0.9504, -0.5913, -0.6765,  0.9318, -0.6591]],\n",
      "\n",
      "        [[ 0.8864,  0.9166, -0.9214,  0.9109,  0.8783],\n",
      "         [-0.3982, -0.4501,  0.5296, -0.1195, -0.6939]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0541 cost = 0.004906\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8474, -0.9399, -0.9467,  0.7257,  0.0289],\n",
      "         [-0.3806,  0.5830,  0.1284, -0.3885,  0.5743]],\n",
      "\n",
      "        [[-0.6867, -0.4510,  0.6908, -0.0909, -0.8700],\n",
      "         [ 0.9081,  0.9050, -0.9354,  0.9041,  0.8684]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0541 cost = 0.001538\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4961,  0.4343,  0.0993, -0.5294,  0.6321],\n",
      "         [ 0.8824, -0.8366, -0.9112,  0.8520, -0.0738]],\n",
      "\n",
      "        [[ 0.8433,  0.8397, -0.8779,  0.8283,  0.8580],\n",
      "         [-0.7206, -0.4418,  0.7332, -0.0959, -0.8996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0541 cost = 0.002989\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4162,  0.5193, -0.0147, -0.4394,  0.6115],\n",
      "         [-0.4963,  0.4343,  0.0993, -0.5295,  0.6321]],\n",
      "\n",
      "        [[ 0.8865,  0.9167, -0.9214,  0.9109,  0.8784],\n",
      "         [ 0.8434,  0.8397, -0.8779,  0.8283,  0.8580]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0542 cost = 0.007114\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8476, -0.9399, -0.9468,  0.7259,  0.0288],\n",
      "         [-0.3810,  0.5831,  0.1285, -0.3885,  0.5744]],\n",
      "\n",
      "        [[-0.6868, -0.4511,  0.6909, -0.0912, -0.8700],\n",
      "         [ 0.9082,  0.9050, -0.9354,  0.9041,  0.8685]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0542 cost = 0.001531\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9504, -0.5916, -0.6768,  0.9318, -0.6593],\n",
      "         [ 0.8824, -0.8366, -0.9112,  0.8520, -0.0739]],\n",
      "\n",
      "        [[-0.3985, -0.4505,  0.5299, -0.1198, -0.6941],\n",
      "         [-0.7207, -0.4421,  0.7333, -0.0961, -0.8996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0542 cost = 0.004828\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4169,  0.5198, -0.0144, -0.4393,  0.6116],\n",
      "         [ 0.9505, -0.5917, -0.6769,  0.9319, -0.6594]],\n",
      "\n",
      "        [[ 0.8866,  0.9167, -0.9215,  0.9110,  0.8785],\n",
      "         [-0.3985, -0.4506,  0.5299, -0.1199, -0.6941]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0543 cost = 0.004862\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8825, -0.8366, -0.9113,  0.8521, -0.0741],\n",
      "         [-0.4970,  0.4344,  0.0993, -0.5299,  0.6325]],\n",
      "\n",
      "        [[-0.7208, -0.4423,  0.7334, -0.0964, -0.8996],\n",
      "         [ 0.8435,  0.8399, -0.8780,  0.8284,  0.8582]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0543 cost = 0.002879\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8478, -0.9400, -0.9468,  0.7262,  0.0287],\n",
      "         [-0.3814,  0.5832,  0.1286, -0.3885,  0.5745]],\n",
      "\n",
      "        [[-0.6869, -0.4513,  0.6911, -0.0915, -0.8701],\n",
      "         [ 0.9083,  0.9050, -0.9355,  0.9042,  0.8686]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0543 cost = 0.001523\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4175,  0.5202, -0.0141, -0.4393,  0.6118],\n",
      "         [ 0.9505, -0.5921, -0.6771,  0.9319, -0.6596]],\n",
      "\n",
      "        [[ 0.8866,  0.9168, -0.9215,  0.9111,  0.8786],\n",
      "         [-0.3987, -0.4509,  0.5301, -0.1201, -0.6943]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0544 cost = 0.004838\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8826, -0.8367, -0.9114,  0.8522, -0.0744],\n",
      "         [-0.3816,  0.5832,  0.1287, -0.3885,  0.5746]],\n",
      "\n",
      "        [[-0.7209, -0.4425,  0.7335, -0.0967, -0.8997],\n",
      "         [ 0.9083,  0.9051, -0.9355,  0.9042,  0.8686]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0544 cost = 0.001541\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4976,  0.4345,  0.0993, -0.5303,  0.6329],\n",
      "         [ 0.8479, -0.9400, -0.9469,  0.7264,  0.0285]],\n",
      "\n",
      "        [[ 0.8436,  0.8399, -0.8780,  0.8285,  0.8583],\n",
      "         [-0.6870, -0.4515,  0.6912, -0.0917, -0.8701]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0544 cost = 0.003403\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9505, -0.5924, -0.6774,  0.9320, -0.6597],\n",
      "         [-0.4978,  0.4345,  0.0994, -0.5304,  0.6330]],\n",
      "\n",
      "        [[-0.3989, -0.4512,  0.5302, -0.1204, -0.6944],\n",
      "         [ 0.8437,  0.8399, -0.8781,  0.8285,  0.8583]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0545 cost = 0.005795\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4183,  0.5206, -0.0137, -0.4394,  0.6119],\n",
      "         [ 0.8480, -0.9400, -0.9469,  0.7266,  0.0285]],\n",
      "\n",
      "        [[ 0.8867,  0.9168, -0.9215,  0.9112,  0.8786],\n",
      "         [-0.6871, -0.4516,  0.6913, -0.0919, -0.8701]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0545 cost = 0.003004\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3821,  0.5832,  0.1288, -0.3885,  0.5747],\n",
      "         [ 0.8828, -0.8369, -0.9115,  0.8524, -0.0748]],\n",
      "\n",
      "        [[ 0.9083,  0.9051, -0.9355,  0.9043,  0.8687],\n",
      "         [-0.7211, -0.4428,  0.7337, -0.0971, -0.8998]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0545 cost = 0.001977\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3822,  0.5832,  0.1288, -0.3885,  0.5747],\n",
      "         [-0.4983,  0.4345,  0.0994, -0.5307,  0.6332]],\n",
      "\n",
      "        [[ 0.9084,  0.9051, -0.9355,  0.9043,  0.8688],\n",
      "         [ 0.8437,  0.8400, -0.8781,  0.8286,  0.8584]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0546 cost = 0.006683\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8482, -0.9401, -0.9470,  0.7269,  0.0283],\n",
      "         [ 0.9505, -0.5929, -0.6777,  0.9320, -0.6600]],\n",
      "\n",
      "        [[-0.6872, -0.4517,  0.6914, -0.0922, -0.8702],\n",
      "         [-0.3991, -0.4515,  0.5304, -0.1207, -0.6946]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0546 cost = 0.004557\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8829, -0.8369, -0.9115,  0.8525, -0.0751],\n",
      "         [-0.4190,  0.5211, -0.0133, -0.4394,  0.6121]],\n",
      "\n",
      "        [[-0.7212, -0.4430,  0.7338, -0.0974, -0.8998],\n",
      "         [ 0.8868,  0.9169, -0.9216,  0.9112,  0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0546 cost = 0.002424\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8483, -0.9401, -0.9470,  0.7271,  0.0283],\n",
      "         [ 0.8829, -0.8370, -0.9115,  0.8525, -0.0752]],\n",
      "\n",
      "        [[-0.6872, -0.4518,  0.6914, -0.0924, -0.8702],\n",
      "         [-0.7213, -0.4430,  0.7339, -0.0976, -0.8998]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0547 cost = 0.002309\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9506, -0.5932, -0.6779,  0.9320, -0.6601],\n",
      "         [-0.3825,  0.5833,  0.1289, -0.3884,  0.5748]],\n",
      "\n",
      "        [[-0.3993, -0.4517,  0.5305, -0.1209, -0.6947],\n",
      "         [ 0.9084,  0.9052, -0.9355,  0.9044,  0.8689]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0547 cost = 0.003902\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4193,  0.5215, -0.0130, -0.4394,  0.6122],\n",
      "         [-0.4988,  0.4347,  0.0994, -0.5310,  0.6337]],\n",
      "\n",
      "        [[ 0.8869,  0.9169, -0.9216,  0.9113,  0.8788],\n",
      "         [ 0.8440,  0.8402, -0.8782,  0.8288,  0.8586]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0547 cost = 0.006992\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3827,  0.5834,  0.1289, -0.3884,  0.5749],\n",
      "         [ 0.8485, -0.9402, -0.9471,  0.7273,  0.0281]],\n",
      "\n",
      "        [[ 0.9085,  0.9052, -0.9355,  0.9044,  0.8690],\n",
      "         [-0.6873, -0.4519,  0.6915, -0.0926, -0.8702]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0548 cost = 0.002233\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4197,  0.5218, -0.0129, -0.4394,  0.6124],\n",
      "         [ 0.9506, -0.5936, -0.6782,  0.9321, -0.6603]],\n",
      "\n",
      "        [[ 0.8869,  0.9170, -0.9216,  0.9114,  0.8789],\n",
      "         [-0.3995, -0.4520,  0.5306, -0.1210, -0.6948]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0548 cost = 0.004749\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4992,  0.4349,  0.0994, -0.5313,  0.6340],\n",
      "         [ 0.8831, -0.8371, -0.9117,  0.8527, -0.0757]],\n",
      "\n",
      "        [[ 0.8441,  0.8403, -0.8783,  0.8289,  0.8588],\n",
      "         [-0.7215, -0.4434,  0.7341, -0.0981, -0.8999]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0548 cost = 0.002900\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3831,  0.5834,  0.1289, -0.3884,  0.5750],\n",
      "         [ 0.8487, -0.9402, -0.9471,  0.7276,  0.0280]],\n",
      "\n",
      "        [[ 0.9086,  0.9052, -0.9356,  0.9045,  0.8692],\n",
      "         [-0.6873, -0.4520,  0.6916, -0.0929, -0.8703]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0549 cost = 0.002224\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9506, -0.5938, -0.6784,  0.9321, -0.6604],\n",
      "         [ 0.8832, -0.8372, -0.9117,  0.8527, -0.0758]],\n",
      "\n",
      "        [[-0.3997, -0.4522,  0.5308, -0.1212, -0.6949],\n",
      "         [-0.7216, -0.4435,  0.7342, -0.0983, -0.8999]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0549 cost = 0.004711\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4997,  0.4350,  0.0994, -0.5315,  0.6343],\n",
      "         [-0.4204,  0.5223, -0.0126, -0.4394,  0.6126]],\n",
      "\n",
      "        [[ 0.8443,  0.8404, -0.8784,  0.8290,  0.8589],\n",
      "         [ 0.8870,  0.9171, -0.9216,  0.9115,  0.8791]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0549 cost = 0.004871\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8833, -0.8372, -0.9118,  0.8528, -0.0760],\n",
      "         [ 0.8489, -0.9402, -0.9472,  0.7279,  0.0279]],\n",
      "\n",
      "        [[-0.7217, -0.4437,  0.7343, -0.0985, -0.9000],\n",
      "         [-0.6874, -0.4522,  0.6917, -0.0931, -0.8703]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0550 cost = 0.004070\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9507, -0.5942, -0.6787,  0.9322, -0.6606],\n",
      "         [-0.3834,  0.5835,  0.1290, -0.3882,  0.5751]],\n",
      "\n",
      "        [[-0.3998, -0.4525,  0.5309, -0.1214, -0.6950],\n",
      "         [ 0.9086,  0.9053, -0.9356,  0.9045,  0.8693]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0550 cost = 0.003859\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4999,  0.4352,  0.0994, -0.5316,  0.6345],\n",
      "         [-0.4205,  0.5229, -0.0125, -0.4393,  0.6127]],\n",
      "\n",
      "        [[ 0.8444,  0.8405, -0.8784,  0.8291,  0.8590],\n",
      "         [ 0.8871,  0.9171, -0.9217,  0.9115,  0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0550 cost = 0.004849\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9507, -0.5945, -0.6789,  0.9322, -0.6607],\n",
      "         [ 0.8835, -0.8373, -0.9119,  0.8530, -0.0763]],\n",
      "\n",
      "        [[-0.4000, -0.4527,  0.5310, -0.1215, -0.6951],\n",
      "         [-0.7218, -0.4440,  0.7344, -0.0989, -0.9000]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0551 cost = 0.004679\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8492, -0.9403, -0.9473,  0.7285,  0.0277],\n",
      "         [-0.4205,  0.5233, -0.0125, -0.4391,  0.6129]],\n",
      "\n",
      "        [[-0.6875, -0.4525,  0.6918, -0.0935, -0.8704],\n",
      "         [ 0.8871,  0.9171, -0.9217,  0.9116,  0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0551 cost = 0.002355\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3833,  0.5837,  0.1290, -0.3878,  0.5753],\n",
      "         [-0.4999,  0.4356,  0.0994, -0.5315,  0.6348]],\n",
      "\n",
      "        [[ 0.9087,  0.9053, -0.9356,  0.9046,  0.8695],\n",
      "         [ 0.8445,  0.8406, -0.8785,  0.8292,  0.8591]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0551 cost = 0.006568\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9508, -0.5950, -0.6793,  0.9322, -0.6609],\n",
      "         [-0.4205,  0.5238, -0.0124, -0.4390,  0.6131]],\n",
      "\n",
      "        [[-0.4001, -0.4529,  0.5311, -0.1217, -0.6952],\n",
      "         [ 0.8872,  0.9172, -0.9217,  0.9116,  0.8793]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0552 cost = 0.004809\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3832,  0.5839,  0.1290, -0.3875,  0.5754],\n",
      "         [-0.4998,  0.4359,  0.0994, -0.5314,  0.6351]],\n",
      "\n",
      "        [[ 0.9088,  0.9053, -0.9357,  0.9046,  0.8696],\n",
      "         [ 0.8446,  0.8407, -0.8786,  0.8293,  0.8592]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0552 cost = 0.006549\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8494, -0.9404, -0.9474,  0.7288,  0.0275],\n",
      "         [ 0.8837, -0.8374, -0.9121,  0.8531, -0.0768]],\n",
      "\n",
      "        [[-0.6875, -0.4527,  0.6919, -0.0938, -0.8704],\n",
      "         [-0.7220, -0.4444,  0.7346, -0.0993, -0.9001]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0552 cost = 0.002256\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4997,  0.4362,  0.0993, -0.5312,  0.6354],\n",
      "         [-0.4205,  0.5245, -0.0124, -0.4388,  0.6134]],\n",
      "\n",
      "        [[ 0.8448,  0.8408, -0.8787,  0.8294,  0.8594],\n",
      "         [ 0.8873,  0.9172, -0.9218,  0.9117,  0.8795]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0553 cost = 0.004786\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9508, -0.5954, -0.6798,  0.9322, -0.6611],\n",
      "         [-0.3830,  0.5842,  0.1289, -0.3870,  0.5756]],\n",
      "\n",
      "        [[-0.4003, -0.4532,  0.5313, -0.1217, -0.6952],\n",
      "         [ 0.9089,  0.9054, -0.9357,  0.9047,  0.8698]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0553 cost = 0.003815\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8838, -0.8374, -0.9122,  0.8532, -0.0770],\n",
      "         [ 0.8496, -0.9404, -0.9474,  0.7289,  0.0274]],\n",
      "\n",
      "        [[-0.7221, -0.4445,  0.7347, -0.0996, -0.9002],\n",
      "         [-0.6875, -0.4528,  0.6920, -0.0940, -0.8704]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0553 cost = 0.004011\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8838, -0.8374, -0.9122,  0.8532, -0.0771],\n",
      "         [-0.3828,  0.5845,  0.1289, -0.3866,  0.5758]],\n",
      "\n",
      "        [[-0.7222, -0.4446,  0.7348, -0.0997, -0.9002],\n",
      "         [ 0.9089,  0.9055, -0.9357,  0.9048,  0.8699]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0554 cost = 0.001480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8497, -0.9404, -0.9474,  0.7292,  0.0273],\n",
      "         [ 0.9508, -0.5958, -0.6801,  0.9322, -0.6612]],\n",
      "\n",
      "        [[-0.6876, -0.4529,  0.6920, -0.0942, -0.8705],\n",
      "         [-0.4005, -0.4534,  0.5314, -0.1218, -0.6953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0554 cost = 0.004429\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4993,  0.4371,  0.0992, -0.5307,  0.6361],\n",
      "         [-0.4202,  0.5258, -0.0125, -0.4383,  0.6139]],\n",
      "\n",
      "        [[ 0.8451,  0.8411, -0.8789,  0.8297,  0.8598],\n",
      "         [ 0.8875,  0.9173, -0.9218,  0.9119,  0.8797]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0554 cost = 0.004738\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9508, -0.5961, -0.6804,  0.9323, -0.6614],\n",
      "         [ 0.8499, -0.9404, -0.9475,  0.7294,  0.0272]],\n",
      "\n",
      "        [[-0.4006, -0.4536,  0.5315, -0.1219, -0.6954],\n",
      "         [-0.6876, -0.4530,  0.6921, -0.0944, -0.8705]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0555 cost = 0.005694\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8841, -0.8375, -0.9124,  0.8534, -0.0776],\n",
      "         [-0.4199,  0.5264, -0.0125, -0.4380,  0.6141]],\n",
      "\n",
      "        [[-0.7223, -0.4450,  0.7350, -0.1001, -0.9003],\n",
      "         [ 0.8875,  0.9174, -0.9219,  0.9119,  0.8798]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0555 cost = 0.002336\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3823,  0.5849,  0.1289, -0.3858,  0.5761],\n",
      "         [-0.4990,  0.4376,  0.0992, -0.5304,  0.6365]],\n",
      "\n",
      "        [[ 0.9090,  0.9055, -0.9358,  0.9049,  0.8702],\n",
      "         [ 0.8452,  0.8413, -0.8790,  0.8298,  0.8599]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0555 cost = 0.006452\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3822,  0.5850,  0.1288, -0.3856,  0.5762],\n",
      "         [ 0.8842, -0.8376, -0.9125,  0.8535, -0.0778]],\n",
      "\n",
      "        [[ 0.9091,  0.9055, -0.9358,  0.9049,  0.8702],\n",
      "         [-0.7224, -0.4452,  0.7351, -0.1003, -0.9003]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0556 cost = 0.001893\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4988,  0.4380,  0.0992, -0.5302,  0.6368],\n",
      "         [ 0.8502, -0.9405, -0.9476,  0.7299,  0.0270]],\n",
      "\n",
      "        [[ 0.8453,  0.8414, -0.8790,  0.8299,  0.8601],\n",
      "         [-0.6877, -0.4533,  0.6922, -0.0948, -0.8706]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0556 cost = 0.003220\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9509, -0.5967, -0.6810,  0.9323, -0.6616],\n",
      "         [-0.4195,  0.5273, -0.0126, -0.4376,  0.6144]],\n",
      "\n",
      "        [[-0.4009, -0.4540,  0.5317, -0.1220, -0.6956],\n",
      "         [ 0.8876,  0.9174, -0.9219,  0.9120,  0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0556 cost = 0.004728\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8843, -0.8376, -0.9126,  0.8536, -0.0781],\n",
      "         [-0.4195,  0.5276, -0.0126, -0.4376,  0.6145]],\n",
      "\n",
      "        [[-0.7226, -0.4455,  0.7352, -0.1006, -0.9004],\n",
      "         [ 0.8877,  0.9174, -0.9219,  0.9120,  0.8800]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0557 cost = 0.002320\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8504, -0.9405, -0.9476,  0.7302,  0.0268],\n",
      "         [-0.3819,  0.5853,  0.1288, -0.3850,  0.5764]],\n",
      "\n",
      "        [[-0.6877, -0.4534,  0.6923, -0.0950, -0.8706],\n",
      "         [ 0.9092,  0.9056, -0.9358,  0.9050,  0.8705]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0557 cost = 0.001438\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9509, -0.5971, -0.6813,  0.9323, -0.6618],\n",
      "         [-0.4987,  0.4385,  0.0991, -0.5299,  0.6372]],\n",
      "\n",
      "        [[-0.4010, -0.4543,  0.5319, -0.1221, -0.6956],\n",
      "         [ 0.8456,  0.8416, -0.8792,  0.8301,  0.8603]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0557 cost = 0.005534\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8505, -0.9406, -0.9477,  0.7305,  0.0267],\n",
      "         [-0.3818,  0.5854,  0.1288, -0.3848,  0.5765]],\n",
      "\n",
      "        [[-0.6877, -0.4535,  0.6923, -0.0952, -0.8706],\n",
      "         [ 0.9092,  0.9056, -0.9359,  0.9051,  0.8706]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0558 cost = 0.001434\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8845, -0.8377, -0.9127,  0.8538, -0.0786],\n",
      "         [ 0.9510, -0.5975, -0.6815,  0.9323, -0.6619]],\n",
      "\n",
      "        [[-0.7227, -0.4457,  0.7353, -0.1010, -0.9005],\n",
      "         [-0.4012, -0.4544,  0.5319, -0.1222, -0.6957]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0558 cost = 0.004663\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4194,  0.5284, -0.0125, -0.4373,  0.6148],\n",
      "         [-0.4986,  0.4387,  0.0991, -0.5299,  0.6375]],\n",
      "\n",
      "        [[ 0.8878,  0.9175, -0.9220,  0.9122,  0.8802],\n",
      "         [ 0.8457,  0.8417, -0.8793,  0.8302,  0.8604]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0558 cost = 0.006708\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3818,  0.5856,  0.1288, -0.3845,  0.5766],\n",
      "         [ 0.9510, -0.5978, -0.6817,  0.9324, -0.6621]],\n",
      "\n",
      "        [[ 0.9093,  0.9057, -0.9359,  0.9051,  0.8707],\n",
      "         [-0.4013, -0.4545,  0.5320, -0.1223, -0.6957]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0559 cost = 0.003812\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4195,  0.5287, -0.0124, -0.4372,  0.6150],\n",
      "         [-0.4987,  0.4389,  0.0991, -0.5299,  0.6377]],\n",
      "\n",
      "        [[ 0.8878,  0.9176, -0.9220,  0.9122,  0.8803],\n",
      "         [ 0.8458,  0.8418, -0.8793,  0.8303,  0.8605]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0559 cost = 0.006690\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8508, -0.9406, -0.9478,  0.7309,  0.0265],\n",
      "         [ 0.8846, -0.8379, -0.9129,  0.8539, -0.0790]],\n",
      "\n",
      "        [[-0.6878, -0.4537,  0.6925, -0.0956, -0.8707],\n",
      "         [-0.7229, -0.4459,  0.7355, -0.1013, -0.9005]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0559 cost = 0.002195\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9510, -0.5981, -0.6820,  0.9324, -0.6622],\n",
      "         [ 0.8508, -0.9406, -0.9478,  0.7309,  0.0265]],\n",
      "\n",
      "        [[-0.4015, -0.4548,  0.5322, -0.1224, -0.6958],\n",
      "         [-0.6879, -0.4537,  0.6925, -0.0957, -0.8707]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0560 cost = 0.005589\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8847, -0.8380, -0.9129,  0.8540, -0.0792],\n",
      "         [-0.3819,  0.5858,  0.1288, -0.3843,  0.5769]],\n",
      "\n",
      "        [[-0.7230, -0.4461,  0.7356, -0.1015, -0.9006],\n",
      "         [ 0.9094,  0.9058, -0.9359,  0.9052,  0.8709]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0560 cost = 0.001440\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4989,  0.4393,  0.0991, -0.5300,  0.6382],\n",
      "         [-0.4199,  0.5294, -0.0122, -0.4371,  0.6153]],\n",
      "\n",
      "        [[ 0.8460,  0.8420, -0.8795,  0.8305,  0.8608],\n",
      "         [ 0.8880,  0.9176, -0.9221,  0.9123,  0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0560 cost = 0.004595\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9511, -0.5985, -0.6823,  0.9324, -0.6624],\n",
      "         [ 0.8510, -0.9407, -0.9478,  0.7312,  0.0263]],\n",
      "\n",
      "        [[-0.4017, -0.4550,  0.5323, -0.1225, -0.6959],\n",
      "         [-0.6879, -0.4539,  0.6926, -0.0959, -0.8707]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0561 cost = 0.005567\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3818,  0.5860,  0.1288, -0.3841,  0.5770],\n",
      "         [ 0.8848, -0.8380, -0.9130,  0.8541, -0.0796]],\n",
      "\n",
      "        [[ 0.9095,  0.9058, -0.9359,  0.9053,  0.8711],\n",
      "         [-0.7231, -0.4464,  0.7357, -0.1018, -0.9006]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0561 cost = 0.001851\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4198,  0.5300, -0.0122, -0.4369,  0.6155],\n",
      "         [-0.4988,  0.4397,  0.0990, -0.5299,  0.6385]],\n",
      "\n",
      "        [[ 0.8880,  0.9177, -0.9221,  0.9124,  0.8806],\n",
      "         [ 0.8462,  0.8421, -0.8795,  0.8306,  0.8609]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0561 cost = 0.006627\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4989,  0.4398,  0.0990, -0.5299,  0.6387],\n",
      "         [ 0.9511, -0.5989, -0.6827,  0.9324, -0.6626]],\n",
      "\n",
      "        [[ 0.8462,  0.8422, -0.8796,  0.8307,  0.8610],\n",
      "         [-0.4018, -0.4553,  0.5325, -0.1226, -0.6960]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0562 cost = 0.004747\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3818,  0.5861,  0.1288, -0.3838,  0.5772],\n",
      "         [-0.4199,  0.5304, -0.0122, -0.4368,  0.6157]],\n",
      "\n",
      "        [[ 0.9095,  0.9058, -0.9360,  0.9053,  0.8712],\n",
      "         [ 0.8881,  0.9177, -0.9221,  0.9124,  0.8807]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0562 cost = 0.003002\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8514, -0.9408, -0.9480,  0.7317,  0.0261],\n",
      "         [ 0.8850, -0.8381, -0.9132,  0.8542, -0.0799]],\n",
      "\n",
      "        [[-0.6880, -0.4542,  0.6927, -0.0964, -0.8708],\n",
      "         [-0.7233, -0.4467,  0.7359, -0.1022, -0.9007]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0562 cost = 0.002169\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3819,  0.5862,  0.1288, -0.3836,  0.5773],\n",
      "         [ 0.8514, -0.9408, -0.9480,  0.7318,  0.0260]],\n",
      "\n",
      "        [[ 0.9096,  0.9059, -0.9360,  0.9054,  0.8714],\n",
      "         [-0.6880, -0.4543,  0.6928, -0.0965, -0.8708]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0563 cost = 0.002087\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8851, -0.8381, -0.9133,  0.8542, -0.0800],\n",
      "         [-0.4201,  0.5311, -0.0121, -0.4367,  0.6159]],\n",
      "\n",
      "        [[-0.7234, -0.4469,  0.7360, -0.1024, -0.9007],\n",
      "         [ 0.8882,  0.9178, -0.9222,  0.9125,  0.8808]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0563 cost = 0.002258\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9511, -0.5992, -0.6831,  0.9325, -0.6627],\n",
      "         [-0.4992,  0.4403,  0.0989, -0.5299,  0.6393]],\n",
      "\n",
      "        [[-0.4021, -0.4557,  0.5327, -0.1228, -0.6962],\n",
      "         [ 0.8465,  0.8424, -0.8798,  0.8310,  0.8613]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0563 cost = 0.005415\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8851, -0.8381, -0.9133,  0.8543, -0.0802],\n",
      "         [-0.4993,  0.4403,  0.0989, -0.5300,  0.6394]],\n",
      "\n",
      "        [[-0.7234, -0.4470,  0.7361, -0.1026, -0.9008],\n",
      "         [ 0.8466,  0.8425, -0.8798,  0.8310,  0.8614]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0564 cost = 0.002643\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9511, -0.5994, -0.6833,  0.9325, -0.6628],\n",
      "         [-0.3821,  0.5863,  0.1288, -0.3834,  0.5775]],\n",
      "\n",
      "        [[-0.4022, -0.4559,  0.5328, -0.1229, -0.6962],\n",
      "         [ 0.9097,  0.9059, -0.9360,  0.9055,  0.8716]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0564 cost = 0.003668\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8517, -0.9409, -0.9481,  0.7323,  0.0258],\n",
      "         [-0.4203,  0.5317, -0.0120, -0.4366,  0.6162]],\n",
      "\n",
      "        [[-0.6881, -0.4545,  0.6929, -0.0969, -0.8708],\n",
      "         [ 0.8883,  0.9179, -0.9222,  0.9126,  0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0564 cost = 0.002226\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8853, -0.8383, -0.9134,  0.8544, -0.0806],\n",
      "         [ 0.8518, -0.9409, -0.9481,  0.7324,  0.0258]],\n",
      "\n",
      "        [[-0.7236, -0.4472,  0.7362, -0.1029, -0.9008],\n",
      "         [-0.6881, -0.4545,  0.6929, -0.0970, -0.8708]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0565 cost = 0.003828\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9512, -0.5999, -0.6836,  0.9325, -0.6631],\n",
      "         [-0.4204,  0.5319, -0.0119, -0.4366,  0.6163]],\n",
      "\n",
      "        [[-0.4024, -0.4561,  0.5329, -0.1231, -0.6963],\n",
      "         [ 0.8883,  0.9179, -0.9222,  0.9126,  0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0565 cost = 0.004579\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3823,  0.5863,  0.1289, -0.3832,  0.5776],\n",
      "         [-0.4995,  0.4405,  0.0990, -0.5302,  0.6398]],\n",
      "\n",
      "        [[ 0.9097,  0.9060, -0.9360,  0.9055,  0.8717],\n",
      "         [ 0.8468,  0.8426, -0.8799,  0.8311,  0.8616]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0565 cost = 0.006211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3823,  0.5864,  0.1289, -0.3832,  0.5777],\n",
      "         [ 0.8520, -0.9409, -0.9481,  0.7327,  0.0255]],\n",
      "\n",
      "        [[ 0.9098,  0.9060, -0.9361,  0.9055,  0.8718],\n",
      "         [-0.6882, -0.4546,  0.6931, -0.0973, -0.8709]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0566 cost = 0.002059\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8854, -0.8385, -0.9135,  0.8546, -0.0811],\n",
      "         [-0.4205,  0.5323, -0.0118, -0.4365,  0.6164]],\n",
      "\n",
      "        [[-0.7238, -0.4474,  0.7364, -0.1034, -0.9009],\n",
      "         [ 0.8884,  0.9179, -0.9222,  0.9127,  0.8811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0566 cost = 0.002230\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9512, -0.6005, -0.6840,  0.9326, -0.6633],\n",
      "         [-0.4996,  0.4407,  0.0990, -0.5302,  0.6402]],\n",
      "\n",
      "        [[-0.4026, -0.4564,  0.5331, -0.1233, -0.6964],\n",
      "         [ 0.8469,  0.8428, -0.8800,  0.8313,  0.8618]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0566 cost = 0.005357\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4206,  0.5325, -0.0117, -0.4365,  0.6165],\n",
      "         [ 0.9512, -0.6007, -0.6841,  0.9326, -0.6634]],\n",
      "\n",
      "        [[ 0.8884,  0.9180, -0.9223,  0.9128,  0.8812],\n",
      "         [-0.4026, -0.4564,  0.5331, -0.1234, -0.6964]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0567 cost = 0.004410\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8522, -0.9410, -0.9482,  0.7332,  0.0253],\n",
      "         [ 0.8856, -0.8386, -0.9136,  0.8547, -0.0815]],\n",
      "\n",
      "        [[-0.6883, -0.4548,  0.6932, -0.0977, -0.8709],\n",
      "         [-0.7239, -0.4476,  0.7365, -0.1037, -0.9009]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0567 cost = 0.002129\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3825,  0.5865,  0.1289, -0.3829,  0.5779],\n",
      "         [-0.4997,  0.4408,  0.0990, -0.5303,  0.6404]],\n",
      "\n",
      "        [[ 0.9099,  0.9061, -0.9361,  0.9056,  0.8720],\n",
      "         [ 0.8470,  0.8429, -0.8800,  0.8313,  0.8619]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0567 cost = 0.006171\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4208,  0.5329, -0.0115, -0.4365,  0.6167],\n",
      "         [ 0.8524, -0.9410, -0.9483,  0.7334,  0.0252]],\n",
      "\n",
      "        [[ 0.8885,  0.9180, -0.9223,  0.9128,  0.8813],\n",
      "         [-0.6883, -0.4548,  0.6932, -0.0979, -0.8709]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0568 cost = 0.002704\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4998,  0.4410,  0.0990, -0.5304,  0.6407],\n",
      "         [-0.3825,  0.5865,  0.1289, -0.3828,  0.5780]],\n",
      "\n",
      "        [[ 0.8471,  0.8430, -0.8801,  0.8314,  0.8620],\n",
      "         [ 0.9099,  0.9061, -0.9361,  0.9057,  0.8721]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0568 cost = 0.003169\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8857, -0.8387, -0.9137,  0.8548, -0.0819],\n",
      "         [ 0.9513, -0.6014, -0.6845,  0.9326, -0.6637]],\n",
      "\n",
      "        [[-0.7241, -0.4478,  0.7367, -0.1041, -0.9010],\n",
      "         [-0.4029, -0.4567,  0.5333, -0.1236, -0.6966]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0568 cost = 0.004499\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8525, -0.9411, -0.9483,  0.7336,  0.0251],\n",
      "         [ 0.9513, -0.6015, -0.6846,  0.9326, -0.6637]],\n",
      "\n",
      "        [[-0.6884, -0.4549,  0.6933, -0.0981, -0.8710],\n",
      "         [-0.4030, -0.4568,  0.5334, -0.1236, -0.6966]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0569 cost = 0.004210\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4996,  0.4415,  0.0989, -0.5302,  0.6412],\n",
      "         [-0.4209,  0.5336, -0.0112, -0.4362,  0.6171]],\n",
      "\n",
      "        [[ 0.8473,  0.8431, -0.8802,  0.8316,  0.8622],\n",
      "         [ 0.8886,  0.9181, -0.9223,  0.9129,  0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0569 cost = 0.004411\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8858, -0.8388, -0.9138,  0.8549, -0.0822],\n",
      "         [-0.3820,  0.5870,  0.1289, -0.3822,  0.5784]],\n",
      "\n",
      "        [[-0.7242, -0.4481,  0.7368, -0.1044, -0.9011],\n",
      "         [ 0.9100,  0.9062, -0.9362,  0.9058,  0.8723]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0569 cost = 0.001385\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8527, -0.9411, -0.9484,  0.7339,  0.0249],\n",
      "         [ 0.9513, -0.6018, -0.6849,  0.9327, -0.6639]],\n",
      "\n",
      "        [[-0.6885, -0.4551,  0.6934, -0.0984, -0.8710],\n",
      "         [-0.4032, -0.4571,  0.5336, -0.1238, -0.6967]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0570 cost = 0.004194\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4208,  0.5343, -0.0112, -0.4359,  0.6173],\n",
      "         [ 0.8859, -0.8389, -0.9139,  0.8550, -0.0824]],\n",
      "\n",
      "        [[ 0.8887,  0.9181, -0.9224,  0.9130,  0.8816],\n",
      "         [-0.7243, -0.4483,  0.7369, -0.1046, -0.9011]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0570 cost = 0.002455\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4993,  0.4421,  0.0989, -0.5298,  0.6417],\n",
      "         [-0.3816,  0.5873,  0.1289, -0.3817,  0.5786]],\n",
      "\n",
      "        [[ 0.8475,  0.8433, -0.8803,  0.8317,  0.8624],\n",
      "         [ 0.9101,  0.9062, -0.9362,  0.9058,  0.8724]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0570 cost = 0.003126\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3814,  0.5875,  0.1289, -0.3815,  0.5787],\n",
      "         [ 0.9514, -0.6021, -0.6851,  0.9327, -0.6640]],\n",
      "\n",
      "        [[ 0.9101,  0.9062, -0.9362,  0.9059,  0.8725],\n",
      "         [-0.4034, -0.4574,  0.5337, -0.1239, -0.6969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0571 cost = 0.003647\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8860, -0.8389, -0.9139,  0.8551, -0.0826],\n",
      "         [-0.4991,  0.4424,  0.0989, -0.5295,  0.6420]],\n",
      "\n",
      "        [[-0.7244, -0.4485,  0.7371, -0.1049, -0.9012],\n",
      "         [ 0.8476,  0.8434, -0.8804,  0.8318,  0.8624]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0571 cost = 0.002564\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8529, -0.9412, -0.9484,  0.7343,  0.0247],\n",
      "         [-0.4207,  0.5351, -0.0110, -0.4354,  0.6177]],\n",
      "\n",
      "        [[-0.6886, -0.4554,  0.6936, -0.0988, -0.8711],\n",
      "         [ 0.8888,  0.9182, -0.9224,  0.9131,  0.8817]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0571 cost = 0.002162\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4207,  0.5353, -0.0109, -0.4353,  0.6178],\n",
      "         [-0.3808,  0.5879,  0.1289, -0.3810,  0.5790]],\n",
      "\n",
      "        [[ 0.8888,  0.9182, -0.9224,  0.9131,  0.8817],\n",
      "         [ 0.9102,  0.9062, -0.9362,  0.9059,  0.8726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0572 cost = 0.002202\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8530, -0.9412, -0.9485,  0.7344,  0.0246],\n",
      "         [-0.4989,  0.4428,  0.0989, -0.5293,  0.6423]],\n",
      "\n",
      "        [[-0.6886, -0.4555,  0.6937, -0.0990, -0.8712],\n",
      "         [ 0.8477,  0.8435, -0.8805,  0.8320,  0.8626]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0572 cost = 0.002606\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9514, -0.6024, -0.6853,  0.9327, -0.6643],\n",
      "         [ 0.8861, -0.8390, -0.9140,  0.8552, -0.0829]],\n",
      "\n",
      "        [[-0.4037, -0.4578,  0.5340, -0.1241, -0.6971],\n",
      "         [-0.7246, -0.4488,  0.7372, -0.1054, -0.9012]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0572 cost = 0.004334\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3804,  0.5882,  0.1289, -0.3806,  0.5793],\n",
      "         [ 0.8531, -0.9412, -0.9485,  0.7345,  0.0246]],\n",
      "\n",
      "        [[ 0.9102,  0.9063, -0.9362,  0.9060,  0.8727],\n",
      "         [-0.6887, -0.4556,  0.6937, -0.0991, -0.8712]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0573 cost = 0.002000\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4209,  0.5359, -0.0106, -0.4349,  0.6181],\n",
      "         [-0.4988,  0.4432,  0.0989, -0.5292,  0.6427]],\n",
      "\n",
      "        [[ 0.8889,  0.9183, -0.9224,  0.9132,  0.8818],\n",
      "         [ 0.8478,  0.8436, -0.8805,  0.8321,  0.8627]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0573 cost = 0.006357\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8862, -0.8390, -0.9140,  0.8553, -0.0831],\n",
      "         [ 0.9514, -0.6027, -0.6855,  0.9327, -0.6644]],\n",
      "\n",
      "        [[-0.7247, -0.4489,  0.7373, -0.1057, -0.9013],\n",
      "         [-0.4039, -0.4580,  0.5341, -0.1242, -0.6971]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0573 cost = 0.004422\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8532, -0.9413, -0.9485,  0.7348,  0.0244],\n",
      "         [ 0.9514, -0.6028, -0.6856,  0.9327, -0.6645]],\n",
      "\n",
      "        [[-0.6888, -0.4557,  0.6938, -0.0994, -0.8712],\n",
      "         [-0.4040, -0.4580,  0.5341, -0.1242, -0.6972]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0574 cost = 0.004138\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3802,  0.5885,  0.1290, -0.3802,  0.5796],\n",
      "         [ 0.8862, -0.8391, -0.9141,  0.8553, -0.0833]],\n",
      "\n",
      "        [[ 0.9103,  0.9063, -0.9363,  0.9060,  0.8728],\n",
      "         [-0.7248, -0.4491,  0.7374, -0.1059, -0.9013]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0574 cost = 0.001757\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4211,  0.5365, -0.0103, -0.4347,  0.6184],\n",
      "         [-0.4988,  0.4435,  0.0989, -0.5292,  0.6432]],\n",
      "\n",
      "        [[ 0.8890,  0.9183, -0.9225,  0.9133,  0.8819],\n",
      "         [ 0.8480,  0.8437, -0.8806,  0.8322,  0.8629]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0574 cost = 0.006328\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8534, -0.9413, -0.9486,  0.7350,  0.0243],\n",
      "         [ 0.9514, -0.6032, -0.6858,  0.9328, -0.6646]],\n",
      "\n",
      "        [[-0.6889, -0.4559,  0.6940, -0.0997, -0.8712],\n",
      "         [-0.4042, -0.4583,  0.5343, -0.1244, -0.6973]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0575 cost = 0.004123\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4213,  0.5368, -0.0102, -0.4346,  0.6185],\n",
      "         [-0.4989,  0.4437,  0.0989, -0.5292,  0.6435]],\n",
      "\n",
      "        [[ 0.8890,  0.9184, -0.9225,  0.9133,  0.8820],\n",
      "         [ 0.8481,  0.8438, -0.8807,  0.8323,  0.8630]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0575 cost = 0.006312\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3802,  0.5888,  0.1290, -0.3800,  0.5799],\n",
      "         [ 0.8863, -0.8392, -0.9142,  0.8554, -0.0837]],\n",
      "\n",
      "        [[ 0.9104,  0.9064, -0.9363,  0.9061,  0.8730],\n",
      "         [-0.7250, -0.4494,  0.7376, -0.1063, -0.9014]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0575 cost = 0.001748\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3802,  0.5889,  0.1290, -0.3800,  0.5800],\n",
      "         [ 0.9514, -0.6033, -0.6859,  0.9328, -0.6647]],\n",
      "\n",
      "        [[ 0.9104,  0.9064, -0.9363,  0.9061,  0.8731],\n",
      "         [-0.4044, -0.4586,  0.5345, -0.1244, -0.6974]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0576 cost = 0.003584\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4992,  0.4440,  0.0988, -0.5293,  0.6439],\n",
      "         [ 0.8535, -0.9413, -0.9486,  0.7352,  0.0242]],\n",
      "\n",
      "        [[ 0.8483,  0.8440, -0.8808,  0.8325,  0.8632],\n",
      "         [-0.6890, -0.4561,  0.6941, -0.1000, -0.8713]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0576 cost = 0.002940\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8864, -0.8392, -0.9142,  0.8555, -0.0839],\n",
      "         [-0.4219,  0.5375, -0.0099, -0.4344,  0.6188]],\n",
      "\n",
      "        [[-0.7251, -0.4497,  0.7378, -0.1066, -0.9014],\n",
      "         [ 0.8891,  0.9184, -0.9225,  0.9134,  0.8822]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0576 cost = 0.002133\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9514, -0.6035, -0.6861,  0.9328, -0.6649],\n",
      "         [ 0.8865, -0.8393, -0.9142,  0.8555, -0.0840]],\n",
      "\n",
      "        [[-0.4046, -0.4589,  0.5347, -0.1246, -0.6975],\n",
      "         [-0.7252, -0.4498,  0.7378, -0.1067, -0.9015]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0577 cost = 0.004276\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4995,  0.4442,  0.0988, -0.5294,  0.6442],\n",
      "         [-0.4221,  0.5378, -0.0098, -0.4344,  0.6189]],\n",
      "\n",
      "        [[ 0.8484,  0.8441, -0.8809,  0.8326,  0.8633],\n",
      "         [ 0.8892,  0.9185, -0.9225,  0.9134,  0.8823]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0577 cost = 0.004242\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8537, -0.9414, -0.9487,  0.7354,  0.0240],\n",
      "         [-0.3805,  0.5891,  0.1290, -0.3798,  0.5802]],\n",
      "\n",
      "        [[-0.6890, -0.4563,  0.6942, -0.1003, -0.8713],\n",
      "         [ 0.9105,  0.9065, -0.9363,  0.9062,  0.8733]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0577 cost = 0.001323\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8537, -0.9414, -0.9487,  0.7355,  0.0240],\n",
      "         [-0.4996,  0.4444,  0.0988, -0.5294,  0.6444]],\n",
      "\n",
      "        [[-0.6891, -0.4563,  0.6943, -0.1004, -0.8713],\n",
      "         [ 0.8485,  0.8442, -0.8809,  0.8327,  0.8634]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0578 cost = 0.002546\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3804,  0.5892,  0.1290, -0.3796,  0.5803],\n",
      "         [ 0.8866, -0.8393, -0.9143,  0.8556, -0.0843]],\n",
      "\n",
      "        [[ 0.9105,  0.9065, -0.9364,  0.9063,  0.8734],\n",
      "         [-0.7254, -0.4500,  0.7380, -0.1071, -0.9015]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0578 cost = 0.001729\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9515, -0.6038, -0.6864,  0.9328, -0.6651],\n",
      "         [-0.4224,  0.5385, -0.0096, -0.4341,  0.6192]],\n",
      "\n",
      "        [[-0.4049, -0.4593,  0.5349, -0.1247, -0.6977],\n",
      "         [ 0.8893,  0.9186, -0.9226,  0.9135,  0.8824]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0578 cost = 0.004369\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8866, -0.8394, -0.9144,  0.8557, -0.0845],\n",
      "         [ 0.9515, -0.6040, -0.6865,  0.9328, -0.6651]],\n",
      "\n",
      "        [[-0.7254, -0.4502,  0.7381, -0.1073, -0.9016],\n",
      "         [-0.4050, -0.4593,  0.5349, -0.1248, -0.6977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0579 cost = 0.004344\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8539, -0.9414, -0.9487,  0.7358,  0.0238],\n",
      "         [-0.3804,  0.5893,  0.1290, -0.3794,  0.5804]],\n",
      "\n",
      "        [[-0.6892, -0.4564,  0.6944, -0.1007, -0.8713],\n",
      "         [ 0.9106,  0.9066, -0.9364,  0.9063,  0.8735]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0579 cost = 0.001314\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4225,  0.5388, -0.0095, -0.4340,  0.6193],\n",
      "         [-0.4998,  0.4446,  0.0988, -0.5295,  0.6448]],\n",
      "\n",
      "        [[ 0.8893,  0.9186, -0.9226,  0.9136,  0.8825],\n",
      "         [ 0.8487,  0.8444, -0.8810,  0.8329,  0.8636]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0579 cost = 0.006211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4999,  0.4447,  0.0988, -0.5295,  0.6449],\n",
      "         [-0.4227,  0.5390, -0.0094, -0.4340,  0.6194]],\n",
      "\n",
      "        [[ 0.8488,  0.8445, -0.8811,  0.8329,  0.8637],\n",
      "         [ 0.8893,  0.9186, -0.9226,  0.9136,  0.8825]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0580 cost = 0.004191\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8868, -0.8395, -0.9145,  0.8558, -0.0849],\n",
      "         [ 0.8540, -0.9414, -0.9488,  0.7360,  0.0237]],\n",
      "\n",
      "        [[-0.7256, -0.4504,  0.7383, -0.1077, -0.9016],\n",
      "         [-0.6893, -0.4565,  0.6945, -0.1009, -0.8714]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0580 cost = 0.003622\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9515, -0.6044, -0.6869,  0.9329, -0.6654],\n",
      "         [-0.3804,  0.5895,  0.1291, -0.3791,  0.5806]],\n",
      "\n",
      "        [[-0.4053, -0.4597,  0.5352, -0.1250, -0.6979],\n",
      "         [ 0.9107,  0.9066, -0.9364,  0.9064,  0.8737]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0580 cost = 0.003471\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8541, -0.9415, -0.9488,  0.7362,  0.0236],\n",
      "         [-0.4228,  0.5396, -0.0093, -0.4337,  0.6196]],\n",
      "\n",
      "        [[-0.6893, -0.4566,  0.6946, -0.1011, -0.8714],\n",
      "         [ 0.8894,  0.9187, -0.9226,  0.9137,  0.8827]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0581 cost = 0.002079\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8869, -0.8395, -0.9145,  0.8559, -0.0852],\n",
      "         [-0.3803,  0.5896,  0.1291, -0.3789,  0.5807]],\n",
      "\n",
      "        [[-0.7257, -0.4506,  0.7384, -0.1080, -0.9017],\n",
      "         [ 0.9107,  0.9066, -0.9364,  0.9064,  0.8738]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0581 cost = 0.001322\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5000,  0.4452,  0.0988, -0.5294,  0.6455],\n",
      "         [ 0.9515, -0.6047, -0.6871,  0.9329, -0.6656]],\n",
      "\n",
      "        [[ 0.8490,  0.8447, -0.8812,  0.8331,  0.8640],\n",
      "         [-0.4054, -0.4599,  0.5353, -0.1251, -0.6980]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0581 cost = 0.004404\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3803,  0.5897,  0.1291, -0.3788,  0.5808],\n",
      "         [ 0.8870, -0.8396, -0.9146,  0.8560, -0.0854]],\n",
      "\n",
      "        [[ 0.9107,  0.9067, -0.9364,  0.9065,  0.8739],\n",
      "         [-0.7258, -0.4508,  0.7385, -0.1082, -0.9017]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0582 cost = 0.001705\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9515, -0.6049, -0.6873,  0.9329, -0.6657],\n",
      "         [-0.4229,  0.5401, -0.0092, -0.4335,  0.6198]],\n",
      "\n",
      "        [[-0.4055, -0.4601,  0.5354, -0.1252, -0.6981],\n",
      "         [ 0.8895,  0.9187, -0.9227,  0.9138,  0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0582 cost = 0.004314\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5001,  0.4453,  0.0988, -0.5294,  0.6456],\n",
      "         [ 0.8544, -0.9415, -0.9489,  0.7366,  0.0234]],\n",
      "\n",
      "        [[ 0.8491,  0.8448, -0.8813,  0.8332,  0.8641],\n",
      "         [-0.6894, -0.4569,  0.6948, -0.1015, -0.8714]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0582 cost = 0.002860\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5002,  0.4453,  0.0988, -0.5295,  0.6457],\n",
      "         [ 0.8545, -0.9415, -0.9489,  0.7367,  0.0233]],\n",
      "\n",
      "        [[ 0.8491,  0.8448, -0.8813,  0.8332,  0.8641],\n",
      "         [-0.6895, -0.4569,  0.6948, -0.1016, -0.8715]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0583 cost = 0.002855\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8871, -0.8397, -0.9147,  0.8561, -0.0859],\n",
      "         [-0.4231,  0.5405, -0.0090, -0.4334,  0.6199]],\n",
      "\n",
      "        [[-0.7260, -0.4511,  0.7387, -0.1086, -0.9018],\n",
      "         [ 0.8895,  0.9188, -0.9227,  0.9138,  0.8829]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0583 cost = 0.002076\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3805,  0.5898,  0.1292, -0.3785,  0.5808],\n",
      "         [ 0.9516, -0.6053, -0.6876,  0.9330, -0.6659]],\n",
      "\n",
      "        [[ 0.9108,  0.9067, -0.9364,  0.9065,  0.8740],\n",
      "         [-0.4058, -0.4604,  0.5356, -0.1254, -0.6982]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0583 cost = 0.003488\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8547, -0.9416, -0.9490,  0.7371,  0.0232],\n",
      "         [ 0.8872, -0.8398, -0.9148,  0.8562, -0.0861]],\n",
      "\n",
      "        [[-0.6896, -0.4571,  0.6949, -0.1018, -0.8715],\n",
      "         [-0.7261, -0.4512,  0.7387, -0.1088, -0.9018]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0584 cost = 0.002001\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9516, -0.6056, -0.6878,  0.9330, -0.6660],\n",
      "         [-0.5006,  0.4453,  0.0988, -0.5296,  0.6458]],\n",
      "\n",
      "        [[-0.4059, -0.4606,  0.5356, -0.1256, -0.6983],\n",
      "         [ 0.8492,  0.8448, -0.8813,  0.8333,  0.8642]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0584 cost = 0.005043\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4234,  0.5408, -0.0088, -0.4333,  0.6200],\n",
      "         [-0.3807,  0.5897,  0.1292, -0.3783,  0.5808]],\n",
      "\n",
      "        [[ 0.8896,  0.9188, -0.9227,  0.9139,  0.8829],\n",
      "         [ 0.9108,  0.9067, -0.9364,  0.9065,  0.8741]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0584 cost = 0.002074\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5008,  0.4453,  0.0989, -0.5296,  0.6458],\n",
      "         [-0.4235,  0.5409, -0.0087, -0.4332,  0.6200]],\n",
      "\n",
      "        [[ 0.8493,  0.8449, -0.8813,  0.8333,  0.8643],\n",
      "         [ 0.8896,  0.9188, -0.9227,  0.9139,  0.8830]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0585 cost = 0.004105\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8550, -0.9417, -0.9490,  0.7375,  0.0230],\n",
      "         [ 0.8874, -0.8399, -0.9148,  0.8564, -0.0865]],\n",
      "\n",
      "        [[-0.6897, -0.4573,  0.6950, -0.1022, -0.8715],\n",
      "         [-0.7262, -0.4515,  0.7389, -0.1092, -0.9019]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0585 cost = 0.001990\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3807,  0.5898,  0.1293, -0.3781,  0.5809],\n",
      "         [ 0.9517, -0.6060, -0.6881,  0.9331, -0.6663]],\n",
      "\n",
      "        [[ 0.9109,  0.9067, -0.9365,  0.9066,  0.8742],\n",
      "         [-0.4061, -0.4609,  0.5358, -0.1259, -0.6984]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0585 cost = 0.003460\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9517, -0.6061, -0.6882,  0.9331, -0.6663],\n",
      "         [ 0.8874, -0.8399, -0.9149,  0.8564, -0.0867]],\n",
      "\n",
      "        [[-0.4061, -0.4610,  0.5358, -0.1259, -0.6984],\n",
      "         [-0.7263, -0.4516,  0.7390, -0.1094, -0.9019]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0586 cost = 0.004147\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5010,  0.4455,  0.0989, -0.5296,  0.6461],\n",
      "         [-0.4238,  0.5415, -0.0085, -0.4330,  0.6202]],\n",
      "\n",
      "        [[ 0.8494,  0.8450, -0.8814,  0.8334,  0.8644],\n",
      "         [ 0.8897,  0.9189, -0.9227,  0.9140,  0.8831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0586 cost = 0.004082\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3807,  0.5899,  0.1293, -0.3778,  0.5810],\n",
      "         [ 0.8552, -0.9417, -0.9491,  0.7379,  0.0228]],\n",
      "\n",
      "        [[ 0.9109,  0.9067, -0.9365,  0.9066,  0.8742],\n",
      "         [-0.6898, -0.4575,  0.6952, -0.1025, -0.8716]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0586 cost = 0.001894\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5010,  0.4456,  0.0989, -0.5295,  0.6462],\n",
      "         [-0.4238,  0.5419, -0.0085, -0.4328,  0.6203]],\n",
      "\n",
      "        [[ 0.8494,  0.8450, -0.8814,  0.8334,  0.8644],\n",
      "         [ 0.8897,  0.9189, -0.9228,  0.9140,  0.8831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0587 cost = 0.004069\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3805,  0.5900,  0.1293, -0.3775,  0.5811],\n",
      "         [ 0.8553, -0.9417, -0.9491,  0.7381,  0.0227]],\n",
      "\n",
      "        [[ 0.9109,  0.9068, -0.9365,  0.9066,  0.8743],\n",
      "         [-0.6898, -0.4576,  0.6952, -0.1026, -0.8716]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0587 cost = 0.001889\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9517, -0.6066, -0.6887,  0.9331, -0.6666],\n",
      "         [ 0.8877, -0.8400, -0.9150,  0.8566, -0.0871]],\n",
      "\n",
      "        [[-0.4064, -0.4614,  0.5360, -0.1262, -0.6986],\n",
      "         [-0.7265, -0.4520,  0.7392, -0.1099, -0.9020]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0587 cost = 0.004121\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3804,  0.5902,  0.1293, -0.3772,  0.5811],\n",
      "         [ 0.9517, -0.6067, -0.6888,  0.9332, -0.6666]],\n",
      "\n",
      "        [[ 0.9110,  0.9068, -0.9365,  0.9067,  0.8744],\n",
      "         [-0.4064, -0.4614,  0.5360, -0.1263, -0.6987]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0588 cost = 0.003428\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5009,  0.4460,  0.0989, -0.5292,  0.6465],\n",
      "         [-0.4236,  0.5427, -0.0085, -0.4323,  0.6206]],\n",
      "\n",
      "        [[ 0.8496,  0.8451, -0.8815,  0.8335,  0.8646],\n",
      "         [ 0.8898,  0.9189, -0.9228,  0.9141,  0.8832]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0588 cost = 0.004042\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8556, -0.9418, -0.9492,  0.7386,  0.0226],\n",
      "         [ 0.8878, -0.8400, -0.9151,  0.8567, -0.0874]],\n",
      "\n",
      "        [[-0.6899, -0.4578,  0.6953, -0.1029, -0.8717],\n",
      "         [-0.7266, -0.4523,  0.7393, -0.1102, -0.9020]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0588 cost = 0.001965\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3802,  0.5903,  0.1293, -0.3767,  0.5813],\n",
      "         [ 0.8557, -0.9418, -0.9492,  0.7387,  0.0225]],\n",
      "\n",
      "        [[ 0.9110,  0.9068, -0.9365,  0.9067,  0.8745],\n",
      "         [-0.6899, -0.4579,  0.6953, -0.1030, -0.8717]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0589 cost = 0.001875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4235,  0.5434, -0.0085, -0.4320,  0.6208],\n",
      "         [ 0.9518, -0.6072, -0.6893,  0.9332, -0.6668]],\n",
      "\n",
      "        [[ 0.8898,  0.9190, -0.9228,  0.9142,  0.8833],\n",
      "         [-0.4066, -0.4618,  0.5362, -0.1266, -0.6988]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0589 cost = 0.004034\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8879, -0.8400, -0.9152,  0.8568, -0.0876],\n",
      "         [-0.5008,  0.4464,  0.0989, -0.5289,  0.6467]],\n",
      "\n",
      "        [[-0.7267, -0.4525,  0.7394, -0.1105, -0.9021],\n",
      "         [ 0.8497,  0.8452, -0.8815,  0.8336,  0.8647]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0589 cost = 0.002388\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8880, -0.8400, -0.9152,  0.8569, -0.0877],\n",
      "         [ 0.8560, -0.9418, -0.9493,  0.7391,  0.0224]],\n",
      "\n",
      "        [[-0.7268, -0.4526,  0.7394, -0.1106, -0.9021],\n",
      "         [-0.6899, -0.4581,  0.6954, -0.1033, -0.8717]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0590 cost = 0.003492\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.5905,  0.1294, -0.3762,  0.5814],\n",
      "         [-0.5008,  0.4465,  0.0990, -0.5289,  0.6468]],\n",
      "\n",
      "        [[ 0.9111,  0.9068, -0.9365,  0.9068,  0.8746],\n",
      "         [ 0.8497,  0.8452, -0.8815,  0.8337,  0.8647]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0590 cost = 0.005757\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9518, -0.6076, -0.6897,  0.9333, -0.6670],\n",
      "         [-0.4233,  0.5441, -0.0084, -0.4317,  0.6210]],\n",
      "\n",
      "        [[-0.4068, -0.4621,  0.5363, -0.1268, -0.6990],\n",
      "         [ 0.8899,  0.9190, -0.9228,  0.9142,  0.8833]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0590 cost = 0.004183\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9519, -0.6077, -0.6898,  0.9333, -0.6671],\n",
      "         [-0.3800,  0.5906,  0.1294, -0.3760,  0.5815]],\n",
      "\n",
      "        [[-0.4069, -0.4622,  0.5364, -0.1268, -0.6990],\n",
      "         [ 0.9111,  0.9069, -0.9366,  0.9068,  0.8747]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0591 cost = 0.003339\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8882, -0.8401, -0.9154,  0.8571, -0.0881],\n",
      "         [-0.4233,  0.5445, -0.0084, -0.4316,  0.6212]],\n",
      "\n",
      "        [[-0.7269, -0.4530,  0.7396, -0.1111, -0.9022],\n",
      "         [ 0.8900,  0.9190, -0.9229,  0.9143,  0.8834]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0591 cost = 0.002014\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5007,  0.4469,  0.0990, -0.5287,  0.6472],\n",
      "         [ 0.8563, -0.9419, -0.9494,  0.7397,  0.0221]],\n",
      "\n",
      "        [[ 0.8499,  0.8454, -0.8816,  0.8338,  0.8649],\n",
      "         [-0.6900, -0.4584,  0.6956, -0.1038, -0.8718]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0591 cost = 0.002746\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9519, -0.6082, -0.6901,  0.9334, -0.6673],\n",
      "         [-0.3800,  0.5907,  0.1294, -0.3757,  0.5816]],\n",
      "\n",
      "        [[-0.4070, -0.4625,  0.5365, -0.1271, -0.6991],\n",
      "         [ 0.9112,  0.9069, -0.9366,  0.9068,  0.8748]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0592 cost = 0.003325\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8883, -0.8402, -0.9155,  0.8572, -0.0885],\n",
      "         [-0.5007,  0.4470,  0.0990, -0.5286,  0.6473]],\n",
      "\n",
      "        [[-0.7271, -0.4532,  0.7397, -0.1114, -0.9022],\n",
      "         [ 0.8499,  0.8454, -0.8816,  0.8338,  0.8649]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0592 cost = 0.002364\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8566, -0.9420, -0.9495,  0.7401,  0.0219],\n",
      "         [-0.4233,  0.5450, -0.0083, -0.4315,  0.6214]],\n",
      "\n",
      "        [[-0.6901, -0.4585,  0.6956, -0.1040, -0.8719],\n",
      "         [ 0.8900,  0.9191, -0.9229,  0.9143,  0.8835]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0592 cost = 0.001989\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8884, -0.8403, -0.9155,  0.8573, -0.0888],\n",
      "         [ 0.8566, -0.9420, -0.9495,  0.7402,  0.0219]],\n",
      "\n",
      "        [[-0.7271, -0.4533,  0.7398, -0.1116, -0.9023],\n",
      "         [-0.6901, -0.4586,  0.6957, -0.1041, -0.8719]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0593 cost = 0.003448\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3800,  0.5908,  0.1294, -0.3754,  0.5817],\n",
      "         [-0.5008,  0.4471,  0.0990, -0.5286,  0.6475]],\n",
      "\n",
      "        [[ 0.9112,  0.9069, -0.9366,  0.9069,  0.8749],\n",
      "         [ 0.8500,  0.8455, -0.8817,  0.8339,  0.8650]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0593 cost = 0.005713\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9520, -0.6090, -0.6906,  0.9334, -0.6676],\n",
      "         [-0.4233,  0.5453, -0.0082, -0.4313,  0.6215]],\n",
      "\n",
      "        [[-0.4073, -0.4628,  0.5367, -0.1274, -0.6993],\n",
      "         [ 0.8901,  0.9191, -0.9229,  0.9144,  0.8836]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0593 cost = 0.004135\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9520, -0.6091, -0.6907,  0.9335, -0.6677],\n",
      "         [-0.4233,  0.5455, -0.0081, -0.4313,  0.6215]],\n",
      "\n",
      "        [[-0.4073, -0.4629,  0.5367, -0.1274, -0.6993],\n",
      "         [ 0.8901,  0.9191, -0.9229,  0.9144,  0.8836]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0594 cost = 0.004130\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5007,  0.4473,  0.0990, -0.5286,  0.6477],\n",
      "         [-0.3800,  0.5909,  0.1294, -0.3751,  0.5818]],\n",
      "\n",
      "        [[ 0.8501,  0.8456, -0.8817,  0.8340,  0.8651],\n",
      "         [ 0.9113,  0.9069, -0.9366,  0.9069,  0.8750]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0594 cost = 0.002788\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8886, -0.8406, -0.9157,  0.8575, -0.0894],\n",
      "         [ 0.8570, -0.9421, -0.9496,  0.7408,  0.0215]],\n",
      "\n",
      "        [[-0.7274, -0.4536,  0.7401, -0.1121, -0.9023],\n",
      "         [-0.6902, -0.4588,  0.6958, -0.1046, -0.8720]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0594 cost = 0.003423\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4232,  0.5459, -0.0080, -0.4311,  0.6217],\n",
      "         [ 0.8571, -0.9421, -0.9496,  0.7409,  0.0215]],\n",
      "\n",
      "        [[ 0.8902,  0.9191, -0.9229,  0.9145,  0.8837],\n",
      "         [-0.6902, -0.4588,  0.6958, -0.1047, -0.8720]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0595 cost = 0.002400\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9520, -0.6098, -0.6911,  0.9335, -0.6679],\n",
      "         [-0.3795,  0.5912,  0.1294, -0.3746,  0.5820]],\n",
      "\n",
      "        [[-0.4075, -0.4632,  0.5369, -0.1277, -0.6994],\n",
      "         [ 0.9113,  0.9070, -0.9366,  0.9070,  0.8751]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0595 cost = 0.003282\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8888, -0.8407, -0.9158,  0.8576, -0.0898],\n",
      "         [-0.5004,  0.4478,  0.0990, -0.5283,  0.6481]],\n",
      "\n",
      "        [[-0.7275, -0.4538,  0.7402, -0.1124, -0.9024],\n",
      "         [ 0.8502,  0.8457, -0.8818,  0.8341,  0.8653]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0595 cost = 0.002334\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8573, -0.9422, -0.9497,  0.7414,  0.0212],\n",
      "         [-0.4231,  0.5463, -0.0079, -0.4309,  0.6219]],\n",
      "\n",
      "        [[-0.6903, -0.4590,  0.6959, -0.1050, -0.8720],\n",
      "         [ 0.8902,  0.9192, -0.9229,  0.9145,  0.8838]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0596 cost = 0.001962\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9521, -0.6104, -0.6913,  0.9336, -0.6682],\n",
      "         [ 0.8889, -0.8408, -0.9158,  0.8577, -0.0901]],\n",
      "\n",
      "        [[-0.4077, -0.4634,  0.5370, -0.1279, -0.6995],\n",
      "         [-0.7276, -0.4540,  0.7403, -0.1127, -0.9024]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0596 cost = 0.003983\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5914,  0.1295, -0.3742,  0.5822],\n",
      "         [-0.5003,  0.4481,  0.0990, -0.5282,  0.6484]],\n",
      "\n",
      "        [[ 0.9114,  0.9070, -0.9366,  0.9070,  0.8752],\n",
      "         [ 0.8503,  0.8457, -0.8818,  0.8341,  0.8654]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0596 cost = 0.005661\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9521, -0.6108, -0.6915,  0.9336, -0.6683],\n",
      "         [ 0.8889, -0.8409, -0.9159,  0.8578, -0.0904]],\n",
      "\n",
      "        [[-0.4078, -0.4636,  0.5370, -0.1280, -0.6996],\n",
      "         [-0.7277, -0.4541,  0.7404, -0.1129, -0.9025]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0597 cost = 0.003971\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5916,  0.1295, -0.3740,  0.5824],\n",
      "         [-0.4231,  0.5468, -0.0077, -0.4307,  0.6221]],\n",
      "\n",
      "        [[ 0.9114,  0.9070, -0.9367,  0.9071,  0.8753],\n",
      "         [ 0.8903,  0.9192, -0.9230,  0.9146,  0.8839]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0597 cost = 0.002610\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5001,  0.4484,  0.0990, -0.5280,  0.6487],\n",
      "         [ 0.8576, -0.9423, -0.9498,  0.7419,  0.0209]],\n",
      "\n",
      "        [[ 0.8504,  0.8459, -0.8819,  0.8342,  0.8655],\n",
      "         [-0.6904, -0.4592,  0.6961, -0.1055, -0.8721]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0597 cost = 0.002677\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9521, -0.6114, -0.6919,  0.9336, -0.6685],\n",
      "         [-0.3790,  0.5917,  0.1294, -0.3738,  0.5825]],\n",
      "\n",
      "        [[-0.4079, -0.4638,  0.5371, -0.1282, -0.6996],\n",
      "         [ 0.9114,  0.9071, -0.9367,  0.9071,  0.8754]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0598 cost = 0.003245\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5001,  0.4486,  0.0990, -0.5280,  0.6489],\n",
      "         [-0.4231,  0.5473, -0.0076, -0.4306,  0.6223]],\n",
      "\n",
      "        [[ 0.8505,  0.8459, -0.8819,  0.8342,  0.8656],\n",
      "         [ 0.8903,  0.9193, -0.9230,  0.9146,  0.8840]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0598 cost = 0.003882\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8578, -0.9423, -0.9499,  0.7422,  0.0208],\n",
      "         [ 0.8892, -0.8411, -0.9161,  0.8580, -0.0910]],\n",
      "\n",
      "        [[-0.6905, -0.4593,  0.6962, -0.1057, -0.8721],\n",
      "         [-0.7279, -0.4544,  0.7406, -0.1134, -0.9026]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0598 cost = 0.001887\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9522, -0.6119, -0.6922,  0.9337, -0.6687],\n",
      "         [-0.4230,  0.5476, -0.0076, -0.4304,  0.6225]],\n",
      "\n",
      "        [[-0.4081, -0.4640,  0.5372, -0.1284, -0.6997],\n",
      "         [ 0.8904,  0.9193, -0.9230,  0.9147,  0.8840]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0599 cost = 0.004049\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8892, -0.8412, -0.9162,  0.8581, -0.0913],\n",
      "         [-0.3788,  0.5920,  0.1294, -0.3733,  0.5827]],\n",
      "\n",
      "        [[-0.7279, -0.4545,  0.7406, -0.1135, -0.9026],\n",
      "         [ 0.9115,  0.9071, -0.9367,  0.9072,  0.8755]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0599 cost = 0.001230\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8580, -0.9424, -0.9499,  0.7425,  0.0206],\n",
      "         [-0.4999,  0.4491,  0.0990, -0.5277,  0.6492]],\n",
      "\n",
      "        [[-0.6905, -0.4594,  0.6962, -0.1059, -0.8722],\n",
      "         [ 0.8506,  0.8460, -0.8820,  0.8343,  0.8657]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0599 cost = 0.002352\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.5921,  0.1294, -0.3731,  0.5828],\n",
      "         [-0.4230,  0.5481, -0.0076, -0.4302,  0.6227]],\n",
      "\n",
      "        [[ 0.9116,  0.9071, -0.9367,  0.9072,  0.8756],\n",
      "         [ 0.8904,  0.9193, -0.9230,  0.9147,  0.8841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0600 cost = 0.002583\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8894, -0.8413, -0.9162,  0.8582, -0.0916],\n",
      "         [ 0.9522, -0.6126, -0.6927,  0.9337, -0.6690]],\n",
      "\n",
      "        [[-0.7280, -0.4546,  0.7407, -0.1138, -0.9026],\n",
      "         [-0.4082, -0.4642,  0.5374, -0.1286, -0.6998]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0600 cost = 0.004016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8581, -0.9424, -0.9500,  0.7428,  0.0205],\n",
      "         [-0.4998,  0.4494,  0.0990, -0.5276,  0.6495]],\n",
      "\n",
      "        [[-0.6906, -0.4595,  0.6963, -0.1061, -0.8722],\n",
      "         [ 0.8507,  0.8461, -0.8821,  0.8344,  0.8658]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0600 cost = 0.002344\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9523, -0.6129, -0.6928,  0.9338, -0.6691],\n",
      "         [-0.3786,  0.5922,  0.1295, -0.3728,  0.5829]],\n",
      "\n",
      "        [[-0.4083, -0.4643,  0.5374, -0.1287, -0.6999],\n",
      "         [ 0.9116,  0.9072, -0.9367,  0.9072,  0.8757]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0601 cost = 0.003207\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8582, -0.9424, -0.9500,  0.7430,  0.0203],\n",
      "         [-0.4998,  0.4495,  0.0991, -0.5276,  0.6496]],\n",
      "\n",
      "        [[-0.6907, -0.4595,  0.6964, -0.1063, -0.8722],\n",
      "         [ 0.8508,  0.8462, -0.8821,  0.8344,  0.8659]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0601 cost = 0.002339\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8895, -0.8415, -0.9164,  0.8583, -0.0921],\n",
      "         [-0.4229,  0.5487, -0.0074, -0.4300,  0.6230]],\n",
      "\n",
      "        [[-0.7282, -0.4548,  0.7409, -0.1142, -0.9027],\n",
      "         [ 0.8905,  0.9194, -0.9231,  0.9148,  0.8842]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0601 cost = 0.001935\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8583, -0.9425, -0.9500,  0.7432,  0.0202],\n",
      "         [-0.4997,  0.4495,  0.0991, -0.5276,  0.6497]],\n",
      "\n",
      "        [[-0.6907, -0.4595,  0.6965, -0.1065, -0.8722],\n",
      "         [ 0.8508,  0.8462, -0.8821,  0.8345,  0.8659]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0602 cost = 0.002333\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9523, -0.6136, -0.6932,  0.9338, -0.6694],\n",
      "         [-0.4230,  0.5489, -0.0073, -0.4299,  0.6230]],\n",
      "\n",
      "        [[-0.4085, -0.4645,  0.5376, -0.1289, -0.7000],\n",
      "         [ 0.8905,  0.9194, -0.9231,  0.9148,  0.8843]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0602 cost = 0.003999\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8896, -0.8416, -0.9164,  0.8584, -0.0925],\n",
      "         [-0.3785,  0.5923,  0.1295, -0.3725,  0.5832]],\n",
      "\n",
      "        [[-0.7283, -0.4548,  0.7410, -0.1145, -0.9027],\n",
      "         [ 0.9117,  0.9072, -0.9367,  0.9073,  0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0602 cost = 0.001215\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8585, -0.9425, -0.9501,  0.7434,  0.0201],\n",
      "         [-0.4998,  0.4496,  0.0991, -0.5276,  0.6500]],\n",
      "\n",
      "        [[-0.6909, -0.4595,  0.6966, -0.1068, -0.8722],\n",
      "         [ 0.8509,  0.8463, -0.8821,  0.8345,  0.8660]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0603 cost = 0.002325\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.5923,  0.1296, -0.3724,  0.5832],\n",
      "         [ 0.8897, -0.8417, -0.9165,  0.8585, -0.0928]],\n",
      "\n",
      "        [[ 0.9117,  0.9073, -0.9367,  0.9073,  0.8759],\n",
      "         [-0.7284, -0.4548,  0.7411, -0.1147, -0.9027]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0603 cost = 0.001568\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9524, -0.6142, -0.6935,  0.9339, -0.6696],\n",
      "         [-0.4231,  0.5491, -0.0071, -0.4298,  0.6232]],\n",
      "\n",
      "        [[-0.4087, -0.4647,  0.5378, -0.1292, -0.7001],\n",
      "         [ 0.8906,  0.9195, -0.9231,  0.9149,  0.8844]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0603 cost = 0.003980\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8586, -0.9426, -0.9501,  0.7437,  0.0199],\n",
      "         [-0.4998,  0.4496,  0.0992, -0.5277,  0.6502]],\n",
      "\n",
      "        [[-0.6910, -0.4594,  0.6968, -0.1070, -0.8722],\n",
      "         [ 0.8509,  0.8464, -0.8821,  0.8346,  0.8661]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0604 cost = 0.002317\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.5923,  0.1296, -0.3723,  0.5834],\n",
      "         [ 0.8898, -0.8419, -0.9165,  0.8586, -0.0932]],\n",
      "\n",
      "        [[ 0.9117,  0.9073, -0.9367,  0.9073,  0.8760],\n",
      "         [-0.7286, -0.4549,  0.7413, -0.1150, -0.9028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0604 cost = 0.001563\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4233,  0.5492, -0.0069, -0.4298,  0.6232],\n",
      "         [ 0.9524, -0.6148, -0.6937,  0.9339, -0.6699]],\n",
      "\n",
      "        [[ 0.8906,  0.9195, -0.9231,  0.9149,  0.8844],\n",
      "         [-0.4089, -0.4648,  0.5379, -0.1294, -0.7001]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0604 cost = 0.003802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4234,  0.5493, -0.0068, -0.4298,  0.6233],\n",
      "         [-0.4999,  0.4495,  0.0993, -0.5278,  0.6503]],\n",
      "\n",
      "        [[ 0.8906,  0.9195, -0.9231,  0.9149,  0.8844],\n",
      "         [ 0.8510,  0.8465, -0.8821,  0.8346,  0.8662]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0605 cost = 0.005809\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8588, -0.9426, -0.9502,  0.7439,  0.0197],\n",
      "         [ 0.8899, -0.8420, -0.9166,  0.8587, -0.0936]],\n",
      "\n",
      "        [[-0.6912, -0.4594,  0.6970, -0.1074, -0.8723],\n",
      "         [-0.7287, -0.4549,  0.7414, -0.1153, -0.9028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0605 cost = 0.001845\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9524, -0.6151, -0.6938,  0.9340, -0.6700],\n",
      "         [-0.3786,  0.5922,  0.1297, -0.3723,  0.5835]],\n",
      "\n",
      "        [[-0.4090, -0.4649,  0.5380, -0.1296, -0.7002],\n",
      "         [ 0.9118,  0.9073, -0.9368,  0.9074,  0.8761]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0605 cost = 0.003154\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9524, -0.6153, -0.6939,  0.9340, -0.6701],\n",
      "         [-0.3787,  0.5922,  0.1297, -0.3723,  0.5836]],\n",
      "\n",
      "        [[-0.4091, -0.4650,  0.5380, -0.1296, -0.7002],\n",
      "         [ 0.9118,  0.9074, -0.9368,  0.9074,  0.8762]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0606 cost = 0.003150\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4238,  0.5496, -0.0065, -0.4297,  0.6234],\n",
      "         [ 0.8589, -0.9427, -0.9502,  0.7441,  0.0196]],\n",
      "\n",
      "        [[ 0.8907,  0.9196, -0.9231,  0.9150,  0.8846],\n",
      "         [-0.6913, -0.4594,  0.6971, -0.1076, -0.8723]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0606 cost = 0.002296\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5003,  0.4496,  0.0993, -0.5281,  0.6508],\n",
      "         [ 0.8900, -0.8422, -0.9167,  0.8589, -0.0941]],\n",
      "\n",
      "        [[ 0.8511,  0.8466, -0.8822,  0.8347,  0.8663],\n",
      "         [-0.7288, -0.4550,  0.7415, -0.1157, -0.9028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0606 cost = 0.002271\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8900, -0.8422, -0.9167,  0.8589, -0.0942],\n",
      "         [ 0.8590, -0.9427, -0.9502,  0.7443,  0.0195]],\n",
      "\n",
      "        [[-0.7289, -0.4550,  0.7416, -0.1158, -0.9029],\n",
      "         [-0.6914, -0.4594,  0.6972, -0.1078, -0.8723]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0607 cost = 0.003273\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4242,  0.5498, -0.0063, -0.4297,  0.6235],\n",
      "         [-0.3790,  0.5923,  0.1298, -0.3723,  0.5837]],\n",
      "\n",
      "        [[ 0.8907,  0.9196, -0.9231,  0.9150,  0.8846],\n",
      "         [ 0.9118,  0.9074, -0.9368,  0.9074,  0.8763]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0607 cost = 0.001897\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9525, -0.6161, -0.6943,  0.9341, -0.6704],\n",
      "         [-0.5006,  0.4496,  0.0993, -0.5283,  0.6509]],\n",
      "\n",
      "        [[-0.4093, -0.4653,  0.5382, -0.1300, -0.7004],\n",
      "         [ 0.8512,  0.8467, -0.8822,  0.8347,  0.8664]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0607 cost = 0.004648\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4244,  0.5499, -0.0061, -0.4297,  0.6236],\n",
      "         [ 0.8901, -0.8424, -0.9168,  0.8590, -0.0946]],\n",
      "\n",
      "        [[ 0.8907,  0.9197, -0.9231,  0.9151,  0.8847],\n",
      "         [-0.7290, -0.4552,  0.7417, -0.1161, -0.9029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0608 cost = 0.002102\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5923,  0.1298, -0.3723,  0.5838],\n",
      "         [-0.5008,  0.4497,  0.0993, -0.5285,  0.6511]],\n",
      "\n",
      "        [[ 0.9118,  0.9074, -0.9368,  0.9074,  0.8763],\n",
      "         [ 0.8512,  0.8467, -0.8822,  0.8347,  0.8664]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0608 cost = 0.005508\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8592, -0.9428, -0.9503,  0.7447,  0.0192],\n",
      "         [ 0.9525, -0.6165, -0.6945,  0.9341, -0.6706]],\n",
      "\n",
      "        [[-0.6915, -0.4596,  0.6974, -0.1082, -0.8723],\n",
      "         [-0.4095, -0.4655,  0.5383, -0.1302, -0.7004]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0608 cost = 0.003659\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8593, -0.9428, -0.9503,  0.7448,  0.0191],\n",
      "         [-0.5010,  0.4498,  0.0993, -0.5286,  0.6513]],\n",
      "\n",
      "        [[-0.6916, -0.4596,  0.6974, -0.1083, -0.8723],\n",
      "         [ 0.8513,  0.8468, -0.8823,  0.8348,  0.8665]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0609 cost = 0.002277\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9525, -0.6168, -0.6946,  0.9341, -0.6707],\n",
      "         [-0.4249,  0.5503, -0.0058, -0.4296,  0.6238]],\n",
      "\n",
      "        [[-0.4096, -0.4656,  0.5384, -0.1303, -0.7005],\n",
      "         [ 0.8908,  0.9197, -0.9231,  0.9151,  0.8848]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0609 cost = 0.003899\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5924,  0.1299, -0.3722,  0.5840],\n",
      "         [ 0.8903, -0.8425, -0.9169,  0.8592, -0.0951]],\n",
      "\n",
      "        [[ 0.9119,  0.9075, -0.9368,  0.9075,  0.8765],\n",
      "         [-0.7292, -0.4555,  0.7419, -0.1166, -0.9030]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0609 cost = 0.001532\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9525, -0.6171, -0.6948,  0.9341, -0.6709],\n",
      "         [ 0.8594, -0.9428, -0.9504,  0.7450,  0.0190]],\n",
      "\n",
      "        [[-0.4097, -0.4657,  0.5385, -0.1304, -0.7005],\n",
      "         [-0.6917, -0.4597,  0.6976, -0.1085, -0.8724]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0610 cost = 0.004667\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3795,  0.5924,  0.1299, -0.3722,  0.5841],\n",
      "         [-0.5012,  0.4500,  0.0994, -0.5287,  0.6516]],\n",
      "\n",
      "        [[ 0.9119,  0.9075, -0.9368,  0.9075,  0.8765],\n",
      "         [ 0.8514,  0.8469, -0.8823,  0.8349,  0.8667]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0610 cost = 0.005478\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4251,  0.5507, -0.0056, -0.4296,  0.6240],\n",
      "         [ 0.8904, -0.8427, -0.9170,  0.8593, -0.0956]],\n",
      "\n",
      "        [[ 0.8909,  0.9198, -0.9231,  0.9152,  0.8849],\n",
      "         [-0.7293, -0.4556,  0.7421, -0.1169, -0.9030]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0610 cost = 0.002078\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4252,  0.5508, -0.0055, -0.4296,  0.6240],\n",
      "         [-0.5012,  0.4501,  0.0993, -0.5287,  0.6519]],\n",
      "\n",
      "        [[ 0.8909,  0.9198, -0.9232,  0.9152,  0.8849],\n",
      "         [ 0.8515,  0.8470, -0.8824,  0.8350,  0.8668]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0611 cost = 0.005716\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8596, -0.9429, -0.9504,  0.7454,  0.0187],\n",
      "         [ 0.9526, -0.6177, -0.6952,  0.9342, -0.6711]],\n",
      "\n",
      "        [[-0.6918, -0.4598,  0.6977, -0.1089, -0.8724],\n",
      "         [-0.4099, -0.4660,  0.5387, -0.1306, -0.7006]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0611 cost = 0.003627\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3797,  0.5926,  0.1299, -0.3720,  0.5844],\n",
      "         [ 0.8905, -0.8427, -0.9171,  0.8593, -0.0958]],\n",
      "\n",
      "        [[ 0.9120,  0.9076, -0.9368,  0.9076,  0.8768],\n",
      "         [-0.7295, -0.4558,  0.7422, -0.1172, -0.9030]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0611 cost = 0.001520\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5015,  0.4504,  0.0993, -0.5288,  0.6523],\n",
      "         [-0.3798,  0.5927,  0.1299, -0.3720,  0.5844]],\n",
      "\n",
      "        [[ 0.8517,  0.8472, -0.8825,  0.8351,  0.8670],\n",
      "         [ 0.9120,  0.9076, -0.9368,  0.9076,  0.8768]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0612 cost = 0.002599\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4255,  0.5514, -0.0053, -0.4294,  0.6243],\n",
      "         [ 0.8905, -0.8428, -0.9171,  0.8594, -0.0960]],\n",
      "\n",
      "        [[ 0.8910,  0.9199, -0.9232,  0.9153,  0.8851],\n",
      "         [-0.7295, -0.4559,  0.7423, -0.1174, -0.9031]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0612 cost = 0.002063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9526, -0.6181, -0.6954,  0.9342, -0.6712],\n",
      "         [ 0.8597, -0.9429, -0.9505,  0.7455,  0.0185]],\n",
      "\n",
      "        [[-0.4101, -0.4662,  0.5389, -0.1307, -0.7007],\n",
      "         [-0.6919, -0.4599,  0.6978, -0.1092, -0.8724]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0612 cost = 0.004627\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9526, -0.6182, -0.6955,  0.9342, -0.6713],\n",
      "         [-0.5014,  0.4509,  0.0993, -0.5287,  0.6527]],\n",
      "\n",
      "        [[-0.4102, -0.4663,  0.5389, -0.1307, -0.7008],\n",
      "         [ 0.8518,  0.8473, -0.8826,  0.8352,  0.8671]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0613 cost = 0.004563\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8598, -0.9429, -0.9505,  0.7457,  0.0184],\n",
      "         [ 0.8906, -0.8428, -0.9172,  0.8595, -0.0963]],\n",
      "\n",
      "        [[-0.6919, -0.4600,  0.6979, -0.1094, -0.8724],\n",
      "         [-0.7297, -0.4561,  0.7424, -0.1176, -0.9031]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0613 cost = 0.001793\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4257,  0.5520, -0.0051, -0.4292,  0.6246],\n",
      "         [-0.3795,  0.5931,  0.1299, -0.3715,  0.5848]],\n",
      "\n",
      "        [[ 0.8911,  0.9199, -0.9232,  0.9154,  0.8852],\n",
      "         [ 0.9121,  0.9076, -0.9369,  0.9077,  0.8770]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0613 cost = 0.001844\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5932,  0.1299, -0.3715,  0.5849],\n",
      "         [-0.4258,  0.5522, -0.0050, -0.4292,  0.6247]],\n",
      "\n",
      "        [[ 0.9121,  0.9077, -0.9369,  0.9077,  0.8770],\n",
      "         [ 0.8911,  0.9199, -0.9232,  0.9154,  0.8853]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0614 cost = 0.002460\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8907, -0.8429, -0.9172,  0.8595, -0.0966],\n",
      "         [ 0.8599, -0.9430, -0.9506,  0.7460,  0.0182]],\n",
      "\n",
      "        [[-0.7298, -0.4563,  0.7425, -0.1179, -0.9032],\n",
      "         [-0.6920, -0.4602,  0.6980, -0.1096, -0.8725]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0614 cost = 0.003186\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5014,  0.4514,  0.0992, -0.5287,  0.6532],\n",
      "         [ 0.9526, -0.6190, -0.6959,  0.9342, -0.6716]],\n",
      "\n",
      "        [[ 0.8520,  0.8475, -0.8827,  0.8354,  0.8673],\n",
      "         [-0.4105, -0.4667,  0.5391, -0.1310, -0.7009]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0614 cost = 0.003893\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4259,  0.5526, -0.0049, -0.4290,  0.6249],\n",
      "         [ 0.9527, -0.6191, -0.6960,  0.9342, -0.6716]],\n",
      "\n",
      "        [[ 0.8912,  0.9200, -0.9233,  0.9154,  0.8854],\n",
      "         [-0.4105, -0.4668,  0.5392, -0.1310, -0.7009]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0615 cost = 0.003656\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5015,  0.4515,  0.0992, -0.5287,  0.6534],\n",
      "         [ 0.8908, -0.8430, -0.9173,  0.8596, -0.0970]],\n",
      "\n",
      "        [[ 0.8520,  0.8475, -0.8827,  0.8354,  0.8674],\n",
      "         [-0.7299, -0.4566,  0.7426, -0.1182, -0.9032]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0615 cost = 0.002194\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5935,  0.1299, -0.3712,  0.5852],\n",
      "         [ 0.8602, -0.9430, -0.9507,  0.7464,  0.0180]],\n",
      "\n",
      "        [[ 0.9122,  0.9077, -0.9369,  0.9078,  0.8772],\n",
      "         [-0.6921, -0.4605,  0.6982, -0.1100, -0.8725]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0615 cost = 0.001682\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4261,  0.5530, -0.0047, -0.4289,  0.6251],\n",
      "         [-0.5016,  0.4516,  0.0992, -0.5288,  0.6535]],\n",
      "\n",
      "        [[ 0.8912,  0.9200, -0.9233,  0.9155,  0.8854],\n",
      "         [ 0.8521,  0.8476, -0.8827,  0.8354,  0.8674]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0616 cost = 0.005620\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8909, -0.8430, -0.9174,  0.8597, -0.0972],\n",
      "         [-0.3796,  0.5935,  0.1299, -0.3711,  0.5853]],\n",
      "\n",
      "        [[-0.7300, -0.4569,  0.7428, -0.1185, -0.9033],\n",
      "         [ 0.9123,  0.9077, -0.9369,  0.9078,  0.8773]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0616 cost = 0.001153\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8604, -0.9431, -0.9507,  0.7467,  0.0178],\n",
      "         [ 0.9527, -0.6194, -0.6963,  0.9343, -0.6719]],\n",
      "\n",
      "        [[-0.6922, -0.4607,  0.6983, -0.1102, -0.8726],\n",
      "         [-0.4108, -0.4672,  0.5394, -0.1313, -0.7011]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0616 cost = 0.003564\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9527, -0.6195, -0.6964,  0.9343, -0.6719],\n",
      "         [ 0.8910, -0.8430, -0.9174,  0.8598, -0.0974]],\n",
      "\n",
      "        [[-0.4108, -0.4673,  0.5394, -0.1313, -0.7011],\n",
      "         [-0.7301, -0.4571,  0.7428, -0.1187, -0.9033]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0617 cost = 0.003697\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5020,  0.4518,  0.0992, -0.5289,  0.6539],\n",
      "         [-0.3797,  0.5936,  0.1299, -0.3710,  0.5854]],\n",
      "\n",
      "        [[ 0.8522,  0.8477, -0.8828,  0.8356,  0.8676],\n",
      "         [ 0.9123,  0.9077, -0.9369,  0.9079,  0.8774]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0617 cost = 0.002536\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4265,  0.5537, -0.0044, -0.4287,  0.6254],\n",
      "         [ 0.8605, -0.9431, -0.9508,  0.7470,  0.0177]],\n",
      "\n",
      "        [[ 0.8913,  0.9201, -0.9233,  0.9156,  0.8856],\n",
      "         [-0.6923, -0.4609,  0.6984, -0.1105, -0.8726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0617 cost = 0.002186\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5020,  0.4521,  0.0992, -0.5288,  0.6541],\n",
      "         [ 0.8911, -0.8431, -0.9175,  0.8599, -0.0977]],\n",
      "\n",
      "        [[ 0.8523,  0.8477, -0.8828,  0.8356,  0.8677],\n",
      "         [-0.7302, -0.4574,  0.7430, -0.1190, -0.9034]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0618 cost = 0.002170\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4266,  0.5540, -0.0043, -0.4286,  0.6255],\n",
      "         [ 0.9527, -0.6199, -0.6967,  0.9343, -0.6721]],\n",
      "\n",
      "        [[ 0.8913,  0.9201, -0.9233,  0.9156,  0.8856],\n",
      "         [-0.4111, -0.4677,  0.5396, -0.1316, -0.7013]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0618 cost = 0.003608\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3795,  0.5940,  0.1299, -0.3706,  0.5857],\n",
      "         [ 0.8607, -0.9432, -0.9508,  0.7472,  0.0175]],\n",
      "\n",
      "        [[ 0.9123,  0.9078, -0.9369,  0.9079,  0.8775],\n",
      "         [-0.6924, -0.4611,  0.6985, -0.1107, -0.8727]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0618 cost = 0.001662\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5021,  0.4523,  0.0992, -0.5287,  0.6543],\n",
      "         [ 0.9528, -0.6201, -0.6968,  0.9344, -0.6722]],\n",
      "\n",
      "        [[ 0.8524,  0.8478, -0.8829,  0.8357,  0.8677],\n",
      "         [-0.4112, -0.4679,  0.5397, -0.1317, -0.7013]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0619 cost = 0.003828\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8608, -0.9432, -0.9509,  0.7474,  0.0174],\n",
      "         [ 0.8912, -0.8431, -0.9176,  0.8600, -0.0980]],\n",
      "\n",
      "        [[-0.6924, -0.4612,  0.6985, -0.1109, -0.8727],\n",
      "         [-0.7304, -0.4577,  0.7431, -0.1194, -0.9034]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0619 cost = 0.001752\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3795,  0.5941,  0.1300, -0.3704,  0.5857],\n",
      "         [-0.4270,  0.5545, -0.0041, -0.4284,  0.6257]],\n",
      "\n",
      "        [[ 0.9124,  0.9078, -0.9370,  0.9079,  0.8775],\n",
      "         [ 0.8914,  0.9201, -0.9233,  0.9157,  0.8857]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0619 cost = 0.002410\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9528, -0.6203, -0.6971,  0.9344, -0.6723],\n",
      "         [-0.4270,  0.5546, -0.0040, -0.4283,  0.6257]],\n",
      "\n",
      "        [[-0.4113, -0.4682,  0.5398, -0.1319, -0.7015],\n",
      "         [ 0.8914,  0.9201, -0.9233,  0.9157,  0.8857]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0620 cost = 0.003754\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8610, -0.9432, -0.9509,  0.7478,  0.0173],\n",
      "         [ 0.8913, -0.8432, -0.9176,  0.8601, -0.0982]],\n",
      "\n",
      "        [[-0.6925, -0.4615,  0.6986, -0.1111, -0.8727],\n",
      "         [-0.7305, -0.4580,  0.7432, -0.1197, -0.9035]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0620 cost = 0.001745\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5024,  0.4526,  0.0992, -0.5287,  0.6546],\n",
      "         [-0.3796,  0.5942,  0.1300, -0.3702,  0.5858]],\n",
      "\n",
      "        [[ 0.8525,  0.8479, -0.8829,  0.8357,  0.8678],\n",
      "         [ 0.9124,  0.9078, -0.9370,  0.9080,  0.8776]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0620 cost = 0.002500\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8914, -0.8432, -0.9177,  0.8602, -0.0984],\n",
      "         [-0.4272,  0.5551, -0.0038, -0.4281,  0.6259]],\n",
      "\n",
      "        [[-0.7306, -0.4582,  0.7433, -0.1199, -0.9035],\n",
      "         [ 0.8914,  0.9202, -0.9234,  0.9157,  0.8858]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0621 cost = 0.001795\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5944,  0.1300, -0.3699,  0.5860],\n",
      "         [ 0.9528, -0.6206, -0.6974,  0.9345, -0.6725]],\n",
      "\n",
      "        [[ 0.9124,  0.9078, -0.9370,  0.9080,  0.8776],\n",
      "         [-0.4115, -0.4686,  0.5399, -0.1322, -0.7016]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0621 cost = 0.003024\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8612, -0.9433, -0.9510,  0.7481,  0.0171],\n",
      "         [-0.5023,  0.4529,  0.0992, -0.5286,  0.6548]],\n",
      "\n",
      "        [[-0.6926, -0.4617,  0.6987, -0.1114, -0.8728],\n",
      "         [ 0.8525,  0.8479, -0.8830,  0.8358,  0.8679]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0621 cost = 0.002174\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3793,  0.5946,  0.1300, -0.3697,  0.5861],\n",
      "         [ 0.9529, -0.6208, -0.6975,  0.9345, -0.6726]],\n",
      "\n",
      "        [[ 0.9125,  0.9078, -0.9370,  0.9080,  0.8777],\n",
      "         [-0.4116, -0.4687,  0.5400, -0.1323, -0.7017]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0622 cost = 0.003016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8613, -0.9433, -0.9510,  0.7483,  0.0170],\n",
      "         [-0.5023,  0.4530,  0.0992, -0.5285,  0.6549]],\n",
      "\n",
      "        [[-0.6926, -0.4618,  0.6988, -0.1116, -0.8728],\n",
      "         [ 0.8526,  0.8479, -0.8830,  0.8358,  0.8679]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0622 cost = 0.002170\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8916, -0.8432, -0.9178,  0.8604, -0.0988],\n",
      "         [-0.4274,  0.5557, -0.0035, -0.4277,  0.6262]],\n",
      "\n",
      "        [[-0.7308, -0.4586,  0.7435, -0.1204, -0.9036],\n",
      "         [ 0.8915,  0.9202, -0.9234,  0.9158,  0.8858]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0622 cost = 0.001784\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8916, -0.8433, -0.9178,  0.8604, -0.0989],\n",
      "         [ 0.9529, -0.6211, -0.6977,  0.9345, -0.6727]],\n",
      "\n",
      "        [[-0.7308, -0.4586,  0.7435, -0.1205, -0.9036],\n",
      "         [-0.4118, -0.4690,  0.5401, -0.1325, -0.7018]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0623 cost = 0.003712\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5947,  0.1301, -0.3694,  0.5863],\n",
      "         [-0.4274,  0.5558, -0.0034, -0.4277,  0.6262]],\n",
      "\n",
      "        [[ 0.9125,  0.9078, -0.9370,  0.9080,  0.8778],\n",
      "         [ 0.8915,  0.9202, -0.9234,  0.9158,  0.8859]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0623 cost = 0.002382\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5023,  0.4531,  0.0992, -0.5284,  0.6552],\n",
      "         [ 0.8615, -0.9434, -0.9511,  0.7486,  0.0168]],\n",
      "\n",
      "        [[ 0.8526,  0.8480, -0.8830,  0.8359,  0.8680],\n",
      "         [-0.6928, -0.4620,  0.6989, -0.1119, -0.8728]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0623 cost = 0.002414\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8917, -0.8434, -0.9178,  0.8605, -0.0992],\n",
      "         [-0.3791,  0.5947,  0.1301, -0.3692,  0.5864]],\n",
      "\n",
      "        [[-0.7309, -0.4588,  0.7437, -0.1208, -0.9036],\n",
      "         [ 0.9125,  0.9078, -0.9370,  0.9081,  0.8778]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0624 cost = 0.001120\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9529, -0.6214, -0.6979,  0.9346, -0.6730],\n",
      "         [ 0.8616, -0.9434, -0.9511,  0.7488,  0.0167]],\n",
      "\n",
      "        [[-0.4120, -0.4693,  0.5403, -0.1328, -0.7020],\n",
      "         [-0.6928, -0.4621,  0.6990, -0.1121, -0.8729]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0624 cost = 0.004439\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5024,  0.4531,  0.0993, -0.5284,  0.6553],\n",
      "         [-0.4275,  0.5562, -0.0032, -0.4275,  0.6264]],\n",
      "\n",
      "        [[ 0.8527,  0.8480, -0.8830,  0.8359,  0.8681],\n",
      "         [ 0.8915,  0.9202, -0.9234,  0.9159,  0.8859]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0624 cost = 0.003521\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8618, -0.9434, -0.9511,  0.7491,  0.0166],\n",
      "         [ 0.9530, -0.6217, -0.6981,  0.9347, -0.6731]],\n",
      "\n",
      "        [[-0.6929, -0.4622,  0.6991, -0.1123, -0.8729],\n",
      "         [-0.4121, -0.4695,  0.5404, -0.1329, -0.7020]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0625 cost = 0.003455\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4275,  0.5565, -0.0031, -0.4273,  0.6265],\n",
      "         [ 0.8919, -0.8435, -0.9179,  0.8607, -0.0996]],\n",
      "\n",
      "        [[ 0.8916,  0.9203, -0.9234,  0.9159,  0.8860],\n",
      "         [-0.7311, -0.4592,  0.7438, -0.1212, -0.9037]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0625 cost = 0.001957\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3789,  0.5949,  0.1301, -0.3688,  0.5866],\n",
      "         [-0.5023,  0.4533,  0.0993, -0.5283,  0.6555]],\n",
      "\n",
      "        [[ 0.9126,  0.9079, -0.9370,  0.9081,  0.8779],\n",
      "         [ 0.8527,  0.8481, -0.8830,  0.8359,  0.8681]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0625 cost = 0.005268\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8920, -0.8435, -0.9180,  0.8608, -0.0998],\n",
      "         [ 0.8620, -0.9435, -0.9512,  0.7495,  0.0164]],\n",
      "\n",
      "        [[-0.7312, -0.4593,  0.7439, -0.1214, -0.9037],\n",
      "         [-0.6930, -0.4624,  0.6992, -0.1125, -0.8729]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0626 cost = 0.003049\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4274,  0.5569, -0.0030, -0.4271,  0.6267],\n",
      "         [-0.3788,  0.5950,  0.1301, -0.3685,  0.5866]],\n",
      "\n",
      "        [[ 0.8916,  0.9203, -0.9234,  0.9160,  0.8860],\n",
      "         [ 0.9126,  0.9079, -0.9370,  0.9081,  0.8780]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0626 cost = 0.001750\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9530, -0.6221, -0.6985,  0.9347, -0.6733],\n",
      "         [-0.5022,  0.4536,  0.0993, -0.5281,  0.6558]],\n",
      "\n",
      "        [[-0.4124, -0.4699,  0.5406, -0.1331, -0.7022],\n",
      "         [ 0.8528,  0.8482, -0.8831,  0.8361,  0.8683]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0626 cost = 0.004367\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8921, -0.8436, -0.9180,  0.8609, -0.1001],\n",
      "         [ 0.8622, -0.9435, -0.9512,  0.7498,  0.0162]],\n",
      "\n",
      "        [[-0.7313, -0.4596,  0.7440, -0.1217, -0.9038],\n",
      "         [-0.6931, -0.4626,  0.6993, -0.1128, -0.8730]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0627 cost = 0.003036\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.5952,  0.1301, -0.3683,  0.5868],\n",
      "         [-0.5022,  0.4538,  0.0993, -0.5281,  0.6560]],\n",
      "\n",
      "        [[ 0.9126,  0.9079, -0.9370,  0.9082,  0.8781],\n",
      "         [ 0.8529,  0.8482, -0.8831,  0.8361,  0.8683]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0627 cost = 0.005241\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4274,  0.5575, -0.0029, -0.4269,  0.6269],\n",
      "         [ 0.9530, -0.6223, -0.6987,  0.9347, -0.6735]],\n",
      "\n",
      "        [[ 0.8917,  0.9203, -0.9234,  0.9160,  0.8862],\n",
      "         [-0.4126, -0.4702,  0.5407, -0.1332, -0.7023]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0627 cost = 0.003478\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5022,  0.4540,  0.0992, -0.5280,  0.6562],\n",
      "         [-0.4274,  0.5577, -0.0028, -0.4268,  0.6270]],\n",
      "\n",
      "        [[ 0.8530,  0.8483, -0.8832,  0.8362,  0.8684],\n",
      "         [ 0.8917,  0.9203, -0.9234,  0.9161,  0.8862]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0628 cost = 0.003471\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8625, -0.9436, -0.9513,  0.7502,  0.0160],\n",
      "         [ 0.9531, -0.6224, -0.6988,  0.9347, -0.6735]],\n",
      "\n",
      "        [[-0.6932, -0.4628,  0.6995, -0.1132, -0.8731],\n",
      "         [-0.4127, -0.4703,  0.5408, -0.1333, -0.7024]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0628 cost = 0.003417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8923, -0.8436, -0.9182,  0.8610, -0.1006],\n",
      "         [-0.3785,  0.5955,  0.1301, -0.3678,  0.5871]],\n",
      "\n",
      "        [[-0.7315, -0.4601,  0.7443, -0.1222, -0.9039],\n",
      "         [ 0.9127,  0.9080, -0.9371,  0.9083,  0.8783]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0628 cost = 0.001100\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9531, -0.6226, -0.6990,  0.9348, -0.6736],\n",
      "         [ 0.8923, -0.8437, -0.9182,  0.8610, -0.1006]],\n",
      "\n",
      "        [[-0.4128, -0.4705,  0.5409, -0.1333, -0.7024],\n",
      "         [-0.7316, -0.4602,  0.7443, -0.1223, -0.9039]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0629 cost = 0.003548\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4272,  0.5584, -0.0028, -0.4265,  0.6273],\n",
      "         [-0.5020,  0.4545,  0.0992, -0.5277,  0.6567]],\n",
      "\n",
      "        [[ 0.8918,  0.9204, -0.9235,  0.9161,  0.8863],\n",
      "         [ 0.8532,  0.8485, -0.8833,  0.8364,  0.8686]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0629 cost = 0.005426\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.5958,  0.1301, -0.3675,  0.5873],\n",
      "         [ 0.8627, -0.9437, -0.9514,  0.7506,  0.0157]],\n",
      "\n",
      "        [[ 0.9128,  0.9080, -0.9371,  0.9083,  0.8784],\n",
      "         [-0.6933, -0.4631,  0.6996, -0.1135, -0.8731]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0629 cost = 0.001588\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9531, -0.6228, -0.6992,  0.9348, -0.6737],\n",
      "         [-0.4273,  0.5588, -0.0027, -0.4263,  0.6275]],\n",
      "\n",
      "        [[-0.4130, -0.4708,  0.5411, -0.1334, -0.7025],\n",
      "         [ 0.8918,  0.9204, -0.9235,  0.9162,  0.8864]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0630 cost = 0.003630\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8628, -0.9437, -0.9514,  0.7508,  0.0156],\n",
      "         [-0.3783,  0.5959,  0.1301, -0.3673,  0.5874]],\n",
      "\n",
      "        [[-0.6933, -0.4632,  0.6997, -0.1137, -0.8732],\n",
      "         [ 0.9128,  0.9080, -0.9371,  0.9084,  0.8785]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0630 cost = 0.001077\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5021,  0.4550,  0.0991, -0.5276,  0.6572],\n",
      "         [ 0.8925, -0.8438, -0.9183,  0.8612, -0.1011]],\n",
      "\n",
      "        [[ 0.8534,  0.8486, -0.8834,  0.8365,  0.8688],\n",
      "         [-0.7318, -0.4606,  0.7445, -0.1227, -0.9040]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0630 cost = 0.002064\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.5960,  0.1301, -0.3672,  0.5875],\n",
      "         [-0.5021,  0.4550,  0.0991, -0.5276,  0.6572]],\n",
      "\n",
      "        [[ 0.9128,  0.9080, -0.9371,  0.9084,  0.8786],\n",
      "         [ 0.8534,  0.8487, -0.8834,  0.8365,  0.8689]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0631 cost = 0.005170\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4275,  0.5594, -0.0026, -0.4261,  0.6277],\n",
      "         [ 0.9531, -0.6232, -0.6995,  0.9348, -0.6739]],\n",
      "\n",
      "        [[ 0.8919,  0.9205, -0.9235,  0.9163,  0.8865],\n",
      "         [-0.4132, -0.4711,  0.5413, -0.1336, -0.7026]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0631 cost = 0.003431\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8629, -0.9437, -0.9515,  0.7510,  0.0155],\n",
      "         [ 0.8925, -0.8438, -0.9184,  0.8612, -0.1013]],\n",
      "\n",
      "        [[-0.6934, -0.4633,  0.6998, -0.1139, -0.8732],\n",
      "         [-0.7319, -0.4607,  0.7446, -0.1229, -0.9040]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0631 cost = 0.001673\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4276,  0.5598, -0.0025, -0.4260,  0.6279],\n",
      "         [-0.3784,  0.5962,  0.1301, -0.3670,  0.5876]],\n",
      "\n",
      "        [[ 0.8920,  0.9205, -0.9235,  0.9163,  0.8866],\n",
      "         [ 0.9129,  0.9081, -0.9371,  0.9085,  0.8787]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0632 cost = 0.001705\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9531, -0.6233, -0.6997,  0.9348, -0.6740],\n",
      "         [ 0.8630, -0.9437, -0.9515,  0.7511,  0.0154]],\n",
      "\n",
      "        [[-0.4134, -0.4713,  0.5414, -0.1336, -0.7027],\n",
      "         [-0.6934, -0.4634,  0.6999, -0.1140, -0.8732]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0632 cost = 0.004321\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5023,  0.4556,  0.0990, -0.5275,  0.6578],\n",
      "         [ 0.8926, -0.8438, -0.9185,  0.8613, -0.1015]],\n",
      "\n",
      "        [[ 0.8536,  0.8489, -0.8836,  0.8368,  0.8691],\n",
      "         [-0.7320, -0.4610,  0.7447, -0.1232, -0.9040]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0632 cost = 0.002047\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5024,  0.4557,  0.0990, -0.5275,  0.6579],\n",
      "         [ 0.9532, -0.6236, -0.6999,  0.9348, -0.6740]],\n",
      "\n",
      "        [[ 0.8537,  0.8489, -0.8836,  0.8368,  0.8692],\n",
      "         [-0.4134, -0.4715,  0.5415, -0.1337, -0.7028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0633 cost = 0.003634\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4279,  0.5604, -0.0024, -0.4258,  0.6281],\n",
      "         [ 0.8632, -0.9438, -0.9515,  0.7514,  0.0152]],\n",
      "\n",
      "        [[ 0.8921,  0.9206, -0.9236,  0.9164,  0.8867],\n",
      "         [-0.6935, -0.4636,  0.6999, -0.1143, -0.8732]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0633 cost = 0.002043\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3786,  0.5964,  0.1301, -0.3667,  0.5878],\n",
      "         [ 0.8927, -0.8438, -0.9185,  0.8614, -0.1018]],\n",
      "\n",
      "        [[ 0.9130,  0.9081, -0.9372,  0.9085,  0.8789],\n",
      "         [-0.7321, -0.4613,  0.7448, -0.1235, -0.9041]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0633 cost = 0.001399\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8633, -0.9438, -0.9516,  0.7516,  0.0151],\n",
      "         [ 0.9532, -0.6238, -0.7001,  0.9348, -0.6742]],\n",
      "\n",
      "        [[-0.6935, -0.4638,  0.7000, -0.1144, -0.8733],\n",
      "         [-0.4136, -0.4718,  0.5416, -0.1339, -0.7029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0634 cost = 0.003356\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4282,  0.5607, -0.0022, -0.4257,  0.6282],\n",
      "         [ 0.8928, -0.8438, -0.9186,  0.8615, -0.1020]],\n",
      "\n",
      "        [[ 0.8921,  0.9206, -0.9236,  0.9164,  0.8868],\n",
      "         [-0.7322, -0.4615,  0.7449, -0.1236, -0.9041]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0634 cost = 0.001885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5028,  0.4559,  0.0990, -0.5275,  0.6582],\n",
      "         [-0.3788,  0.5965,  0.1301, -0.3666,  0.5879]],\n",
      "\n",
      "        [[ 0.8538,  0.8490, -0.8836,  0.8369,  0.8693],\n",
      "         [ 0.9130,  0.9082, -0.9372,  0.9086,  0.8790]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0634 cost = 0.002348\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9532, -0.6241, -0.7003,  0.9349, -0.6743],\n",
      "         [-0.5028,  0.4560,  0.0990, -0.5275,  0.6583]],\n",
      "\n",
      "        [[-0.4138, -0.4721,  0.5417, -0.1341, -0.7030],\n",
      "         [ 0.8538,  0.8490, -0.8836,  0.8369,  0.8693]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0635 cost = 0.004252\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8929, -0.8439, -0.9186,  0.8616, -0.1022],\n",
      "         [-0.3787,  0.5967,  0.1301, -0.3664,  0.5880]],\n",
      "\n",
      "        [[-0.7323, -0.4617,  0.7450, -0.1239, -0.9042],\n",
      "         [ 0.9131,  0.9082, -0.9372,  0.9086,  0.8790]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0635 cost = 0.001073\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4284,  0.5612, -0.0020, -0.4254,  0.6284],\n",
      "         [ 0.8636, -0.9439, -0.9517,  0.7521,  0.0149]],\n",
      "\n",
      "        [[ 0.8921,  0.9206, -0.9236,  0.9165,  0.8869],\n",
      "         [-0.6937, -0.4641,  0.7002, -0.1148, -0.8733]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0635 cost = 0.002023\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8636, -0.9439, -0.9517,  0.7522,  0.0148],\n",
      "         [-0.4284,  0.5613, -0.0019, -0.4254,  0.6285]],\n",
      "\n",
      "        [[-0.6937, -0.4642,  0.7002, -0.1149, -0.8734],\n",
      "         [ 0.8922,  0.9206, -0.9236,  0.9165,  0.8869]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0636 cost = 0.001685\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3786,  0.5969,  0.1302, -0.3662,  0.5881],\n",
      "         [ 0.8930, -0.8439, -0.9187,  0.8616, -0.1025]],\n",
      "\n",
      "        [[ 0.9131,  0.9082, -0.9372,  0.9086,  0.8791],\n",
      "         [-0.7324, -0.4619,  0.7452, -0.1242, -0.9042]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0636 cost = 0.001385\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5029,  0.4564,  0.0990, -0.5274,  0.6586],\n",
      "         [ 0.9533, -0.6245, -0.7006,  0.9349, -0.6745]],\n",
      "\n",
      "        [[ 0.8540,  0.8491, -0.8837,  0.8370,  0.8694],\n",
      "         [-0.4140, -0.4724,  0.5419, -0.1343, -0.7031]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0636 cost = 0.003585\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4287,  0.5616, -0.0017, -0.4252,  0.6286],\n",
      "         [ 0.8930, -0.8440, -0.9187,  0.8617, -0.1027]],\n",
      "\n",
      "        [[ 0.8922,  0.9207, -0.9236,  0.9166,  0.8869],\n",
      "         [-0.7325, -0.4621,  0.7452, -0.1244, -0.9042]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0637 cost = 0.001865\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5031,  0.4565,  0.0990, -0.5274,  0.6587],\n",
      "         [ 0.8638, -0.9439, -0.9517,  0.7525,  0.0146]],\n",
      "\n",
      "        [[ 0.8540,  0.8492, -0.8837,  0.8370,  0.8695],\n",
      "         [-0.6938, -0.4644,  0.7003, -0.1152, -0.8734]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0637 cost = 0.002276\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.5970,  0.1302, -0.3659,  0.5882],\n",
      "         [ 0.9533, -0.6247, -0.7008,  0.9350, -0.6747]],\n",
      "\n",
      "        [[ 0.9131,  0.9082, -0.9372,  0.9087,  0.8792],\n",
      "         [-0.4141, -0.4726,  0.5420, -0.1345, -0.7032]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0637 cost = 0.002853\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9533, -0.6248, -0.7008,  0.9350, -0.6747],\n",
      "         [ 0.8639, -0.9439, -0.9518,  0.7527,  0.0145]],\n",
      "\n",
      "        [[-0.4142, -0.4727,  0.5420, -0.1346, -0.7033],\n",
      "         [-0.6938, -0.4645,  0.7004, -0.1153, -0.8734]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0638 cost = 0.004238\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8932, -0.8440, -0.9188,  0.8618, -0.1030],\n",
      "         [-0.3788,  0.5970,  0.1303, -0.3659,  0.5882]],\n",
      "\n",
      "        [[-0.7326, -0.4624,  0.7454, -0.1248, -0.9043],\n",
      "         [ 0.9131,  0.9082, -0.9372,  0.9087,  0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0638 cost = 0.001062\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4291,  0.5620, -0.0013, -0.4250,  0.6287],\n",
      "         [-0.5034,  0.4565,  0.0991, -0.5275,  0.6588]],\n",
      "\n",
      "        [[ 0.8922,  0.9207, -0.9236,  0.9166,  0.8870],\n",
      "         [ 0.8541,  0.8492, -0.8838,  0.8371,  0.8695]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0638 cost = 0.005285\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5035,  0.4565,  0.0991, -0.5275,  0.6589],\n",
      "         [-0.4292,  0.5621, -0.0012, -0.4250,  0.6287]],\n",
      "\n",
      "        [[ 0.8541,  0.8492, -0.8838,  0.8371,  0.8696],\n",
      "         [ 0.8923,  0.9207, -0.9236,  0.9166,  0.8870]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0639 cost = 0.003319\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9533, -0.6251, -0.7012,  0.9351, -0.6749],\n",
      "         [ 0.8933, -0.8441, -0.9189,  0.8619, -0.1033]],\n",
      "\n",
      "        [[-0.4144, -0.4731,  0.5422, -0.1348, -0.7034],\n",
      "         [-0.7327, -0.4626,  0.7455, -0.1250, -0.9043]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0639 cost = 0.003431\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3789,  0.5971,  0.1303, -0.3656,  0.5883],\n",
      "         [ 0.8642, -0.9440, -0.9519,  0.7532,  0.0143]],\n",
      "\n",
      "        [[ 0.9132,  0.9082, -0.9372,  0.9087,  0.8793],\n",
      "         [-0.6940, -0.4648,  0.7005, -0.1157, -0.8735]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0639 cost = 0.001526\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8934, -0.8441, -0.9189,  0.8620, -0.1035],\n",
      "         [-0.4294,  0.5626, -0.0011, -0.4248,  0.6289]],\n",
      "\n",
      "        [[-0.7328, -0.4628,  0.7456, -0.1252, -0.9044],\n",
      "         [ 0.8923,  0.9207, -0.9237,  0.9167,  0.8871]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0640 cost = 0.001671\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5037,  0.4568,  0.0991, -0.5274,  0.6591],\n",
      "         [ 0.9534, -0.6254, -0.7014,  0.9351, -0.6750]],\n",
      "\n",
      "        [[ 0.8542,  0.8493, -0.8839,  0.8372,  0.8697],\n",
      "         [-0.4145, -0.4733,  0.5423, -0.1350, -0.7035]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0640 cost = 0.003536\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8644, -0.9441, -0.9519,  0.7535,  0.0141],\n",
      "         [-0.3790,  0.5972,  0.1303, -0.3654,  0.5884]],\n",
      "\n",
      "        [[-0.6940, -0.4650,  0.7006, -0.1160, -0.8735],\n",
      "         [ 0.9132,  0.9083, -0.9373,  0.9088,  0.8794]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0640 cost = 0.001036\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4296,  0.5629, -0.0010, -0.4246,  0.6290],\n",
      "         [-0.3790,  0.5972,  0.1303, -0.3654,  0.5884]],\n",
      "\n",
      "        [[ 0.8924,  0.9208, -0.9237,  0.9167,  0.8872],\n",
      "         [ 0.9133,  0.9083, -0.9373,  0.9088,  0.8794]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0641 cost = 0.001643\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8935, -0.8441, -0.9190,  0.8621, -0.1038],\n",
      "         [ 0.9534, -0.6256, -0.7016,  0.9351, -0.6751]],\n",
      "\n",
      "        [[-0.7329, -0.4631,  0.7457, -0.1255, -0.9044],\n",
      "         [-0.4146, -0.4735,  0.5424, -0.1352, -0.7036]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0641 cost = 0.003491\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8645, -0.9441, -0.9520,  0.7538,  0.0140],\n",
      "         [-0.5040,  0.4570,  0.0991, -0.5274,  0.6593]],\n",
      "\n",
      "        [[-0.6941, -0.4651,  0.7006, -0.1162, -0.8736],\n",
      "         [ 0.8543,  0.8494, -0.8839,  0.8373,  0.8698]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0641 cost = 0.002026\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8935, -0.8442, -0.9190,  0.8622, -0.1040],\n",
      "         [ 0.8646, -0.9441, -0.9520,  0.7539,  0.0139]],\n",
      "\n",
      "        [[-0.7330, -0.4632,  0.7458, -0.1257, -0.9044],\n",
      "         [-0.6941, -0.4652,  0.7007, -0.1162, -0.8736]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0642 cost = 0.002872\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-4.2987e-01,  5.6333e-01, -7.7661e-04, -4.2441e-01,  6.2911e-01],\n",
      "         [-5.0411e-01,  4.5701e-01,  9.9112e-02, -5.2742e-01,  6.5942e-01]],\n",
      "\n",
      "        [[ 8.9241e-01,  9.2081e-01, -9.2370e-01,  9.1679e-01,  8.8726e-01],\n",
      "         [ 8.5435e-01,  8.4941e-01, -8.8390e-01,  8.3729e-01,  8.6986e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0642 cost = 0.005235\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5974,  0.1304, -0.3651,  0.5885],\n",
      "         [ 0.9534, -0.6259, -0.7019,  0.9352, -0.6753]],\n",
      "\n",
      "        [[ 0.9133,  0.9083, -0.9373,  0.9088,  0.8795],\n",
      "         [-0.4148, -0.4739,  0.5425, -0.1354, -0.7037]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0642 cost = 0.002802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8648, -0.9442, -0.9520,  0.7542,  0.0138],\n",
      "         [ 0.8936, -0.8442, -0.9191,  0.8623, -0.1043]],\n",
      "\n",
      "        [[-0.6942, -0.4653,  0.7008, -0.1165, -0.8736],\n",
      "         [-0.7331, -0.4635,  0.7459, -0.1260, -0.9045]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0643 cost = 0.001605\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-4.3007e-01,  5.6365e-01, -6.1537e-04, -4.2429e-01,  6.2923e-01],\n",
      "         [-5.0433e-01,  4.5711e-01,  9.9094e-02, -5.2749e-01,  6.5966e-01]],\n",
      "\n",
      "        [[ 8.9245e-01,  9.2085e-01, -9.2371e-01,  9.1683e-01,  8.8734e-01],\n",
      "         [ 8.5445e-01,  8.4951e-01, -8.8396e-01,  8.3738e-01,  8.6997e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0643 cost = 0.005219\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3793,  0.5975,  0.1304, -0.3651,  0.5886],\n",
      "         [ 0.9534, -0.6259, -0.7020,  0.9352, -0.6754]],\n",
      "\n",
      "        [[ 0.9133,  0.9083, -0.9373,  0.9089,  0.8796],\n",
      "         [-0.4150, -0.4741,  0.5427, -0.1355, -0.7038]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0643 cost = 0.002793\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9534, -0.6260, -0.7021,  0.9352, -0.6755],\n",
      "         [-0.5045,  0.4572,  0.0991, -0.5276,  0.6599]],\n",
      "\n",
      "        [[-0.4151, -0.4742,  0.5427, -0.1355, -0.7038],\n",
      "         [ 0.8545,  0.8496, -0.8840,  0.8375,  0.8701]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0644 cost = 0.004138\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-4.3038e-01,  5.6400e-01, -4.5435e-04, -4.2419e-01,  6.2938e-01],\n",
      "         [-3.7945e-01,  5.9749e-01,  1.3042e-01, -3.6504e-01,  5.8865e-01]],\n",
      "\n",
      "        [[ 8.9251e-01,  9.2089e-01, -9.2373e-01,  9.1688e-01,  8.8745e-01],\n",
      "         [ 9.1338e-01,  9.0834e-01, -9.3729e-01,  9.0889e-01,  8.7969e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0644 cost = 0.001620\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8938, -0.8443, -0.9192,  0.8624, -0.1047],\n",
      "         [ 0.8650, -0.9442, -0.9521,  0.7545,  0.0135]],\n",
      "\n",
      "        [[-0.7333, -0.4639,  0.7461, -0.1265, -0.9045],\n",
      "         [-0.6944, -0.4656,  0.7010, -0.1169, -0.8737]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0644 cost = 0.002844\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8650, -0.9443, -0.9521,  0.7546,  0.0135],\n",
      "         [ 0.9535, -0.6262, -0.7023,  0.9352, -0.6756]],\n",
      "\n",
      "        [[-0.6944, -0.4656,  0.7011, -0.1170, -0.8737],\n",
      "         [-0.4153, -0.4745,  0.5429, -0.1357, -0.7039]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0645 cost = 0.003234\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.7961e-01,  5.9758e-01,  1.3043e-01, -3.6501e-01,  5.8880e-01],\n",
      "         [-4.3065e-01,  5.6431e-01, -2.7662e-04, -4.2412e-01,  6.2953e-01]],\n",
      "\n",
      "        [[ 9.1342e-01,  9.0837e-01, -9.3730e-01,  9.0892e-01,  8.7979e-01],\n",
      "         [ 8.9255e-01,  9.2093e-01, -9.2374e-01,  9.1692e-01,  8.8753e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0645 cost = 0.002199\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8939, -0.8444, -0.9192,  0.8625, -0.1050],\n",
      "         [-0.5049,  0.4574,  0.0990, -0.5277,  0.6603]],\n",
      "\n",
      "        [[-0.7335, -0.4642,  0.7463, -0.1268, -0.9046],\n",
      "         [ 0.8547,  0.8497, -0.8841,  0.8376,  0.8703]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0645 cost = 0.001949\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-4.3078e-01,  5.6453e-01, -1.8335e-04, -4.2408e-01,  6.2965e-01],\n",
      "         [-5.0498e-01,  4.5744e-01,  9.9026e-02, -5.2777e-01,  6.6043e-01]],\n",
      "\n",
      "        [[ 8.9258e-01,  9.2095e-01, -9.2375e-01,  9.1695e-01,  8.8758e-01],\n",
      "         [ 8.5472e-01,  8.4977e-01, -8.8411e-01,  8.3760e-01,  8.7029e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0646 cost = 0.005173\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8652, -0.9443, -0.9522,  0.7550,  0.0132],\n",
      "         [-0.3798,  0.5977,  0.1304, -0.3650,  0.5890]],\n",
      "\n",
      "        [[-0.6945, -0.4659,  0.7012, -0.1174, -0.8737],\n",
      "         [ 0.9135,  0.9084, -0.9373,  0.9090,  0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0646 cost = 0.001014\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9535, -0.6264, -0.7026,  0.9352, -0.6758],\n",
      "         [ 0.8939, -0.8444, -0.9193,  0.8625, -0.1052]],\n",
      "\n",
      "        [[-0.4156, -0.4749,  0.5431, -0.1359, -0.7041],\n",
      "         [-0.7336, -0.4644,  0.7464, -0.1271, -0.9046]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0646 cost = 0.003355\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5052,  0.4576,  0.0990, -0.5279,  0.6608],\n",
      "         [-0.3799,  0.5977,  0.1304, -0.3649,  0.5891]],\n",
      "\n",
      "        [[ 0.8549,  0.8499, -0.8842,  0.8377,  0.8705],\n",
      "         [ 0.9135,  0.9084, -0.9373,  0.9090,  0.8800]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0647 cost = 0.002232\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8940, -0.8445, -0.9193,  0.8626, -0.1054],\n",
      "         [ 0.8653, -0.9443, -0.9522,  0.7551,  0.0131]],\n",
      "\n",
      "        [[-0.7337, -0.4645,  0.7465, -0.1273, -0.9047],\n",
      "         [-0.6946, -0.4660,  0.7013, -0.1176, -0.8738]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0647 cost = 0.002816\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.5351e-01, -6.2661e-01, -7.0273e-01,  9.3523e-01, -6.7586e-01],\n",
      "         [-4.3106e-01,  5.6520e-01,  4.7794e-05, -4.2380e-01,  6.2999e-01]],\n",
      "\n",
      "        [[-4.1572e-01, -4.7511e-01,  5.4325e-01, -1.3597e-01, -7.0416e-01],\n",
      "         [ 8.9267e-01,  9.2103e-01, -9.2378e-01,  9.1704e-01,  8.8776e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0647 cost = 0.003426\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-4.3107e-01,  5.6533e-01,  8.7309e-05, -4.2374e-01,  6.3005e-01],\n",
      "         [ 9.5351e-01, -6.2673e-01, -7.0280e-01,  9.3524e-01, -6.7592e-01]],\n",
      "\n",
      "        [[ 8.9269e-01,  9.2104e-01, -9.2379e-01,  9.1705e-01,  8.8779e-01],\n",
      "         [-4.1578e-01, -4.7519e-01,  5.4330e-01, -1.3602e-01, -7.0419e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0648 cost = 0.003225\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8941, -0.8446, -0.9194,  0.8627, -0.1057],\n",
      "         [-0.3796,  0.5981,  0.1304, -0.3646,  0.5894]],\n",
      "\n",
      "        [[-0.7338, -0.4648,  0.7466, -0.1276, -0.9047],\n",
      "         [ 0.9136,  0.9085, -0.9373,  0.9091,  0.8801]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0648 cost = 0.001023\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8655, -0.9444, -0.9523,  0.7555,  0.0128],\n",
      "         [-0.5051,  0.4581,  0.0989, -0.5277,  0.6613]],\n",
      "\n",
      "        [[-0.6947, -0.4663,  0.7015, -0.1179, -0.8738],\n",
      "         [ 0.8550,  0.8501, -0.8843,  0.8379,  0.8706]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0648 cost = 0.001974\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5051,  0.4582,  0.0989, -0.5277,  0.6614],\n",
      "         [ 0.8941, -0.8447, -0.9195,  0.8627, -0.1059]],\n",
      "\n",
      "        [[ 0.8550,  0.8501, -0.8843,  0.8379,  0.8707],\n",
      "         [-0.7339, -0.4649,  0.7467, -0.1278, -0.9047]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0649 cost = 0.001921\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9536, -0.6272, -0.7031,  0.9353, -0.6761],\n",
      "         [ 0.8656, -0.9444, -0.9523,  0.7557,  0.0127]],\n",
      "\n",
      "        [[-0.4160, -0.4755,  0.5435, -0.1363, -0.7043],\n",
      "         [-0.6948, -0.4663,  0.7016, -0.1181, -0.8739]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0649 cost = 0.004082\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.7942e-01,  5.9825e-01,  1.3045e-01, -3.6437e-01,  5.8955e-01],\n",
      "         [-4.3122e-01,  5.6583e-01,  3.4796e-04, -4.2350e-01,  6.3030e-01]],\n",
      "\n",
      "        [[ 9.1359e-01,  9.0849e-01, -9.3735e-01,  9.0908e-01,  8.8020e-01],\n",
      "         [ 8.9274e-01,  9.2108e-01, -9.2379e-01,  9.1711e-01,  8.8788e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0649 cost = 0.002164\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8658, -0.9445, -0.9524,  0.7559,  0.0126],\n",
      "         [ 0.8943, -0.8448, -0.9195,  0.8628, -0.1063]],\n",
      "\n",
      "        [[-0.6949, -0.4665,  0.7016, -0.1183, -0.8739],\n",
      "         [-0.7340, -0.4651,  0.7468, -0.1280, -0.9048]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0650 cost = 0.001565\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5052,  0.4584,  0.0989, -0.5277,  0.6617],\n",
      "         [-0.3794,  0.5983,  0.1305, -0.3643,  0.5896]],\n",
      "\n",
      "        [[ 0.8551,  0.8502, -0.8843,  0.8379,  0.8707],\n",
      "         [ 0.9136,  0.9085, -0.9374,  0.9091,  0.8802]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0650 cost = 0.002201\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.5360e-01, -6.2768e-01, -7.0339e-01,  9.3534e-01, -6.7635e-01],\n",
      "         [-4.3118e-01,  5.6618e-01,  4.5106e-04, -4.2334e-01,  6.3048e-01]],\n",
      "\n",
      "        [[-4.1619e-01, -4.7585e-01,  5.4361e-01, -1.3649e-01, -7.0445e-01],\n",
      "         [ 8.9277e-01,  9.2110e-01, -9.2380e-01,  9.1715e-01,  8.8794e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0650 cost = 0.003391\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-4.3116e-01,  5.6631e-01,  4.9092e-04, -4.2325e-01,  6.3055e-01],\n",
      "         [ 8.9436e-01, -8.4484e-01, -9.1960e-01,  8.6295e-01, -1.0658e-01]],\n",
      "\n",
      "        [[ 8.9278e-01,  9.2111e-01, -9.2380e-01,  9.1717e-01,  8.8796e-01],\n",
      "         [-7.3414e-01, -4.6536e-01,  7.4695e-01, -1.2834e-01, -9.0484e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0651 cost = 0.001764\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9536, -0.6279, -0.7035,  0.9354, -0.6765],\n",
      "         [-0.5050,  0.4587,  0.0989, -0.5275,  0.6619]],\n",
      "\n",
      "        [[-0.4163, -0.4760,  0.5437, -0.1366, -0.7045],\n",
      "         [ 0.8552,  0.8502, -0.8844,  0.8380,  0.8708]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0651 cost = 0.004044\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8660, -0.9446, -0.9524,  0.7564,  0.0123],\n",
      "         [-0.3789,  0.5987,  0.1305, -0.3638,  0.5899]],\n",
      "\n",
      "        [[-0.6950, -0.4667,  0.7018, -0.1187, -0.8740],\n",
      "         [ 0.9136,  0.9085, -0.9374,  0.9091,  0.8803]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0651 cost = 0.000994\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.9445e-01, -8.4493e-01, -9.1967e-01,  8.6304e-01, -1.0690e-01],\n",
      "         [-4.3123e-01,  5.6665e-01,  6.4995e-04, -4.2303e-01,  6.3073e-01]],\n",
      "\n",
      "        [[-7.3425e-01, -4.6552e-01,  7.4707e-01, -1.2861e-01, -9.0488e-01],\n",
      "         [ 8.9281e-01,  9.2114e-01, -9.2381e-01,  9.1721e-01,  8.8801e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0652 cost = 0.001597\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3788,  0.5988,  0.1305, -0.3637,  0.5901],\n",
      "         [ 0.9536, -0.6284, -0.7038,  0.9354, -0.6766]],\n",
      "\n",
      "        [[ 0.9137,  0.9085, -0.9374,  0.9092,  0.8804],\n",
      "         [-0.4164, -0.4762,  0.5438, -0.1368, -0.7046]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0652 cost = 0.002712\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5049,  0.4589,  0.0990, -0.5274,  0.6622],\n",
      "         [ 0.8662, -0.9446, -0.9525,  0.7566,  0.0121]],\n",
      "\n",
      "        [[ 0.8552,  0.8503, -0.8844,  0.8380,  0.8709],\n",
      "         [-0.6951, -0.4668,  0.7019, -0.1189, -0.8740]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0652 cost = 0.002138\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.6623e-01, -9.4461e-01, -9.5249e-01,  7.5672e-01,  1.2037e-02],\n",
      "         [-4.3134e-01,  5.6690e-01,  8.2889e-04, -4.2287e-01,  6.3085e-01]],\n",
      "\n",
      "        [[-6.9510e-01, -4.6688e-01,  7.0194e-01, -1.1897e-01, -8.7400e-01],\n",
      "         [ 8.9285e-01,  9.2117e-01, -9.2382e-01,  9.1724e-01,  8.8806e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0653 cost = 0.001581\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.5989,  0.1305, -0.3635,  0.5902],\n",
      "         [ 0.9537, -0.6288, -0.7040,  0.9354, -0.6768]],\n",
      "\n",
      "        [[ 0.9137,  0.9086, -0.9374,  0.9092,  0.8804],\n",
      "         [-0.4165, -0.4764,  0.5439, -0.1370, -0.7047]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0653 cost = 0.002701\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5050,  0.4590,  0.0990, -0.5274,  0.6623],\n",
      "         [ 0.8946, -0.8451, -0.9198,  0.8632, -0.1074]],\n",
      "\n",
      "        [[ 0.8553,  0.8503, -0.8844,  0.8380,  0.8709],\n",
      "         [-0.7344, -0.4658,  0.7472, -0.1291, -0.9049]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0653 cost = 0.001890\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8664, -0.9447, -0.9525,  0.7570,  0.0119],\n",
      "         [-0.5051,  0.4590,  0.0990, -0.5274,  0.6623]],\n",
      "\n",
      "        [[-0.6952, -0.4670,  0.7020, -0.1192, -0.8740],\n",
      "         [ 0.8553,  0.8503, -0.8844,  0.8380,  0.8710]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0654 cost = 0.001941\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4315,  0.5672,  0.0011, -0.4227,  0.6310],\n",
      "         [ 0.8947, -0.8451, -0.9198,  0.8633, -0.1076]],\n",
      "\n",
      "        [[ 0.8929,  0.9212, -0.9238,  0.9173,  0.8881],\n",
      "         [-0.7345, -0.4659,  0.7473, -0.1292, -0.9050]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0654 cost = 0.001742\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9537, -0.6292, -0.7042,  0.9355, -0.6770],\n",
      "         [-0.3788,  0.5989,  0.1306, -0.3633,  0.5903]],\n",
      "\n",
      "        [[-0.4167, -0.4766,  0.5440, -0.1373, -0.7048],\n",
      "         [ 0.9137,  0.9086, -0.9374,  0.9092,  0.8805]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0654 cost = 0.002671\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5053,  0.4590,  0.0991, -0.5275,  0.6624],\n",
      "         [ 0.8947, -0.8452, -0.9198,  0.8633, -0.1078]],\n",
      "\n",
      "        [[ 0.8553,  0.8504, -0.8844,  0.8381,  0.8710],\n",
      "         [-0.7346, -0.4660,  0.7474, -0.1294, -0.9050]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0655 cost = 0.001881\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9537, -0.6294, -0.7043,  0.9355, -0.6771],\n",
      "         [-0.3789,  0.5989,  0.1306, -0.3632,  0.5903]],\n",
      "\n",
      "        [[-0.4168, -0.4767,  0.5441, -0.1374, -0.7048],\n",
      "         [ 0.9137,  0.9086, -0.9374,  0.9092,  0.8805]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0655 cost = 0.002665\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8666, -0.9447, -0.9526,  0.7575,  0.0116],\n",
      "         [-0.4318,  0.5674,  0.0014, -0.4226,  0.6310]],\n",
      "\n",
      "        [[-0.6954, -0.4671,  0.7022, -0.1196, -0.8741],\n",
      "         [ 0.8929,  0.9212, -0.9238,  0.9173,  0.8882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0655 cost = 0.001567\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9538, -0.6298, -0.7045,  0.9356, -0.6772],\n",
      "         [ 0.8667, -0.9447, -0.9526,  0.7575,  0.0116]],\n",
      "\n",
      "        [[-0.4169, -0.4769,  0.5441, -0.1376, -0.7049],\n",
      "         [-0.6954, -0.4672,  0.7023, -0.1196, -0.8741]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0656 cost = 0.003986\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8949, -0.8453, -0.9199,  0.8635, -0.1083],\n",
      "         [-0.4320,  0.5675,  0.0015, -0.4226,  0.6310]],\n",
      "\n",
      "        [[-0.7347, -0.4662,  0.7476, -0.1298, -0.9050],\n",
      "         [ 0.8929,  0.9212, -0.9238,  0.9173,  0.8882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0656 cost = 0.001573\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5989,  0.1307, -0.3631,  0.5903],\n",
      "         [-0.5056,  0.4590,  0.0991, -0.5276,  0.6625]],\n",
      "\n",
      "        [[ 0.9137,  0.9086, -0.9374,  0.9092,  0.8806],\n",
      "         [ 0.8554,  0.8504, -0.8844,  0.8380,  0.8710]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0656 cost = 0.004869\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5989,  0.1307, -0.3631,  0.5904],\n",
      "         [-0.5056,  0.4591,  0.0991, -0.5276,  0.6626]],\n",
      "\n",
      "        [[ 0.9138,  0.9086, -0.9374,  0.9092,  0.8806],\n",
      "         [ 0.8554,  0.8504, -0.8844,  0.8380,  0.8711]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0657 cost = 0.004865\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8669, -0.9448, -0.9527,  0.7579,  0.0113],\n",
      "         [ 0.8950, -0.8454, -0.9200,  0.8636, -0.1086]],\n",
      "\n",
      "        [[-0.6955, -0.4673,  0.7024, -0.1199, -0.8741],\n",
      "         [-0.7348, -0.4663,  0.7477, -0.1300, -0.9051]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0657 cost = 0.001525\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9538, -0.6305, -0.7048,  0.9357, -0.6775],\n",
      "         [-0.4321,  0.5678,  0.0017, -0.4224,  0.6312]],\n",
      "\n",
      "        [[-0.4171, -0.4772,  0.5443, -0.1379, -0.7050],\n",
      "         [ 0.8930,  0.9213, -0.9239,  0.9174,  0.8883]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0657 cost = 0.003308\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8669, -0.9448, -0.9527,  0.7580,  0.0112],\n",
      "         [-0.4322,  0.5680,  0.0017, -0.4224,  0.6312]],\n",
      "\n",
      "        [[-0.6955, -0.4673,  0.7024, -0.1201, -0.8741],\n",
      "         [ 0.8930,  0.9213, -0.9239,  0.9174,  0.8883]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0658 cost = 0.001555\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5991,  0.1307, -0.3628,  0.5906],\n",
      "         [ 0.8950, -0.8455, -0.9200,  0.8637, -0.1089]],\n",
      "\n",
      "        [[ 0.9138,  0.9087, -0.9374,  0.9093,  0.8808],\n",
      "         [-0.7349, -0.4665,  0.7478, -0.1303, -0.9051]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0658 cost = 0.001281\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9538, -0.6308, -0.7050,  0.9357, -0.6776],\n",
      "         [-0.5057,  0.4595,  0.0991, -0.5275,  0.6630]],\n",
      "\n",
      "        [[-0.4172, -0.4773,  0.5444, -0.1380, -0.7051],\n",
      "         [ 0.8556,  0.8506, -0.8845,  0.8382,  0.8713]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0658 cost = 0.003951\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8670, -0.9448, -0.9527,  0.7582,  0.0111],\n",
      "         [-0.5057,  0.4595,  0.0991, -0.5275,  0.6631]],\n",
      "\n",
      "        [[-0.6956, -0.4674,  0.7025, -0.1203, -0.8741],\n",
      "         [ 0.8556,  0.8506, -0.8845,  0.8382,  0.8713]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0659 cost = 0.001910\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.5992,  0.1307, -0.3627,  0.5907],\n",
      "         [ 0.9538, -0.6311, -0.7052,  0.9357, -0.6777]],\n",
      "\n",
      "        [[ 0.9139,  0.9087, -0.9374,  0.9093,  0.8809],\n",
      "         [-0.4173, -0.4774,  0.5445, -0.1381, -0.7051]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0659 cost = 0.002646\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4324,  0.5684,  0.0020, -0.4222,  0.6314],\n",
      "         [ 0.8951, -0.8456, -0.9201,  0.8638, -0.1093]],\n",
      "\n",
      "        [[ 0.8931,  0.9214, -0.9239,  0.9175,  0.8885],\n",
      "         [-0.7351, -0.4666,  0.7479, -0.1306, -0.9051]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0659 cost = 0.001709\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4325,  0.5684,  0.0021, -0.4222,  0.6315],\n",
      "         [ 0.8952, -0.8456, -0.9201,  0.8638, -0.1095]],\n",
      "\n",
      "        [[ 0.8931,  0.9214, -0.9239,  0.9175,  0.8885],\n",
      "         [-0.7351, -0.4666,  0.7479, -0.1307, -0.9051]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0660 cost = 0.001707\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5992,  0.1308, -0.3626,  0.5908],\n",
      "         [ 0.9539, -0.6314, -0.7053,  0.9357, -0.6779]],\n",
      "\n",
      "        [[ 0.9139,  0.9087, -0.9374,  0.9094,  0.8810],\n",
      "         [-0.4174, -0.4775,  0.5446, -0.1383, -0.7052]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0660 cost = 0.002636\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5059,  0.4596,  0.0992, -0.5276,  0.6634],\n",
      "         [ 0.8672, -0.9449, -0.9528,  0.7585,  0.0108]],\n",
      "\n",
      "        [[ 0.8558,  0.8508, -0.8846,  0.8383,  0.8715],\n",
      "         [-0.6957, -0.4674,  0.7027, -0.1206, -0.8741]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0660 cost = 0.002079\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8953, -0.8457, -0.9202,  0.8639, -0.1098],\n",
      "         [ 0.8673, -0.9449, -0.9528,  0.7586,  0.0108]],\n",
      "\n",
      "        [[-0.7352, -0.4667,  0.7481, -0.1310, -0.9052],\n",
      "         [-0.6957, -0.4675,  0.7027, -0.1207, -0.8741]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0661 cost = 0.002684\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9539, -0.6318, -0.7055,  0.9358, -0.6781],\n",
      "         [-0.4328,  0.5686,  0.0023, -0.4222,  0.6316]],\n",
      "\n",
      "        [[-0.4176, -0.4777,  0.5447, -0.1385, -0.7052],\n",
      "         [ 0.8931,  0.9214, -0.9239,  0.9175,  0.8885]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0661 cost = 0.003267\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5062,  0.4596,  0.0992, -0.5277,  0.6635],\n",
      "         [-0.3797,  0.5991,  0.1308, -0.3625,  0.5909]],\n",
      "\n",
      "        [[ 0.8558,  0.8508, -0.8846,  0.8383,  0.8715],\n",
      "         [ 0.9140,  0.9088, -0.9374,  0.9094,  0.8811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0661 cost = 0.002117\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8954, -0.8458, -0.9203,  0.8641, -0.1102],\n",
      "         [ 0.8675, -0.9449, -0.9529,  0.7590,  0.0105]],\n",
      "\n",
      "        [[-0.7353, -0.4669,  0.7482, -0.1313, -0.9052],\n",
      "         [-0.6958, -0.4676,  0.7028, -0.1210, -0.8742]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0662 cost = 0.002673\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3795,  0.5992,  0.1308, -0.3623,  0.5909],\n",
      "         [-0.5061,  0.4597,  0.0992, -0.5276,  0.6636]],\n",
      "\n",
      "        [[ 0.9140,  0.9088, -0.9374,  0.9094,  0.8811],\n",
      "         [ 0.8558,  0.8508, -0.8846,  0.8384,  0.8716]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0662 cost = 0.004802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9540, -0.6323, -0.7058,  0.9358, -0.6783],\n",
      "         [-0.4328,  0.5690,  0.0026, -0.4220,  0.6317]],\n",
      "\n",
      "        [[-0.4178, -0.4781,  0.5448, -0.1387, -0.7054],\n",
      "         [ 0.8932,  0.9214, -0.9239,  0.9176,  0.8886]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0662 cost = 0.003253\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9540, -0.6325, -0.7059,  0.9358, -0.6783],\n",
      "         [-0.4328,  0.5691,  0.0026, -0.4219,  0.6318]],\n",
      "\n",
      "        [[-0.4178, -0.4781,  0.5449, -0.1388, -0.7054],\n",
      "         [ 0.8932,  0.9215, -0.9239,  0.9176,  0.8886]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0663 cost = 0.003249\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8677, -0.9450, -0.9529,  0.7594,  0.0103],\n",
      "         [-0.5060,  0.4600,  0.0992, -0.5275,  0.6639]],\n",
      "\n",
      "        [[-0.6960, -0.4678,  0.7030, -0.1213, -0.8743],\n",
      "         [ 0.8559,  0.8509, -0.8847,  0.8384,  0.8717]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0663 cost = 0.001883\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8956, -0.8460, -0.9204,  0.8642, -0.1108],\n",
      "         [-0.3793,  0.5994,  0.1308, -0.3619,  0.5911]],\n",
      "\n",
      "        [[-0.7355, -0.4673,  0.7484, -0.1317, -0.9053],\n",
      "         [ 0.9140,  0.9088, -0.9375,  0.9095,  0.8812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0663 cost = 0.000969\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5059,  0.4602,  0.0992, -0.5274,  0.6640],\n",
      "         [ 0.9540, -0.6329, -0.7061,  0.9359, -0.6785]],\n",
      "\n",
      "        [[ 0.8560,  0.8510, -0.8847,  0.8385,  0.8717],\n",
      "         [-0.4180, -0.4783,  0.5450, -0.1390, -0.7055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0664 cost = 0.003257\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8679, -0.9451, -0.9530,  0.7597,  0.0101],\n",
      "         [-0.4328,  0.5695,  0.0028, -0.4217,  0.6319]],\n",
      "\n",
      "        [[-0.6961, -0.4679,  0.7031, -0.1215, -0.8743],\n",
      "         [ 0.8932,  0.9215, -0.9239,  0.9177,  0.8887]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0664 cost = 0.001520\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8956, -0.8461, -0.9204,  0.8643, -0.1111],\n",
      "         [-0.3792,  0.5995,  0.1309, -0.3617,  0.5912]],\n",
      "\n",
      "        [[-0.7356, -0.4674,  0.7485, -0.1320, -0.9053],\n",
      "         [ 0.9141,  0.9088, -0.9375,  0.9095,  0.8813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0664 cost = 0.000965\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.5996,  0.1309, -0.3616,  0.5913],\n",
      "         [ 0.9540, -0.6332, -0.7063,  0.9359, -0.6787]],\n",
      "\n",
      "        [[ 0.9141,  0.9088, -0.9375,  0.9095,  0.8813],\n",
      "         [-0.4181, -0.4785,  0.5451, -0.1392, -0.7055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0665 cost = 0.002592\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8680, -0.9451, -0.9530,  0.7599,  0.0099],\n",
      "         [ 0.8957, -0.8462, -0.9205,  0.8644, -0.1114]],\n",
      "\n",
      "        [[-0.6961, -0.4680,  0.7032, -0.1217, -0.8743],\n",
      "         [-0.7357, -0.4675,  0.7486, -0.1322, -0.9053]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0665 cost = 0.001484\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4329,  0.5698,  0.0030, -0.4216,  0.6320],\n",
      "         [-0.5059,  0.4603,  0.0993, -0.5273,  0.6642]],\n",
      "\n",
      "        [[ 0.8933,  0.9215, -0.9239,  0.9177,  0.8888],\n",
      "         [ 0.8561,  0.8511, -0.8847,  0.8385,  0.8718]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0665 cost = 0.004943\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8681, -0.9451, -0.9531,  0.7601,  0.0098],\n",
      "         [-0.3792,  0.5996,  0.1309, -0.3615,  0.5913]],\n",
      "\n",
      "        [[-0.6962, -0.4681,  0.7033, -0.1219, -0.8743],\n",
      "         [ 0.9141,  0.9089, -0.9375,  0.9095,  0.8814]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0666 cost = 0.000945\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4330,  0.5699,  0.0032, -0.4215,  0.6321],\n",
      "         [-0.5060,  0.4604,  0.0992, -0.5273,  0.6644]],\n",
      "\n",
      "        [[ 0.8933,  0.9216, -0.9239,  0.9177,  0.8889],\n",
      "         [ 0.8561,  0.8511, -0.8848,  0.8386,  0.8719]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0666 cost = 0.004933\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9541, -0.6335, -0.7066,  0.9360, -0.6789],\n",
      "         [ 0.8958, -0.8462, -0.9205,  0.8645, -0.1117]],\n",
      "\n",
      "        [[-0.4184, -0.4788,  0.5453, -0.1394, -0.7057],\n",
      "         [-0.7359, -0.4678,  0.7487, -0.1325, -0.9054]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0666 cost = 0.003134\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4332,  0.5702,  0.0033, -0.4214,  0.6322],\n",
      "         [ 0.8682, -0.9451, -0.9531,  0.7602,  0.0097]],\n",
      "\n",
      "        [[ 0.8934,  0.9216, -0.9240,  0.9178,  0.8889],\n",
      "         [-0.6963, -0.4681,  0.7034, -0.1221, -0.8743]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0667 cost = 0.001785\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8958, -0.8463, -0.9206,  0.8645, -0.1118],\n",
      "         [ 0.9541, -0.6336, -0.7067,  0.9360, -0.6789]],\n",
      "\n",
      "        [[-0.7359, -0.4679,  0.7488, -0.1327, -0.9054],\n",
      "         [-0.4184, -0.4789,  0.5453, -0.1395, -0.7057]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0667 cost = 0.003204\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.5998,  0.1309, -0.3614,  0.5915],\n",
      "         [-0.5063,  0.4606,  0.0992, -0.5274,  0.6648]],\n",
      "\n",
      "        [[ 0.9142,  0.9089, -0.9375,  0.9096,  0.8816],\n",
      "         [ 0.8563,  0.8513, -0.8849,  0.8387,  0.8721]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0667 cost = 0.004734\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4335,  0.5705,  0.0034, -0.4213,  0.6323],\n",
      "         [-0.3794,  0.5998,  0.1309, -0.3613,  0.5915]],\n",
      "\n",
      "        [[ 0.8934,  0.9216, -0.9240,  0.9178,  0.8891],\n",
      "         [ 0.9142,  0.9089, -0.9375,  0.9096,  0.8817]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0668 cost = 0.001480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8959, -0.8463, -0.9206,  0.8646, -0.1121],\n",
      "         [ 0.9541, -0.6338, -0.7069,  0.9360, -0.6790]],\n",
      "\n",
      "        [[-0.7360, -0.4680,  0.7489, -0.1329, -0.9054],\n",
      "         [-0.4186, -0.4792,  0.5455, -0.1396, -0.7058]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0668 cost = 0.003196\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5065,  0.4609,  0.0991, -0.5274,  0.6652],\n",
      "         [ 0.8683, -0.9452, -0.9531,  0.7604,  0.0095]],\n",
      "\n",
      "        [[ 0.8565,  0.8514, -0.8850,  0.8388,  0.8723],\n",
      "         [-0.6963, -0.4683,  0.7035, -0.1224, -0.8743]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0668 cost = 0.002016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9541, -0.6339, -0.7070,  0.9360, -0.6791],\n",
      "         [ 0.8959, -0.8464, -0.9207,  0.8646, -0.1122]],\n",
      "\n",
      "        [[-0.4187, -0.4793,  0.5456, -0.1396, -0.7058],\n",
      "         [-0.7361, -0.4682,  0.7490, -0.1331, -0.9055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0669 cost = 0.003115\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5066,  0.4611,  0.0991, -0.5274,  0.6653],\n",
      "         [-0.4339,  0.5710,  0.0036, -0.4211,  0.6326]],\n",
      "\n",
      "        [[ 0.8565,  0.8515, -0.8850,  0.8389,  0.8724],\n",
      "         [ 0.8935,  0.9217, -0.9240,  0.9179,  0.8892]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0669 cost = 0.002976\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8684, -0.9452, -0.9532,  0.7606,  0.0093],\n",
      "         [-0.3795,  0.6001,  0.1309, -0.3611,  0.5917]],\n",
      "\n",
      "        [[-0.6964, -0.4685,  0.7036, -0.1226, -0.8744],\n",
      "         [ 0.9143,  0.9090, -0.9376,  0.9097,  0.8819]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0669 cost = 0.000933\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5066,  0.4613,  0.0991, -0.5273,  0.6655],\n",
      "         [ 0.8684, -0.9452, -0.9532,  0.7607,  0.0093]],\n",
      "\n",
      "        [[ 0.8566,  0.8516, -0.8851,  0.8390,  0.8724],\n",
      "         [-0.6964, -0.4685,  0.7036, -0.1227, -0.8744]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0670 cost = 0.002005\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4340,  0.5715,  0.0036, -0.4208,  0.6327],\n",
      "         [-0.3795,  0.6003,  0.1309, -0.3609,  0.5918]],\n",
      "\n",
      "        [[ 0.8936,  0.9217, -0.9240,  0.9179,  0.8893],\n",
      "         [ 0.9143,  0.9090, -0.9376,  0.9097,  0.8819]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0670 cost = 0.001464\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9541, -0.6342, -0.7073,  0.9360, -0.6792],\n",
      "         [ 0.8961, -0.8464, -0.9208,  0.8647, -0.1126]],\n",
      "\n",
      "        [[-0.4190, -0.4797,  0.5458, -0.1398, -0.7060],\n",
      "         [-0.7363, -0.4685,  0.7492, -0.1335, -0.9055]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0670 cost = 0.003100\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8961, -0.8464, -0.9208,  0.8648, -0.1127],\n",
      "         [-0.4341,  0.5718,  0.0037, -0.4207,  0.6328]],\n",
      "\n",
      "        [[-0.7363, -0.4686,  0.7492, -0.1335, -0.9055],\n",
      "         [ 0.8936,  0.9218, -0.9240,  0.9180,  0.8893]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0671 cost = 0.001491\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3795,  0.6004,  0.1309, -0.3608,  0.5919],\n",
      "         [ 0.8686, -0.9453, -0.9532,  0.7610,  0.0091]],\n",
      "\n",
      "        [[ 0.9144,  0.9091, -0.9376,  0.9098,  0.8820],\n",
      "         [-0.6965, -0.4687,  0.7037, -0.1229, -0.8744]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0671 cost = 0.001356\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9541, -0.6345, -0.7075,  0.9360, -0.6793],\n",
      "         [-0.5069,  0.4617,  0.0991, -0.5271,  0.6659]],\n",
      "\n",
      "        [[-0.4191, -0.4799,  0.5459, -0.1399, -0.7060],\n",
      "         [ 0.8568,  0.8517, -0.8852,  0.8391,  0.8726]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0671 cost = 0.003796\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5069,  0.4617,  0.0991, -0.5271,  0.6659],\n",
      "         [ 0.8687, -0.9453, -0.9533,  0.7611,  0.0090]],\n",
      "\n",
      "        [[ 0.8568,  0.8517, -0.8852,  0.8391,  0.8727],\n",
      "         [-0.6965, -0.4688,  0.7038, -0.1231, -0.8744]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0672 cost = 0.001989\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9541, -0.6347, -0.7076,  0.9360, -0.6794],\n",
      "         [-0.3796,  0.6005,  0.1310, -0.3606,  0.5919]],\n",
      "\n",
      "        [[-0.4192, -0.4800,  0.5459, -0.1400, -0.7061],\n",
      "         [ 0.9144,  0.9091, -0.9376,  0.9098,  0.8821]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0672 cost = 0.002522\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4344,  0.5722,  0.0039, -0.4204,  0.6330],\n",
      "         [ 0.8963, -0.8466, -0.9209,  0.8649, -0.1132]],\n",
      "\n",
      "        [[ 0.8937,  0.9218, -0.9241,  0.9180,  0.8895],\n",
      "         [-0.7365, -0.4689,  0.7494, -0.1339, -0.9056]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0672 cost = 0.001626\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8689, -0.9453, -0.9533,  0.7615,  0.0089],\n",
      "         [ 0.9542, -0.6350, -0.7078,  0.9361, -0.6795]],\n",
      "\n",
      "        [[-0.6966, -0.4689,  0.7039, -0.1233, -0.8745],\n",
      "         [-0.4192, -0.4801,  0.5460, -0.1401, -0.7061]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0673 cost = 0.002957\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8963, -0.8466, -0.9209,  0.8650, -0.1134],\n",
      "         [-0.3797,  0.6006,  0.1310, -0.3605,  0.5919]],\n",
      "\n",
      "        [[-0.7366, -0.4690,  0.7495, -0.1341, -0.9056],\n",
      "         [ 0.9144,  0.9091, -0.9376,  0.9098,  0.8822]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0673 cost = 0.000936\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5072,  0.4619,  0.0991, -0.5271,  0.6661],\n",
      "         [-0.4346,  0.5725,  0.0041, -0.4204,  0.6330]],\n",
      "\n",
      "        [[ 0.8569,  0.8518, -0.8852,  0.8392,  0.8727],\n",
      "         [ 0.8937,  0.9218, -0.9241,  0.9181,  0.8895]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0673 cost = 0.002929\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5072,  0.4619,  0.0991, -0.5271,  0.6661],\n",
      "         [ 0.8690, -0.9454, -0.9534,  0.7618,  0.0087]],\n",
      "\n",
      "        [[ 0.8569,  0.8518, -0.8852,  0.8392,  0.8728],\n",
      "         [-0.6967, -0.4690,  0.7039, -0.1235, -0.8745]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0674 cost = 0.001974\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9542, -0.6355, -0.7081,  0.9361, -0.6797],\n",
      "         [ 0.8964, -0.8467, -0.9210,  0.8651, -0.1137]],\n",
      "\n",
      "        [[-0.4194, -0.4804,  0.5461, -0.1404, -0.7062],\n",
      "         [-0.7367, -0.4692,  0.7496, -0.1344, -0.9057]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0674 cost = 0.003062\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4347,  0.5728,  0.0042, -0.4201,  0.6331],\n",
      "         [-0.3797,  0.6007,  0.1311, -0.3602,  0.5920]],\n",
      "\n",
      "        [[ 0.8937,  0.9219, -0.9241,  0.9181,  0.8895],\n",
      "         [ 0.9145,  0.9091, -0.9376,  0.9099,  0.8822]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0674 cost = 0.001440\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4347,  0.5729,  0.0042, -0.4200,  0.6331],\n",
      "         [-0.5074,  0.4621,  0.0991, -0.5270,  0.6662]],\n",
      "\n",
      "        [[ 0.8937,  0.9219, -0.9241,  0.9181,  0.8896],\n",
      "         [ 0.8570,  0.8519, -0.8853,  0.8392,  0.8728]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0675 cost = 0.004810\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8692, -0.9454, -0.9534,  0.7621,  0.0085],\n",
      "         [ 0.9542, -0.6358, -0.7083,  0.9361, -0.6799]],\n",
      "\n",
      "        [[-0.6968, -0.4692,  0.7040, -0.1237, -0.8745],\n",
      "         [-0.4195, -0.4806,  0.5462, -0.1405, -0.7063]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0675 cost = 0.002934\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3798,  0.6009,  0.1311, -0.3601,  0.5920],\n",
      "         [ 0.8965, -0.8468, -0.9211,  0.8652, -0.1141]],\n",
      "\n",
      "        [[ 0.9145,  0.9091, -0.9376,  0.9099,  0.8823],\n",
      "         [-0.7368, -0.4695,  0.7497, -0.1347, -0.9057]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0675 cost = 0.001206\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5076,  0.4623,  0.0991, -0.5269,  0.6664],\n",
      "         [ 0.8693, -0.9454, -0.9534,  0.7622,  0.0085]],\n",
      "\n",
      "        [[ 0.8571,  0.8520, -0.8853,  0.8393,  0.8730],\n",
      "         [-0.6968, -0.4693,  0.7041, -0.1238, -0.8745]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0676 cost = 0.001958\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4351,  0.5734,  0.0044, -0.4198,  0.6333],\n",
      "         [ 0.9542, -0.6361, -0.7085,  0.9362, -0.6800]],\n",
      "\n",
      "        [[ 0.8938,  0.9219, -0.9241,  0.9182,  0.8897],\n",
      "         [-0.4197, -0.4808,  0.5463, -0.1406, -0.7064]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0676 cost = 0.002913\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8966, -0.8468, -0.9211,  0.8652, -0.1143],\n",
      "         [-0.3799,  0.6010,  0.1311, -0.3599,  0.5921]],\n",
      "\n",
      "        [[-0.7369, -0.4697,  0.7498, -0.1349, -0.9058],\n",
      "         [ 0.9146,  0.9092, -0.9376,  0.9099,  0.8824]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0676 cost = 0.000925\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8966, -0.8468, -0.9211,  0.8653, -0.1144],\n",
      "         [ 0.8694, -0.9455, -0.9535,  0.7625,  0.0083]],\n",
      "\n",
      "        [[-0.7369, -0.4697,  0.7499, -0.1350, -0.9058],\n",
      "         [-0.6969, -0.4694,  0.7042, -0.1240, -0.8746]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0677 cost = 0.002542\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3800,  0.6010,  0.1311, -0.3599,  0.5921],\n",
      "         [-0.5079,  0.4625,  0.0991, -0.5270,  0.6666]],\n",
      "\n",
      "        [[ 0.9146,  0.9092, -0.9376,  0.9099,  0.8824],\n",
      "         [ 0.8572,  0.8520, -0.8854,  0.8394,  0.8731]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0677 cost = 0.004618\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4354,  0.5738,  0.0046, -0.4196,  0.6334],\n",
      "         [ 0.9543, -0.6364, -0.7087,  0.9362, -0.6801]],\n",
      "\n",
      "        [[ 0.8939,  0.9220, -0.9241,  0.9183,  0.8898],\n",
      "         [-0.4199, -0.4812,  0.5465, -0.1408, -0.7065]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0677 cost = 0.002899\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8967, -0.8469, -0.9212,  0.8654, -0.1147],\n",
      "         [ 0.9543, -0.6364, -0.7088,  0.9362, -0.6801]],\n",
      "\n",
      "        [[-0.7370, -0.4700,  0.7500, -0.1352, -0.9058],\n",
      "         [-0.4199, -0.4812,  0.5465, -0.1408, -0.7065]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0678 cost = 0.003101\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5081,  0.4627,  0.0991, -0.5269,  0.6668],\n",
      "         [-0.4355,  0.5741,  0.0046, -0.4195,  0.6335]],\n",
      "\n",
      "        [[ 0.8573,  0.8521, -0.8854,  0.8395,  0.8732],\n",
      "         [ 0.8939,  0.9220, -0.9242,  0.9183,  0.8898]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0678 cost = 0.002877\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.6012,  0.1311, -0.3596,  0.5922],\n",
      "         [ 0.8697, -0.9455, -0.9536,  0.7630,  0.0080]],\n",
      "\n",
      "        [[ 0.9146,  0.9092, -0.9377,  0.9100,  0.8826],\n",
      "         [-0.6970, -0.4698,  0.7043, -0.1244, -0.8747]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0678 cost = 0.001319\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8968, -0.8469, -0.9213,  0.8655, -0.1150],\n",
      "         [ 0.8697, -0.9456, -0.9536,  0.7631,  0.0080]],\n",
      "\n",
      "        [[-0.7372, -0.4703,  0.7501, -0.1355, -0.9059],\n",
      "         [-0.6970, -0.4699,  0.7044, -0.1245, -0.8747]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0679 cost = 0.002522\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4355,  0.5745,  0.0047, -0.4193,  0.6336],\n",
      "         [-0.3801,  0.6013,  0.1311, -0.3595,  0.5922]],\n",
      "\n",
      "        [[ 0.8939,  0.9220, -0.9242,  0.9183,  0.8899],\n",
      "         [ 0.9147,  0.9092, -0.9377,  0.9100,  0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0679 cost = 0.001413\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9543, -0.6367, -0.7091,  0.9362, -0.6803],\n",
      "         [-0.5081,  0.4630,  0.0991, -0.5268,  0.6671]],\n",
      "\n",
      "        [[-0.4202, -0.4817,  0.5468, -0.1410, -0.7067],\n",
      "         [ 0.8574,  0.8522, -0.8855,  0.8396,  0.8733]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0679 cost = 0.003707\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9543, -0.6368, -0.7091,  0.9363, -0.6804],\n",
      "         [-0.4355,  0.5748,  0.0048, -0.4192,  0.6337]],\n",
      "\n",
      "        [[-0.4203, -0.4818,  0.5468, -0.1410, -0.7067],\n",
      "         [ 0.8940,  0.9220, -0.9242,  0.9184,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0680 cost = 0.003078\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.6015,  0.1311, -0.3593,  0.5923],\n",
      "         [ 0.8700, -0.9456, -0.9537,  0.7635,  0.0077]],\n",
      "\n",
      "        [[ 0.9147,  0.9092, -0.9377,  0.9101,  0.8827],\n",
      "         [-0.6971, -0.4702,  0.7045, -0.1248, -0.8748]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0680 cost = 0.001309\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8970, -0.8471, -0.9214,  0.8656, -0.1155],\n",
      "         [-0.5082,  0.4632,  0.0991, -0.5267,  0.6673]],\n",
      "\n",
      "        [[-0.7374, -0.4708,  0.7503, -0.1359, -0.9059],\n",
      "         [ 0.8575,  0.8523, -0.8855,  0.8397,  0.8734]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0680 cost = 0.001730\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3802,  0.6015,  0.1311, -0.3592,  0.5924],\n",
      "         [ 0.8701, -0.9457, -0.9537,  0.7638,  0.0076]],\n",
      "\n",
      "        [[ 0.9147,  0.9092, -0.9377,  0.9101,  0.8827],\n",
      "         [-0.6971, -0.4703,  0.7046, -0.1250, -0.8748]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0681 cost = 0.001306\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8971, -0.8471, -0.9214,  0.8657, -0.1157],\n",
      "         [-0.4357,  0.5751,  0.0050, -0.4190,  0.6339]],\n",
      "\n",
      "        [[-0.7374, -0.4709,  0.7504, -0.1361, -0.9060],\n",
      "         [ 0.8940,  0.9220, -0.9242,  0.9184,  0.8900]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0681 cost = 0.001437\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9544, -0.6374, -0.7095,  0.9363, -0.6806],\n",
      "         [-0.5083,  0.4633,  0.0991, -0.5268,  0.6675]],\n",
      "\n",
      "        [[-0.4205, -0.4822,  0.5470, -0.1413, -0.7069],\n",
      "         [ 0.8576,  0.8523, -0.8856,  0.8397,  0.8735]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0681 cost = 0.003685\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4358,  0.5753,  0.0051, -0.4190,  0.6339],\n",
      "         [-0.3803,  0.6015,  0.1312, -0.3591,  0.5924]],\n",
      "\n",
      "        [[ 0.8940,  0.9221, -0.9242,  0.9184,  0.8900],\n",
      "         [ 0.9147,  0.9093, -0.9377,  0.9101,  0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0682 cost = 0.001399\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9544, -0.6377, -0.7096,  0.9364, -0.6807],\n",
      "         [ 0.8972, -0.8472, -0.9215,  0.8658, -0.1160]],\n",
      "\n",
      "        [[-0.4206, -0.4823,  0.5471, -0.1414, -0.7069],\n",
      "         [-0.7375, -0.4710,  0.7505, -0.1364, -0.9060]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0682 cost = 0.002986\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8704, -0.9458, -0.9538,  0.7643,  0.0073],\n",
      "         [-0.5085,  0.4633,  0.0991, -0.5268,  0.6676]],\n",
      "\n",
      "        [[-0.6973, -0.4705,  0.7047, -0.1254, -0.8749],\n",
      "         [ 0.8576,  0.8524, -0.8856,  0.8397,  0.8735]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0682 cost = 0.001764\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8972, -0.8473, -0.9215,  0.8659, -0.1163],\n",
      "         [-0.4360,  0.5755,  0.0053, -0.4189,  0.6340]],\n",
      "\n",
      "        [[-0.7376, -0.4711,  0.7506, -0.1366, -0.9060],\n",
      "         [ 0.8941,  0.9221, -0.9242,  0.9185,  0.8901]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0683 cost = 0.001429\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3804,  0.6016,  0.1312, -0.3590,  0.5926],\n",
      "         [-0.5086,  0.4634,  0.0991, -0.5269,  0.6678]],\n",
      "\n",
      "        [[ 0.9148,  0.9093, -0.9377,  0.9101,  0.8829],\n",
      "         [ 0.8576,  0.8524, -0.8856,  0.8398,  0.8736]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0683 cost = 0.004554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9544, -0.6382, -0.7098,  0.9364, -0.6809],\n",
      "         [ 0.8705, -0.9458, -0.9538,  0.7645,  0.0071]],\n",
      "\n",
      "        [[-0.4208, -0.4824,  0.5472, -0.1417, -0.7070],\n",
      "         [-0.6974, -0.4705,  0.7049, -0.1256, -0.8749]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0683 cost = 0.003643\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4362,  0.5757,  0.0054, -0.4188,  0.6341],\n",
      "         [-0.3804,  0.6017,  0.1312, -0.3589,  0.5927]],\n",
      "\n",
      "        [[ 0.8941,  0.9221, -0.9242,  0.9185,  0.8901],\n",
      "         [ 0.9148,  0.9093, -0.9377,  0.9102,  0.8830]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0684 cost = 0.001388\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8706, -0.9458, -0.9538,  0.7647,  0.0070],\n",
      "         [ 0.8973, -0.8475, -0.9216,  0.8660, -0.1167]],\n",
      "\n",
      "        [[-0.6974, -0.4705,  0.7049, -0.1258, -0.8749],\n",
      "         [-0.7378, -0.4712,  0.7508, -0.1369, -0.9061]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0684 cost = 0.001391\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5087,  0.4636,  0.0991, -0.5269,  0.6682],\n",
      "         [ 0.9544, -0.6386, -0.7100,  0.9364, -0.6811]],\n",
      "\n",
      "        [[ 0.8578,  0.8525, -0.8857,  0.8399,  0.8737],\n",
      "         [-0.4209, -0.4826,  0.5473, -0.1418, -0.7071]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0684 cost = 0.003038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9544, -0.6387, -0.7101,  0.9364, -0.6811],\n",
      "         [-0.5088,  0.4637,  0.0991, -0.5269,  0.6682]],\n",
      "\n",
      "        [[-0.4209, -0.4827,  0.5474, -0.1419, -0.7071],\n",
      "         [ 0.8578,  0.8526, -0.8857,  0.8399,  0.8737]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0685 cost = 0.003647\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8974, -0.8476, -0.9217,  0.8660, -0.1170],\n",
      "         [-0.4364,  0.5760,  0.0057, -0.4186,  0.6343]],\n",
      "\n",
      "        [[-0.7379, -0.4714,  0.7509, -0.1372, -0.9061],\n",
      "         [ 0.8941,  0.9222, -0.9242,  0.9185,  0.8902]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0685 cost = 0.001417\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8708, -0.9459, -0.9539,  0.7650,  0.0067],\n",
      "         [-0.3805,  0.6018,  0.1313, -0.3587,  0.5929]],\n",
      "\n",
      "        [[-0.6975, -0.4707,  0.7051, -0.1261, -0.8749],\n",
      "         [ 0.9149,  0.9094, -0.9377,  0.9102,  0.8831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0685 cost = 0.000881\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5089,  0.4637,  0.0992, -0.5269,  0.6684],\n",
      "         [-0.3805,  0.6018,  0.1313, -0.3586,  0.5929]],\n",
      "\n",
      "        [[ 0.8579,  0.8526, -0.8857,  0.8399,  0.8738],\n",
      "         [ 0.9149,  0.9094, -0.9377,  0.9102,  0.8831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0686 cost = 0.001922\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9545, -0.6391, -0.7103,  0.9365, -0.6813],\n",
      "         [-0.4365,  0.5763,  0.0058, -0.4184,  0.6344]],\n",
      "\n",
      "        [[-0.4211, -0.4829,  0.5475, -0.1421, -0.7072],\n",
      "         [ 0.8942,  0.9222, -0.9242,  0.9186,  0.8903]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0686 cost = 0.003013\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8975, -0.8477, -0.9217,  0.8661, -0.1174],\n",
      "         [ 0.8709, -0.9459, -0.9539,  0.7652,  0.0066]],\n",
      "\n",
      "        [[-0.7380, -0.4716,  0.7510, -0.1376, -0.9062],\n",
      "         [-0.6976, -0.4707,  0.7052, -0.1263, -0.8750]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0686 cost = 0.002453\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9545, -0.6394, -0.7104,  0.9365, -0.6814],\n",
      "         [ 0.8709, -0.9459, -0.9540,  0.7653,  0.0065]],\n",
      "\n",
      "        [[-0.4212, -0.4830,  0.5476, -0.1422, -0.7072],\n",
      "         [-0.6976, -0.4708,  0.7052, -0.1264, -0.8750]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0687 cost = 0.003603\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4365,  0.5765,  0.0060, -0.4182,  0.6346],\n",
      "         [ 0.8976, -0.8478, -0.9218,  0.8662, -0.1176]],\n",
      "\n",
      "        [[ 0.8942,  0.9222, -0.9242,  0.9186,  0.8903],\n",
      "         [-0.7381, -0.4717,  0.7511, -0.1378, -0.9062]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0687 cost = 0.001539\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3800,  0.6022,  0.1313, -0.3582,  0.5932],\n",
      "         [-0.5087,  0.4641,  0.0992, -0.5266,  0.6688]],\n",
      "\n",
      "        [[ 0.9149,  0.9094, -0.9377,  0.9103,  0.8832],\n",
      "         [ 0.8579,  0.8527, -0.8857,  0.8400,  0.8739]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0687 cost = 0.004511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5086,  0.4642,  0.0992, -0.5266,  0.6689],\n",
      "         [-0.4365,  0.5767,  0.0061, -0.4181,  0.6347]],\n",
      "\n",
      "        [[ 0.8580,  0.8527, -0.8857,  0.8400,  0.8739],\n",
      "         [ 0.8942,  0.9222, -0.9242,  0.9186,  0.8904]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0688 cost = 0.002785\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3798,  0.6024,  0.1313, -0.3579,  0.5933],\n",
      "         [ 0.8712, -0.9460, -0.9540,  0.7657,  0.0062]],\n",
      "\n",
      "        [[ 0.9149,  0.9094, -0.9377,  0.9103,  0.8833],\n",
      "         [-0.6977, -0.4710,  0.7053, -0.1267, -0.8750]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0688 cost = 0.001270\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9545, -0.6400, -0.7108,  0.9365, -0.6816],\n",
      "         [ 0.8977, -0.8479, -0.9219,  0.8663, -0.1180]],\n",
      "\n",
      "        [[-0.4214, -0.4833,  0.5478, -0.1425, -0.7073],\n",
      "         [-0.7383, -0.4720,  0.7512, -0.1381, -0.9062]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0688 cost = 0.002924\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4364,  0.5772,  0.0061, -0.4178,  0.6349],\n",
      "         [-0.3797,  0.6025,  0.1313, -0.3577,  0.5935]],\n",
      "\n",
      "        [[ 0.8943,  0.9223, -0.9243,  0.9187,  0.8904],\n",
      "         [ 0.9150,  0.9094, -0.9378,  0.9103,  0.8834]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0689 cost = 0.001362\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5085,  0.4647,  0.0992, -0.5262,  0.6693],\n",
      "         [ 0.8977, -0.8479, -0.9219,  0.8664, -0.1182]],\n",
      "\n",
      "        [[ 0.8581,  0.8529, -0.8858,  0.8401,  0.8741],\n",
      "         [-0.7383, -0.4721,  0.7513, -0.1383, -0.9063]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0689 cost = 0.001662\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9545, -0.6403, -0.7110,  0.9365, -0.6818],\n",
      "         [ 0.8713, -0.9460, -0.9541,  0.7661,  0.0060]],\n",
      "\n",
      "        [[-0.4215, -0.4835,  0.5479, -0.1426, -0.7074],\n",
      "         [-0.6978, -0.4712,  0.7054, -0.1270, -0.8751]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0689 cost = 0.003569\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8978, -0.8480, -0.9220,  0.8664, -0.1184],\n",
      "         [ 0.9546, -0.6405, -0.7111,  0.9366, -0.6818]],\n",
      "\n",
      "        [[-0.7384, -0.4722,  0.7514, -0.1384, -0.9063],\n",
      "         [-0.4216, -0.4836,  0.5479, -0.1426, -0.7074]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0690 cost = 0.002984\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8715, -0.9461, -0.9541,  0.7663,  0.0058],\n",
      "         [-0.3795,  0.6028,  0.1313, -0.3573,  0.5937]],\n",
      "\n",
      "        [[-0.6979, -0.4713,  0.7055, -0.1272, -0.8751],\n",
      "         [ 0.9150,  0.9095, -0.9378,  0.9104,  0.8835]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0690 cost = 0.000866\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4363,  0.5778,  0.0063, -0.4175,  0.6351],\n",
      "         [-0.5084,  0.4651,  0.0992, -0.5261,  0.6695]],\n",
      "\n",
      "        [[ 0.8943,  0.9223, -0.9243,  0.9187,  0.8905],\n",
      "         [ 0.8582,  0.8529, -0.8859,  0.8402,  0.8742]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0690 cost = 0.004626\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8979, -0.8481, -0.9220,  0.8665, -0.1188],\n",
      "         [ 0.8716, -0.9461, -0.9542,  0.7665,  0.0057]],\n",
      "\n",
      "        [[-0.7385, -0.4725,  0.7515, -0.1387, -0.9063],\n",
      "         [-0.6979, -0.4714,  0.7055, -0.1273, -0.8751]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0691 cost = 0.002411\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5084,  0.4653,  0.0991, -0.5260,  0.6697],\n",
      "         [-0.3794,  0.6029,  0.1313, -0.3571,  0.5938]],\n",
      "\n",
      "        [[ 0.8582,  0.8530, -0.8859,  0.8402,  0.8742],\n",
      "         [ 0.9151,  0.9095, -0.9378,  0.9104,  0.8835]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0691 cost = 0.001880\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4363,  0.5782,  0.0063, -0.4173,  0.6353],\n",
      "         [ 0.9546, -0.6411, -0.7115,  0.9366, -0.6821]],\n",
      "\n",
      "        [[ 0.8944,  0.9223, -0.9243,  0.9188,  0.8906],\n",
      "         [-0.4218, -0.4840,  0.5481, -0.1428, -0.7075]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0691 cost = 0.002761\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9546, -0.6412, -0.7116,  0.9366, -0.6821],\n",
      "         [ 0.8980, -0.8481, -0.9221,  0.8666, -0.1191]],\n",
      "\n",
      "        [[-0.4219, -0.4841,  0.5481, -0.1429, -0.7076],\n",
      "         [-0.7386, -0.4728,  0.7516, -0.1389, -0.9064]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0692 cost = 0.002892\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8718, -0.9462, -0.9543,  0.7669,  0.0054],\n",
      "         [-0.4362,  0.5785,  0.0064, -0.4171,  0.6355]],\n",
      "\n",
      "        [[-0.6980, -0.4717,  0.7057, -0.1276, -0.8752],\n",
      "         [ 0.8944,  0.9223, -0.9243,  0.9188,  0.8907]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0692 cost = 0.001374\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3789,  0.6033,  0.1313, -0.3567,  0.5940],\n",
      "         [-0.5082,  0.4658,  0.0991, -0.5258,  0.6700]],\n",
      "\n",
      "        [[ 0.9151,  0.9095, -0.9378,  0.9104,  0.8836],\n",
      "         [ 0.8583,  0.8531, -0.8860,  0.8403,  0.8744]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0692 cost = 0.004451\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5081,  0.4659,  0.0991, -0.5257,  0.6701],\n",
      "         [ 0.9546, -0.6415, -0.7118,  0.9366, -0.6822]],\n",
      "\n",
      "        [[ 0.8584,  0.8531, -0.8860,  0.8403,  0.8744],\n",
      "         [-0.4220, -0.4844,  0.5482, -0.1430, -0.7077]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0693 cost = 0.002954\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4362,  0.5789,  0.0065, -0.4169,  0.6356],\n",
      "         [ 0.8720, -0.9462, -0.9543,  0.7672,  0.0052]],\n",
      "\n",
      "        [[ 0.8944,  0.9224, -0.9243,  0.9188,  0.8907],\n",
      "         [-0.6981, -0.4719,  0.7058, -0.1278, -0.8752]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0693 cost = 0.001605\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.6036,  0.1313, -0.3564,  0.5942],\n",
      "         [ 0.8981, -0.8482, -0.9222,  0.8668, -0.1195]],\n",
      "\n",
      "        [[ 0.9151,  0.9095, -0.9378,  0.9105,  0.8837],\n",
      "         [-0.7388, -0.4732,  0.7518, -0.1394, -0.9065]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0693 cost = 0.001132\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.6037,  0.1313, -0.3563,  0.5942],\n",
      "         [ 0.8721, -0.9462, -0.9543,  0.7673,  0.0051]],\n",
      "\n",
      "        [[ 0.9151,  0.9095, -0.9378,  0.9105,  0.8838],\n",
      "         [-0.6981, -0.4720,  0.7058, -0.1280, -0.8753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0694 cost = 0.001242\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8982, -0.8482, -0.9223,  0.8668, -0.1197],\n",
      "         [ 0.9547, -0.6417, -0.7121,  0.9367, -0.6824]],\n",
      "\n",
      "        [[-0.7389, -0.4733,  0.7519, -0.1395, -0.9065],\n",
      "         [-0.4222, -0.4847,  0.5484, -0.1432, -0.7078]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0694 cost = 0.002942\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5081,  0.4664,  0.0990, -0.5254,  0.6705],\n",
      "         [-0.4363,  0.5794,  0.0067, -0.4166,  0.6358]],\n",
      "\n",
      "        [[ 0.8585,  0.8533, -0.8861,  0.8405,  0.8746],\n",
      "         [ 0.8945,  0.9224, -0.9243,  0.9189,  0.8908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0694 cost = 0.002712\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3786,  0.6038,  0.1313, -0.3561,  0.5944],\n",
      "         [-0.5081,  0.4665,  0.0990, -0.5254,  0.6706]],\n",
      "\n",
      "        [[ 0.9152,  0.9096, -0.9378,  0.9105,  0.8839],\n",
      "         [ 0.8586,  0.8533, -0.8861,  0.8405,  0.8746]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0695 cost = 0.004420\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4363,  0.5797,  0.0067, -0.4164,  0.6360],\n",
      "         [ 0.8722, -0.9463, -0.9544,  0.7677,  0.0049]],\n",
      "\n",
      "        [[ 0.8945,  0.9224, -0.9243,  0.9189,  0.8909],\n",
      "         [-0.6982, -0.4723,  0.7059, -0.1283, -0.8753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0695 cost = 0.001592\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8983, -0.8482, -0.9223,  0.8669, -0.1200],\n",
      "         [ 0.9547, -0.6419, -0.7123,  0.9367, -0.6825]],\n",
      "\n",
      "        [[-0.7390, -0.4737,  0.7520, -0.1398, -0.9065],\n",
      "         [-0.4224, -0.4850,  0.5485, -0.1432, -0.7079]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0695 cost = 0.002931\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5080,  0.4669,  0.0990, -0.5251,  0.6709],\n",
      "         [-0.4363,  0.5801,  0.0067, -0.4163,  0.6361]],\n",
      "\n",
      "        [[ 0.8587,  0.8534, -0.8862,  0.8406,  0.8748],\n",
      "         [ 0.8946,  0.9224, -0.9244,  0.9190,  0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0696 cost = 0.002694\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8984, -0.8482, -0.9224,  0.8669, -0.1201],\n",
      "         [ 0.8724, -0.9463, -0.9544,  0.7679,  0.0048]],\n",
      "\n",
      "        [[-0.7391, -0.4738,  0.7521, -0.1400, -0.9066],\n",
      "         [-0.6982, -0.4724,  0.7060, -0.1285, -0.8753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0696 cost = 0.002364\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9547, -0.6419, -0.7124,  0.9367, -0.6825],\n",
      "         [-0.3783,  0.6043,  0.1312, -0.3554,  0.5946]],\n",
      "\n",
      "        [[-0.4226, -0.4853,  0.5487, -0.1433, -0.7079],\n",
      "         [ 0.9153,  0.9096, -0.9379,  0.9106,  0.8841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0696 cost = 0.002333\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8725, -0.9463, -0.9545,  0.7681,  0.0047],\n",
      "         [-0.3782,  0.6044,  0.1312, -0.3553,  0.5946]],\n",
      "\n",
      "        [[-0.6983, -0.4726,  0.7061, -0.1286, -0.8754],\n",
      "         [ 0.9153,  0.9096, -0.9379,  0.9106,  0.8841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0697 cost = 0.000844\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5078,  0.4674,  0.0989, -0.5247,  0.6712],\n",
      "         [ 0.9547, -0.6421, -0.7126,  0.9367, -0.6826]],\n",
      "\n",
      "        [[ 0.8589,  0.8536, -0.8863,  0.8408,  0.8749],\n",
      "         [-0.4227, -0.4855,  0.5488, -0.1433, -0.7080]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0697 cost = 0.002912\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8985, -0.8483, -0.9225,  0.8671, -0.1205],\n",
      "         [-0.4361,  0.5809,  0.0068, -0.4158,  0.6364]],\n",
      "\n",
      "        [[-0.7392, -0.4742,  0.7523, -0.1403, -0.9066],\n",
      "         [ 0.8946,  0.9225, -0.9244,  0.9191,  0.8911]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0697 cost = 0.001356\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8727, -0.9464, -0.9545,  0.7684,  0.0045],\n",
      "         [-0.4360,  0.5810,  0.0068, -0.4157,  0.6364]],\n",
      "\n",
      "        [[-0.6984, -0.4728,  0.7062, -0.1288, -0.8754],\n",
      "         [ 0.8947,  0.9225, -0.9244,  0.9191,  0.8911]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0698 cost = 0.001346\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8986, -0.8483, -0.9225,  0.8671, -0.1206],\n",
      "         [ 0.9547, -0.6422, -0.7128,  0.9367, -0.6827]],\n",
      "\n",
      "        [[-0.7393, -0.4744,  0.7523, -0.1405, -0.9067],\n",
      "         [-0.4228, -0.4857,  0.5489, -0.1435, -0.7081]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0698 cost = 0.002907\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6046,  0.1312, -0.3548,  0.5947],\n",
      "         [-0.5078,  0.4677,  0.0989, -0.5245,  0.6714]],\n",
      "\n",
      "        [[ 0.9153,  0.9097, -0.9379,  0.9107,  0.8842],\n",
      "         [ 0.8590,  0.8536, -0.8863,  0.8409,  0.8750]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0698 cost = 0.004369\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9548, -0.6423, -0.7129,  0.9367, -0.6828],\n",
      "         [-0.4360,  0.5814,  0.0069, -0.4154,  0.6365]],\n",
      "\n",
      "        [[-0.4229, -0.4859,  0.5490, -0.1436, -0.7082],\n",
      "         [ 0.8947,  0.9225, -0.9244,  0.9191,  0.8912]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0699 cost = 0.002897\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6048,  0.1312, -0.3547,  0.5948],\n",
      "         [ 0.8728, -0.9464, -0.9546,  0.7687,  0.0042]],\n",
      "\n",
      "        [[ 0.9154,  0.9097, -0.9379,  0.9107,  0.8843],\n",
      "         [-0.6984, -0.4730,  0.7063, -0.1291, -0.8755]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0699 cost = 0.001216\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8987, -0.8484, -0.9226,  0.8673, -0.1211],\n",
      "         [-0.5077,  0.4679,  0.0989, -0.5243,  0.6716]],\n",
      "\n",
      "        [[-0.7395, -0.4747,  0.7525, -0.1408, -0.9067],\n",
      "         [ 0.8591,  0.8537, -0.8864,  0.8409,  0.8751]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0699 cost = 0.001620\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3779,  0.6048,  0.1312, -0.3545,  0.5948],\n",
      "         [ 0.8987, -0.8485, -0.9226,  0.8673, -0.1212]],\n",
      "\n",
      "        [[ 0.9154,  0.9097, -0.9379,  0.9108,  0.8844],\n",
      "         [-0.7395, -0.4747,  0.7525, -0.1409, -0.9067]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0700 cost = 0.001107\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5077,  0.4680,  0.0989, -0.5242,  0.6718],\n",
      "         [ 0.9548, -0.6426, -0.7132,  0.9368, -0.6830]],\n",
      "\n",
      "        [[ 0.8591,  0.8538, -0.8864,  0.8410,  0.8752],\n",
      "         [-0.4231, -0.4862,  0.5491, -0.1437, -0.7083]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0700 cost = 0.002882\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8730, -0.9465, -0.9546,  0.7690,  0.0040],\n",
      "         [-0.4361,  0.5819,  0.0070, -0.4152,  0.6367]],\n",
      "\n",
      "        [[-0.6985, -0.4732,  0.7064, -0.1294, -0.8755],\n",
      "         [ 0.8948,  0.9226, -0.9244,  0.9192,  0.8913]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0700 cost = 0.001334\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6049,  0.1312, -0.3543,  0.5949],\n",
      "         [ 0.8988, -0.8486, -0.9227,  0.8674, -0.1215]],\n",
      "\n",
      "        [[ 0.9154,  0.9097, -0.9379,  0.9108,  0.8844],\n",
      "         [-0.7396, -0.4749,  0.7527, -0.1411, -0.9068]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0701 cost = 0.001104\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5078,  0.4681,  0.0989, -0.5241,  0.6719],\n",
      "         [ 0.8731, -0.9465, -0.9546,  0.7691,  0.0039]],\n",
      "\n",
      "        [[ 0.8592,  0.8538, -0.8865,  0.8410,  0.8753],\n",
      "         [-0.6986, -0.4733,  0.7065, -0.1295, -0.8755]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0701 cost = 0.001770\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4362,  0.5821,  0.0071, -0.4151,  0.6367],\n",
      "         [ 0.9548, -0.6429, -0.7134,  0.9368, -0.6831]],\n",
      "\n",
      "        [[ 0.8948,  0.9226, -0.9244,  0.9192,  0.8913],\n",
      "         [-0.4233, -0.4865,  0.5493, -0.1439, -0.7084]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0701 cost = 0.002669\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3781,  0.6049,  0.1313, -0.3542,  0.5949],\n",
      "         [ 0.8732, -0.9465, -0.9547,  0.7693,  0.0038]],\n",
      "\n",
      "        [[ 0.9155,  0.9097, -0.9379,  0.9108,  0.8845],\n",
      "         [-0.6986, -0.4734,  0.7065, -0.1297, -0.8756]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0702 cost = 0.001205\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8989, -0.8486, -0.9228,  0.8675, -0.1219],\n",
      "         [-0.5080,  0.4681,  0.0989, -0.5241,  0.6720]],\n",
      "\n",
      "        [[-0.7398, -0.4751,  0.7528, -0.1415, -0.9068],\n",
      "         [ 0.8593,  0.8539, -0.8865,  0.8411,  0.8753]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0702 cost = 0.001606\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4364,  0.5823,  0.0073, -0.4150,  0.6368],\n",
      "         [ 0.9549, -0.6430, -0.7135,  0.9369, -0.6833]],\n",
      "\n",
      "        [[ 0.8948,  0.9226, -0.9244,  0.9192,  0.8914],\n",
      "         [-0.4235, -0.4867,  0.5494, -0.1441, -0.7085]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0702 cost = 0.002660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8733, -0.9466, -0.9547,  0.7696,  0.0036],\n",
      "         [-0.5082,  0.4681,  0.0990, -0.5242,  0.6720]],\n",
      "\n",
      "        [[-0.6987, -0.4735,  0.7066, -0.1299, -0.8756],\n",
      "         [ 0.8593,  0.8539, -0.8865,  0.8411,  0.8754]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0703 cost = 0.001646\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9549, -0.6431, -0.7136,  0.9369, -0.6833],\n",
      "         [-0.4365,  0.5824,  0.0075, -0.4149,  0.6368]],\n",
      "\n",
      "        [[-0.4236, -0.4868,  0.5495, -0.1442, -0.7086],\n",
      "         [ 0.8948,  0.9226, -0.9244,  0.9193,  0.8914]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0703 cost = 0.002860\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8991, -0.8487, -0.9228,  0.8676, -0.1223],\n",
      "         [-0.3785,  0.6048,  0.1314, -0.3541,  0.5949]],\n",
      "\n",
      "        [[-0.7399, -0.4754,  0.7530, -0.1419, -0.9069],\n",
      "         [ 0.9155,  0.9098, -0.9379,  0.9108,  0.8846]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0703 cost = 0.000840\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8991, -0.8488, -0.9228,  0.8677, -0.1224],\n",
      "         [-0.3785,  0.6048,  0.1314, -0.3540,  0.5949]],\n",
      "\n",
      "        [[-0.7399, -0.4754,  0.7530, -0.1420, -0.9069],\n",
      "         [ 0.9155,  0.9098, -0.9379,  0.9109,  0.8846]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0704 cost = 0.000839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4367,  0.5825,  0.0077, -0.4149,  0.6368],\n",
      "         [-0.5085,  0.4680,  0.0990, -0.5243,  0.6721]],\n",
      "\n",
      "        [[ 0.8949,  0.9226, -0.9244,  0.9193,  0.8914],\n",
      "         [ 0.8594,  0.8540, -0.8865,  0.8411,  0.8754]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0704 cost = 0.004460\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8735, -0.9466, -0.9548,  0.7700,  0.0034],\n",
      "         [ 0.9549, -0.6435, -0.7138,  0.9370, -0.6835]],\n",
      "\n",
      "        [[-0.6989, -0.4736,  0.7068, -0.1303, -0.8756],\n",
      "         [-0.4237, -0.4870,  0.5496, -0.1444, -0.7087]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0704 cost = 0.002675\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3787,  0.6048,  0.1314, -0.3540,  0.5949],\n",
      "         [-0.5086,  0.4680,  0.0990, -0.5243,  0.6723]],\n",
      "\n",
      "        [[ 0.9156,  0.9098, -0.9380,  0.9109,  0.8847],\n",
      "         [ 0.8594,  0.8540, -0.8866,  0.8412,  0.8755]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0705 cost = 0.004311\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9549, -0.6435, -0.7139,  0.9370, -0.6836],\n",
      "         [-0.4370,  0.5827,  0.0078, -0.4148,  0.6369]],\n",
      "\n",
      "        [[-0.4238, -0.4872,  0.5497, -0.1445, -0.7087],\n",
      "         [ 0.8949,  0.9227, -0.9245,  0.9193,  0.8915]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0705 cost = 0.002843\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8992, -0.8489, -0.9229,  0.8678, -0.1228],\n",
      "         [ 0.8736, -0.9466, -0.9548,  0.7701,  0.0033]],\n",
      "\n",
      "        [[-0.7401, -0.4756,  0.7532, -0.1424, -0.9069],\n",
      "         [-0.6989, -0.4736,  0.7069, -0.1305, -0.8756]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0705 cost = 0.002290\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8992, -0.8489, -0.9229,  0.8678, -0.1229],\n",
      "         [-0.3789,  0.6049,  0.1314, -0.3539,  0.5950]],\n",
      "\n",
      "        [[-0.7402, -0.4756,  0.7532, -0.1425, -0.9069],\n",
      "         [ 0.9156,  0.9098, -0.9380,  0.9109,  0.8848]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0706 cost = 0.000834\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5088,  0.4683,  0.0990, -0.5243,  0.6726],\n",
      "         [ 0.9550, -0.6438, -0.7141,  0.9370, -0.6837]],\n",
      "\n",
      "        [[ 0.8596,  0.8542, -0.8867,  0.8413,  0.8757],\n",
      "         [-0.4240, -0.4874,  0.5498, -0.1446, -0.7088]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0706 cost = 0.002827\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4372,  0.5830,  0.0080, -0.4146,  0.6370],\n",
      "         [ 0.8737, -0.9467, -0.9548,  0.7703,  0.0031]],\n",
      "\n",
      "        [[ 0.8950,  0.9227, -0.9245,  0.9194,  0.8916],\n",
      "         [-0.6990, -0.4737,  0.7070, -0.1307, -0.8757]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0706 cost = 0.001525\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4373,  0.5831,  0.0081, -0.4146,  0.6370],\n",
      "         [-0.5089,  0.4683,  0.0990, -0.5243,  0.6727]],\n",
      "\n",
      "        [[ 0.8950,  0.9227, -0.9245,  0.9194,  0.8917],\n",
      "         [ 0.8596,  0.8542, -0.8867,  0.8414,  0.8757]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0707 cost = 0.004424\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3791,  0.6050,  0.1314, -0.3538,  0.5950],\n",
      "         [ 0.9550, -0.6439, -0.7142,  0.9370, -0.6838]],\n",
      "\n",
      "        [[ 0.9157,  0.9099, -0.9380,  0.9110,  0.8849],\n",
      "         [-0.4241, -0.4876,  0.5499, -0.1448, -0.7089]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0707 cost = 0.002261\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8738, -0.9467, -0.9549,  0.7705,  0.0029],\n",
      "         [ 0.8993, -0.8490, -0.9230,  0.8679, -0.1234]],\n",
      "\n",
      "        [[-0.6991, -0.4738,  0.7071, -0.1309, -0.8757],\n",
      "         [-0.7403, -0.4759,  0.7534, -0.1429, -0.9070]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0707 cost = 0.001284\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.6050,  0.1314, -0.3538,  0.5950],\n",
      "         [ 0.8739, -0.9467, -0.9549,  0.7706,  0.0029]],\n",
      "\n",
      "        [[ 0.9157,  0.9099, -0.9380,  0.9110,  0.8850],\n",
      "         [-0.6991, -0.4739,  0.7071, -0.1309, -0.8757]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0708 cost = 0.001180\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5093,  0.4685,  0.0990, -0.5244,  0.6730],\n",
      "         [-0.4376,  0.5835,  0.0083, -0.4145,  0.6371]],\n",
      "\n",
      "        [[ 0.8598,  0.8544, -0.8868,  0.8415,  0.8759],\n",
      "         [ 0.8951,  0.9228, -0.9245,  0.9195,  0.8918]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0708 cost = 0.002576\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8994, -0.8490, -0.9231,  0.8680, -0.1236],\n",
      "         [ 0.9550, -0.6439, -0.7144,  0.9370, -0.6838]],\n",
      "\n",
      "        [[-0.7404, -0.4761,  0.7535, -0.1432, -0.9070],\n",
      "         [-0.4243, -0.4879,  0.5501, -0.1449, -0.7090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0708 cost = 0.002819\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4377,  0.5837,  0.0083, -0.4144,  0.6372],\n",
      "         [-0.3794,  0.6051,  0.1314, -0.3537,  0.5951]],\n",
      "\n",
      "        [[ 0.8951,  0.9228, -0.9245,  0.9195,  0.8918],\n",
      "         [ 0.9157,  0.9099, -0.9380,  0.9110,  0.8851]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0709 cost = 0.001261\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8740, -0.9468, -0.9549,  0.7709,  0.0027],\n",
      "         [ 0.9550, -0.6440, -0.7145,  0.9370, -0.6839]],\n",
      "\n",
      "        [[-0.6992, -0.4741,  0.7072, -0.1312, -0.8757],\n",
      "         [-0.4244, -0.4880,  0.5502, -0.1450, -0.7090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0709 cost = 0.002642\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8995, -0.8491, -0.9231,  0.8681, -0.1238],\n",
      "         [-0.5094,  0.4687,  0.0989, -0.5243,  0.6733]],\n",
      "\n",
      "        [[-0.7405, -0.4764,  0.7536, -0.1434, -0.9071],\n",
      "         [ 0.8599,  0.8545, -0.8868,  0.8416,  0.8760]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0709 cost = 0.001567\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9550, -0.6441, -0.7146,  0.9370, -0.6840],\n",
      "         [ 0.8995, -0.8491, -0.9231,  0.8681, -0.1239]],\n",
      "\n",
      "        [[-0.4246, -0.4882,  0.5503, -0.1451, -0.7091],\n",
      "         [-0.7406, -0.4765,  0.7537, -0.1435, -0.9071]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0710 cost = 0.002749\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8741, -0.9468, -0.9550,  0.7711,  0.0025],\n",
      "         [-0.5095,  0.4688,  0.0989, -0.5242,  0.6734]],\n",
      "\n",
      "        [[-0.6992, -0.4743,  0.7073, -0.1314, -0.8758],\n",
      "         [ 0.8599,  0.8545, -0.8869,  0.8416,  0.8761]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0710 cost = 0.001607\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4379,  0.5842,  0.0085, -0.4141,  0.6374],\n",
      "         [-0.3794,  0.6052,  0.1315, -0.3535,  0.5952]],\n",
      "\n",
      "        [[ 0.8951,  0.9229, -0.9245,  0.9195,  0.8919],\n",
      "         [ 0.9158,  0.9099, -0.9380,  0.9111,  0.8852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0710 cost = 0.001254\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9551, -0.6444, -0.7148,  0.9371, -0.6841],\n",
      "         [-0.3794,  0.6053,  0.1315, -0.3534,  0.5952]],\n",
      "\n",
      "        [[-0.4247, -0.4884,  0.5504, -0.1452, -0.7092],\n",
      "         [ 0.9158,  0.9099, -0.9380,  0.9111,  0.8852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0711 cost = 0.002238\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8743, -0.9468, -0.9550,  0.7713,  0.0024],\n",
      "         [-0.5096,  0.4689,  0.0989, -0.5242,  0.6736]],\n",
      "\n",
      "        [[-0.6993, -0.4744,  0.7074, -0.1317, -0.8758],\n",
      "         [ 0.8600,  0.8546, -0.8869,  0.8416,  0.8761]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0711 cost = 0.001602\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4381,  0.5844,  0.0087, -0.4140,  0.6375],\n",
      "         [ 0.8996, -0.8493, -0.9232,  0.8682, -0.1245]],\n",
      "\n",
      "        [[ 0.8952,  0.9229, -0.9245,  0.9196,  0.8920],\n",
      "         [-0.7408, -0.4768,  0.7539, -0.1440, -0.9071]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0711 cost = 0.001407\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8743, -0.9469, -0.9550,  0.7715,  0.0023],\n",
      "         [ 0.8997, -0.8493, -0.9232,  0.8683, -0.1246]],\n",
      "\n",
      "        [[-0.6994, -0.4744,  0.7075, -0.1318, -0.8758],\n",
      "         [-0.7408, -0.4768,  0.7539, -0.1441, -0.9071]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0712 cost = 0.001265\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5097,  0.4690,  0.0989, -0.5243,  0.6738],\n",
      "         [ 0.9551, -0.6448, -0.7150,  0.9371, -0.6843]],\n",
      "\n",
      "        [[ 0.8600,  0.8546, -0.8869,  0.8417,  0.8762],\n",
      "         [-0.4249, -0.4887,  0.5505, -0.1455, -0.7093]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0712 cost = 0.002772\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.6054,  0.1315, -0.3533,  0.5954],\n",
      "         [-0.4383,  0.5846,  0.0088, -0.4139,  0.6375]],\n",
      "\n",
      "        [[ 0.9158,  0.9100, -0.9380,  0.9111,  0.8853],\n",
      "         [ 0.8952,  0.9229, -0.9245,  0.9196,  0.8920]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0712 cost = 0.001761\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8744, -0.9469, -0.9550,  0.7717,  0.0021],\n",
      "         [-0.5098,  0.4690,  0.0989, -0.5243,  0.6738]],\n",
      "\n",
      "        [[-0.6995, -0.4745,  0.7076, -0.1320, -0.8758],\n",
      "         [ 0.8600,  0.8546, -0.8869,  0.8417,  0.8762]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0713 cost = 0.001594\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8998, -0.8494, -0.9233,  0.8684, -0.1249],\n",
      "         [-0.3796,  0.6054,  0.1315, -0.3533,  0.5955]],\n",
      "\n",
      "        [[-0.7410, -0.4770,  0.7541, -0.1445, -0.9072],\n",
      "         [ 0.9158,  0.9100, -0.9380,  0.9111,  0.8853]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0713 cost = 0.000814\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4385,  0.5848,  0.0090, -0.4138,  0.6376],\n",
      "         [ 0.9551, -0.6450, -0.7152,  0.9372, -0.6845]],\n",
      "\n",
      "        [[ 0.8952,  0.9229, -0.9245,  0.9196,  0.8921],\n",
      "         [-0.4251, -0.4889,  0.5507, -0.1457, -0.7094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0713 cost = 0.002564\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8998, -0.8494, -0.9233,  0.8684, -0.1251],\n",
      "         [ 0.8745, -0.9469, -0.9551,  0.7718,  0.0020]],\n",
      "\n",
      "        [[-0.7410, -0.4771,  0.7542, -0.1446, -0.9072],\n",
      "         [-0.6996, -0.4746,  0.7078, -0.1323, -0.8758]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0714 cost = 0.002229\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9551, -0.6451, -0.7153,  0.9372, -0.6845],\n",
      "         [-0.4386,  0.5849,  0.0091, -0.4137,  0.6377]],\n",
      "\n",
      "        [[-0.4252, -0.4891,  0.5507, -0.1458, -0.7095],\n",
      "         [ 0.8952,  0.9230, -0.9245,  0.9196,  0.8921]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0714 cost = 0.002768\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3798,  0.6054,  0.1316, -0.3533,  0.5955],\n",
      "         [-0.5101,  0.4690,  0.0990, -0.5244,  0.6741]],\n",
      "\n",
      "        [[ 0.9159,  0.9100, -0.9380,  0.9111,  0.8854],\n",
      "         [ 0.8601,  0.8547, -0.8869,  0.8417,  0.8763]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0714 cost = 0.004215\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3798,  0.6055,  0.1316, -0.3532,  0.5956],\n",
      "         [ 0.8999, -0.8495, -0.9234,  0.8685, -0.1254]],\n",
      "\n",
      "        [[ 0.9159,  0.9100, -0.9380,  0.9112,  0.8854],\n",
      "         [-0.7412, -0.4773,  0.7543, -0.1449, -0.9072]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0715 cost = 0.001054\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8747, -0.9470, -0.9551,  0.7722,  0.0017],\n",
      "         [-0.4388,  0.5851,  0.0092, -0.4136,  0.6378]],\n",
      "\n",
      "        [[-0.6998, -0.4747,  0.7079, -0.1326, -0.8759],\n",
      "         [ 0.8952,  0.9230, -0.9245,  0.9197,  0.8921]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0715 cost = 0.001269\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5102,  0.4691,  0.0990, -0.5244,  0.6743],\n",
      "         [ 0.9552, -0.6455, -0.7155,  0.9373, -0.6847]],\n",
      "\n",
      "        [[ 0.8602,  0.8548, -0.8869,  0.8418,  0.8764],\n",
      "         [-0.4254, -0.4893,  0.5509, -0.1460, -0.7096]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0715 cost = 0.002743\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3800,  0.6055,  0.1316, -0.3531,  0.5957],\n",
      "         [-0.4389,  0.5852,  0.0093, -0.4136,  0.6378]],\n",
      "\n",
      "        [[ 0.9159,  0.9100, -0.9380,  0.9112,  0.8855],\n",
      "         [ 0.8953,  0.9230, -0.9245,  0.9197,  0.8922]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0716 cost = 0.001744\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9552, -0.6455, -0.7156,  0.9373, -0.6848],\n",
      "         [ 0.9000, -0.8496, -0.9235,  0.8686, -0.1258]],\n",
      "\n",
      "        [[-0.4255, -0.4894,  0.5510, -0.1461, -0.7096],\n",
      "         [-0.7413, -0.4775,  0.7544, -0.1452, -0.9073]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0716 cost = 0.002697\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5103,  0.4692,  0.0990, -0.5244,  0.6745],\n",
      "         [ 0.8748, -0.9470, -0.9552,  0.7724,  0.0015]],\n",
      "\n",
      "        [[ 0.8602,  0.8548, -0.8870,  0.8418,  0.8764],\n",
      "         [-0.6999, -0.4749,  0.7081, -0.1328, -0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0716 cost = 0.001677\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.6055,  0.1316, -0.3530,  0.5957],\n",
      "         [ 0.8749, -0.9470, -0.9552,  0.7725,  0.0015]],\n",
      "\n",
      "        [[ 0.9159,  0.9101, -0.9380,  0.9112,  0.8856],\n",
      "         [-0.6999, -0.4749,  0.7081, -0.1329, -0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0717 cost = 0.001144\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4391,  0.5856,  0.0095, -0.4135,  0.6379],\n",
      "         [-0.5104,  0.4692,  0.0990, -0.5244,  0.6746]],\n",
      "\n",
      "        [[ 0.8953,  0.9230, -0.9245,  0.9197,  0.8923],\n",
      "         [ 0.8603,  0.8549, -0.8870,  0.8418,  0.8765]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0717 cost = 0.004318\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9552, -0.6458, -0.7159,  0.9373, -0.6849],\n",
      "         [ 0.9001, -0.8496, -0.9235,  0.8687, -0.1261]],\n",
      "\n",
      "        [[-0.4256, -0.4897,  0.5511, -0.1463, -0.7097],\n",
      "         [-0.7414, -0.4777,  0.7546, -0.1456, -0.9073]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0717 cost = 0.002686\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4392,  0.5858,  0.0095, -0.4134,  0.6380],\n",
      "         [-0.3803,  0.6056,  0.1316, -0.3529,  0.5958]],\n",
      "\n",
      "        [[ 0.8953,  0.9231, -0.9246,  0.9198,  0.8923],\n",
      "         [ 0.9160,  0.9101, -0.9381,  0.9113,  0.8857]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0718 cost = 0.001222\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9002, -0.8497, -0.9236,  0.8688, -0.1263],\n",
      "         [ 0.8750, -0.9471, -0.9552,  0.7728,  0.0013]],\n",
      "\n",
      "        [[-0.7415, -0.4778,  0.7546, -0.1457, -0.9073],\n",
      "         [-0.7000, -0.4750,  0.7082, -0.1331, -0.8759]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0718 cost = 0.002198\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5108,  0.4693,  0.0989, -0.5245,  0.6748],\n",
      "         [ 0.9552, -0.6460, -0.7161,  0.9373, -0.6850]],\n",
      "\n",
      "        [[ 0.8604,  0.8550, -0.8871,  0.8419,  0.8766],\n",
      "         [-0.4257, -0.4899,  0.5512, -0.1465, -0.7098]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0718 cost = 0.002717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9552, -0.6461, -0.7162,  0.9373, -0.6850],\n",
      "         [-0.3805,  0.6056,  0.1316, -0.3529,  0.5958]],\n",
      "\n",
      "        [[-0.4258, -0.4900,  0.5512, -0.1465, -0.7098],\n",
      "         [ 0.9160,  0.9101, -0.9381,  0.9113,  0.8858]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0719 cost = 0.002185\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8752, -0.9471, -0.9553,  0.7731,  0.0011],\n",
      "         [-0.4395,  0.5861,  0.0097, -0.4133,  0.6381]],\n",
      "\n",
      "        [[-0.7001, -0.4752,  0.7083, -0.1334, -0.8760],\n",
      "         [ 0.8954,  0.9231, -0.9246,  0.9198,  0.8924]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0719 cost = 0.001252\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5109,  0.4694,  0.0989, -0.5245,  0.6749],\n",
      "         [ 0.9003, -0.8498, -0.9237,  0.8689, -0.1267]],\n",
      "\n",
      "        [[ 0.8604,  0.8550, -0.8871,  0.8419,  0.8767],\n",
      "         [-0.7416, -0.4781,  0.7548, -0.1460, -0.9074]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0719 cost = 0.001494\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4397,  0.5863,  0.0098, -0.4132,  0.6382],\n",
      "         [ 0.9003, -0.8498, -0.9237,  0.8689, -0.1268]],\n",
      "\n",
      "        [[ 0.8954,  0.9231, -0.9246,  0.9198,  0.8924],\n",
      "         [-0.7417, -0.4782,  0.7548, -0.1461, -0.9074]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0720 cost = 0.001366\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5111,  0.4694,  0.0990, -0.5246,  0.6750],\n",
      "         [ 0.9553, -0.6464, -0.7165,  0.9374, -0.6852]],\n",
      "\n",
      "        [[ 0.8605,  0.8550, -0.8871,  0.8420,  0.8767],\n",
      "         [-0.4259, -0.4903,  0.5513, -0.1468, -0.7099]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0720 cost = 0.002701\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.7536e-01, -9.4717e-01, -9.5533e-01,  7.7338e-01,  8.7344e-04],\n",
      "         [-3.8091e-01,  6.0564e-01,  1.3168e-01, -3.5282e-01,  5.9581e-01]],\n",
      "\n",
      "        [[-7.0017e-01, -4.7544e-01,  7.0842e-01, -1.3362e-01, -8.7599e-01],\n",
      "         [ 9.1606e-01,  9.1013e-01, -9.3808e-01,  9.1131e-01,  8.8587e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0720 cost = 0.000779\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.7541e-01, -9.4718e-01, -9.5534e-01,  7.7346e-01,  8.2082e-04],\n",
      "         [ 9.0040e-01, -8.4982e-01, -9.2375e-01,  8.6902e-01, -1.2710e-01]],\n",
      "\n",
      "        [[-7.0020e-01, -4.7549e-01,  7.0845e-01, -1.3368e-01, -8.7600e-01],\n",
      "         [-7.4175e-01, -4.7842e-01,  7.5491e-01, -1.4633e-01, -9.0741e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0721 cost = 0.001229\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5114,  0.4694,  0.0990, -0.5246,  0.6750],\n",
      "         [ 0.9553, -0.6466, -0.7167,  0.9374, -0.6853]],\n",
      "\n",
      "        [[ 0.8605,  0.8550, -0.8871,  0.8420,  0.8768],\n",
      "         [-0.4260, -0.4905,  0.5514, -0.1470, -0.7100]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0721 cost = 0.002692\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3811,  0.6056,  0.1317, -0.3528,  0.5958],\n",
      "         [-0.4401,  0.5866,  0.0100, -0.4131,  0.6382]],\n",
      "\n",
      "        [[ 0.9161,  0.9101, -0.9381,  0.9113,  0.8859],\n",
      "         [ 0.8954,  0.9231, -0.9246,  0.9199,  0.8925]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0721 cost = 0.001715\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-5.1153e-01,  4.6939e-01,  9.8984e-02, -5.2464e-01,  6.7503e-01],\n",
      "         [ 8.7555e-01, -9.4721e-01, -9.5538e-01,  7.7371e-01,  6.6085e-04]],\n",
      "\n",
      "        [[ 8.6051e-01,  8.5505e-01, -8.8709e-01,  8.4197e-01,  8.7679e-01],\n",
      "         [-7.0028e-01, -4.7568e-01,  7.0854e-01, -1.3388e-01, -8.7603e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0722 cost = 0.001644\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6056,  0.1317, -0.3527,  0.5957],\n",
      "         [ 0.9005, -0.8499, -0.9238,  0.8691, -0.1274]],\n",
      "\n",
      "        [[ 0.9161,  0.9101, -0.9381,  0.9113,  0.8859],\n",
      "         [-0.7419, -0.4787,  0.7551, -0.1467, -0.9075]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0722 cost = 0.001029\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4403,  0.5868,  0.0101, -0.4130,  0.6382],\n",
      "         [ 0.9554, -0.6467, -0.7169,  0.9375, -0.6855]],\n",
      "\n",
      "        [[ 0.8955,  0.9232, -0.9246,  0.9199,  0.8926],\n",
      "         [-0.4262, -0.4909,  0.5515, -0.1472, -0.7101]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0722 cost = 0.002485\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5118,  0.4693,  0.0990, -0.5247,  0.6750],\n",
      "         [ 0.9554, -0.6468, -0.7170,  0.9375, -0.6855]],\n",
      "\n",
      "        [[ 0.8606,  0.8551, -0.8871,  0.8420,  0.8768],\n",
      "         [-0.4262, -0.4909,  0.5516, -0.1473, -0.7101]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0723 cost = 0.002676\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9006, -0.8499, -0.9239,  0.8692, -0.1277],\n",
      "         [-0.4404,  0.5869,  0.0102, -0.4130,  0.6382]],\n",
      "\n",
      "        [[-0.7420, -0.4790,  0.7552, -0.1469, -0.9075],\n",
      "         [ 0.8955,  0.9232, -0.9246,  0.9199,  0.8926]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0723 cost = 0.001243\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.8168e-01,  6.0553e-01,  1.3181e-01, -3.5265e-01,  5.9565e-01],\n",
      "         [ 8.7578e-01, -9.4727e-01, -9.5545e-01,  7.7414e-01,  4.2470e-04]],\n",
      "\n",
      "        [[ 9.1611e-01,  9.1014e-01, -9.3809e-01,  9.1135e-01,  8.8599e-01],\n",
      "         [-7.0041e-01, -4.7599e-01,  7.0868e-01, -1.3419e-01, -8.7607e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0723 cost = 0.001117\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-5.1212e-01,  4.6926e-01,  9.9038e-02, -5.2472e-01,  6.7503e-01],\n",
      "         [ 8.7583e-01, -9.4728e-01, -9.5546e-01,  7.7423e-01,  3.7410e-04]],\n",
      "\n",
      "        [[ 8.6058e-01,  8.5506e-01, -8.8710e-01,  8.4199e-01,  8.7686e-01],\n",
      "         [-7.0043e-01, -4.7605e-01,  7.0870e-01, -1.3425e-01, -8.7608e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0724 cost = 0.001632\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9007, -0.8499, -0.9239,  0.8693, -0.1279],\n",
      "         [-0.4407,  0.5871,  0.0104, -0.4129,  0.6382]],\n",
      "\n",
      "        [[-0.7421, -0.4792,  0.7553, -0.1471, -0.9075],\n",
      "         [ 0.8955,  0.9232, -0.9246,  0.9200,  0.8926]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0724 cost = 0.001239\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3820,  0.6054,  0.1319, -0.3526,  0.5956],\n",
      "         [ 0.9554, -0.6469, -0.7173,  0.9376, -0.6857]],\n",
      "\n",
      "        [[ 0.9161,  0.9101, -0.9381,  0.9114,  0.8860],\n",
      "         [-0.4264, -0.4913,  0.5517, -0.1476, -0.7103]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0724 cost = 0.002134\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5125,  0.4692,  0.0991, -0.5248,  0.6750],\n",
      "         [-0.3821,  0.6054,  0.1319, -0.3526,  0.5955]],\n",
      "\n",
      "        [[ 0.8606,  0.8551, -0.8871,  0.8420,  0.8769],\n",
      "         [ 0.9161,  0.9101, -0.9381,  0.9114,  0.8860]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0725 cost = 0.001665\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4409,  0.5872,  0.0106, -0.4127,  0.6382],\n",
      "         [ 0.9008, -0.8499, -0.9239,  0.8694, -0.1281]],\n",
      "\n",
      "        [[ 0.8955,  0.9232, -0.9246,  0.9200,  0.8926],\n",
      "         [-0.7422, -0.4794,  0.7554, -0.1474, -0.9075]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0725 cost = 0.001342\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.7608e-01, -9.4734e-01, -9.5553e-01,  7.7469e-01,  1.4429e-04],\n",
      "         [ 9.5546e-01, -6.4700e-01, -7.1740e-01,  9.3762e-01, -6.8575e-01]],\n",
      "\n",
      "        [[-7.0054e-01, -4.7632e-01,  7.0883e-01, -1.3457e-01, -8.7614e-01],\n",
      "         [-4.2652e-01, -4.9155e-01,  5.5179e-01, -1.4782e-01, -7.1039e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0725 cost = 0.002509\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5125,  0.4692,  0.0991, -0.5246,  0.6751],\n",
      "         [-0.3820,  0.6055,  0.1319, -0.3524,  0.5955]],\n",
      "\n",
      "        [[ 0.8607,  0.8551, -0.8871,  0.8420,  0.8769],\n",
      "         [ 0.9162,  0.9101, -0.9381,  0.9114,  0.8861]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0726 cost = 0.001659\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9009, -0.8499, -0.9240,  0.8695, -0.1283],\n",
      "         [-0.4410,  0.5875,  0.0108, -0.4125,  0.6383]],\n",
      "\n",
      "        [[-0.7423, -0.4797,  0.7555, -0.1476, -0.9076],\n",
      "         [ 0.8955,  0.9232, -0.9246,  0.9200,  0.8927]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0726 cost = 0.001231\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.5548e-01, -6.4711e-01, -7.1751e-01,  9.3765e-01, -6.8584e-01],\n",
      "         [ 8.7620e-01, -9.4737e-01, -9.5556e-01,  7.7491e-01,  1.6567e-05]],\n",
      "\n",
      "        [[-4.2665e-01, -4.9178e-01,  5.5188e-01, -1.4796e-01, -7.1049e-01],\n",
      "         [-7.0061e-01, -4.7650e-01,  7.0890e-01, -1.3475e-01, -8.7618e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0726 cost = 0.003188\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9009, -0.8499, -0.9240,  0.8696, -0.1285],\n",
      "         [ 0.9555, -0.6472, -0.7176,  0.9377, -0.6859]],\n",
      "\n",
      "        [[-0.7423, -0.4798,  0.7555, -0.1478, -0.9076],\n",
      "         [-0.4267, -0.4919,  0.5519, -0.1480, -0.7105]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0727 cost = 0.002661\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3815,  0.6059,  0.1320, -0.3519,  0.5957],\n",
      "         [-0.4409,  0.5878,  0.0109, -0.4122,  0.6384]],\n",
      "\n",
      "        [[ 0.9162,  0.9102, -0.9381,  0.9114,  0.8861],\n",
      "         [ 0.8956,  0.9232, -0.9246,  0.9200,  0.8927]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0727 cost = 0.001690\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.7636e-01, -9.4741e-01, -9.5561e-01,  7.7523e-01, -1.8553e-04],\n",
      "         [-5.1224e-01,  4.6964e-01,  9.9135e-02, -5.2419e-01,  6.7521e-01]],\n",
      "\n",
      "        [[-7.0068e-01, -4.7674e-01,  7.0899e-01, -1.3496e-01, -8.7623e-01],\n",
      "         [ 8.6070e-01,  8.5510e-01, -8.8714e-01,  8.4206e-01,  8.7698e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0727 cost = 0.001529\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-3.8137e-01,  6.0600e-01,  1.3198e-01, -3.5169e-01,  5.9578e-01],\n",
      "         [ 8.7641e-01, -9.4743e-01, -9.5562e-01,  7.7533e-01, -2.4764e-04]],\n",
      "\n",
      "        [[ 9.1619e-01,  9.1015e-01, -9.3811e-01,  9.1142e-01,  8.8613e-01],\n",
      "         [-7.0072e-01, -4.7680e-01,  7.0902e-01, -1.3504e-01, -8.7624e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0728 cost = 0.001100\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9011, -0.8501, -0.9241,  0.8697, -0.1289],\n",
      "         [-0.4409,  0.5881,  0.0111, -0.4120,  0.6385]],\n",
      "\n",
      "        [[-0.7425, -0.4802,  0.7557, -0.1481, -0.9076],\n",
      "         [ 0.8956,  0.9232, -0.9246,  0.9201,  0.8927]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0728 cost = 0.001223\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5122,  0.4698,  0.0992, -0.5240,  0.6753],\n",
      "         [ 0.9555, -0.6477, -0.7178,  0.9378, -0.6861]],\n",
      "\n",
      "        [[ 0.8607,  0.8551, -0.8871,  0.8421,  0.8770],\n",
      "         [-0.4269, -0.4923,  0.5521, -0.1483, -0.7107]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0728 cost = 0.002624\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4409,  0.5882,  0.0112, -0.4118,  0.6386],\n",
      "         [-0.5122,  0.4698,  0.0992, -0.5240,  0.6754]],\n",
      "\n",
      "        [[ 0.8956,  0.9232, -0.9246,  0.9201,  0.8927],\n",
      "         [ 0.8607,  0.8551, -0.8871,  0.8421,  0.8770]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0729 cost = 0.004233\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 8.7660e-01, -9.4748e-01, -9.5568e-01,  7.7568e-01, -4.7163e-04],\n",
      "         [ 9.0113e-01, -8.5010e-01, -9.2410e-01,  8.6980e-01, -1.2912e-01]],\n",
      "\n",
      "        [[-7.0084e-01, -4.7698e-01,  7.0916e-01, -1.3533e-01, -8.7629e-01],\n",
      "         [-7.4257e-01, -4.8035e-01,  7.5577e-01, -1.4838e-01, -9.0768e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0729 cost = 0.001195\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9556, -0.6477, -0.7180,  0.9378, -0.6862],\n",
      "         [-0.3812,  0.6062,  0.1320, -0.3514,  0.5959]],\n",
      "\n",
      "        [[-0.4271, -0.4925,  0.5522, -0.1484, -0.7108],\n",
      "         [ 0.9162,  0.9102, -0.9381,  0.9115,  0.8862]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0729 cost = 0.002113\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.0117e-01, -8.5012e-01, -9.2412e-01,  8.6984e-01, -1.2927e-01],\n",
      "         [ 8.7667e-01, -9.4750e-01, -9.5570e-01,  7.7582e-01, -5.6555e-04]],\n",
      "\n",
      "        [[-7.4264e-01, -4.8048e-01,  7.5584e-01, -1.4855e-01, -9.0770e-01],\n",
      "         [-7.0089e-01, -4.7706e-01,  7.0922e-01, -1.3546e-01, -8.7630e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0730 cost = 0.002114\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9556, -0.6479, -0.7181,  0.9378, -0.6863],\n",
      "         [-0.4411,  0.5885,  0.0113, -0.4116,  0.6387]],\n",
      "\n",
      "        [[-0.4271, -0.4927,  0.5523, -0.1485, -0.7108],\n",
      "         [ 0.8956,  0.9233, -0.9246,  0.9201,  0.8928]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0730 cost = 0.002637\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5123,  0.4700,  0.0992, -0.5239,  0.6756],\n",
      "         [-0.3812,  0.6063,  0.1320, -0.3512,  0.5959]],\n",
      "\n",
      "        [[ 0.8608,  0.8552, -0.8872,  0.8422,  0.8771],\n",
      "         [ 0.9163,  0.9102, -0.9381,  0.9115,  0.8863]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0730 cost = 0.001631\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5122,  0.4701,  0.0992, -0.5238,  0.6756],\n",
      "         [-0.3811,  0.6063,  0.1320, -0.3511,  0.5960]],\n",
      "\n",
      "        [[ 0.8609,  0.8552, -0.8872,  0.8422,  0.8772],\n",
      "         [ 0.9163,  0.9102, -0.9381,  0.9115,  0.8863]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0731 cost = 0.001629\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9013, -0.8502, -0.9242,  0.8700, -0.1296],\n",
      "         [-0.4410,  0.5889,  0.0115, -0.4113,  0.6388]],\n",
      "\n",
      "        [[-0.7428, -0.4808,  0.7560, -0.1489, -0.9077],\n",
      "         [ 0.8956,  0.9233, -0.9246,  0.9202,  0.8929]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0731 cost = 0.001211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.5559e-01, -6.4819e-01, -7.1829e-01,  9.3784e-01, -6.8644e-01],\n",
      "         [ 8.7690e-01, -9.4757e-01, -9.5577e-01,  7.7624e-01, -8.6682e-04]],\n",
      "\n",
      "        [[-4.2731e-01, -4.9294e-01,  5.5239e-01, -1.4871e-01, -7.1095e-01],\n",
      "         [-7.0101e-01, -4.7734e-01,  7.0937e-01, -1.3580e-01, -8.7637e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0731 cost = 0.003137\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 9.0133e-01, -8.5026e-01, -9.2422e-01,  8.7001e-01, -1.2980e-01],\n",
      "         [ 8.7695e-01, -9.4758e-01, -9.5579e-01,  7.7634e-01, -9.3689e-04]],\n",
      "\n",
      "        [[-7.4284e-01, -4.8090e-01,  7.5606e-01, -1.4903e-01, -9.0777e-01],\n",
      "         [-7.0103e-01, -4.7740e-01,  7.0940e-01, -1.3587e-01, -8.7639e-01]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0732 cost = 0.002099\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5117,  0.4706,  0.0992, -0.5233,  0.6759],\n",
      "         [-0.3802,  0.6068,  0.1320, -0.3505,  0.5963]],\n",
      "\n",
      "        [[ 0.8609,  0.8553, -0.8872,  0.8422,  0.8773],\n",
      "         [ 0.9163,  0.9102, -0.9381,  0.9115,  0.8864]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0732 cost = 0.001619\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4406,  0.5894,  0.0116, -0.4108,  0.6390],\n",
      "         [ 0.9556, -0.6485, -0.7185,  0.9379, -0.6866]],\n",
      "\n",
      "        [[ 0.8957,  0.9233, -0.9246,  0.9202,  0.8929],\n",
      "         [-0.4274, -0.4932,  0.5525, -0.1488, -0.7110]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0732 cost = 0.002400\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9556, -0.6486, -0.7185,  0.9379, -0.6866],\n",
      "         [-0.3797,  0.6071,  0.1320, -0.3501,  0.5964]],\n",
      "\n",
      "        [[-0.4275, -0.4933,  0.5525, -0.1489, -0.7111],\n",
      "         [ 0.9163,  0.9102, -0.9381,  0.9115,  0.8864]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0733 cost = 0.002091\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9015, -0.8504, -0.9243,  0.8701, -0.1302],\n",
      "         [ 0.8772, -0.9477, -0.9559,  0.7768, -0.0013]],\n",
      "\n",
      "        [[-0.7430, -0.4813,  0.7562, -0.1494, -0.9078],\n",
      "         [-0.7011, -0.4777,  0.7095, -0.1362, -0.8765]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0733 cost = 0.002087\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5111,  0.4712,  0.0991, -0.5227,  0.6762],\n",
      "         [-0.4403,  0.5897,  0.0117, -0.4104,  0.6392]],\n",
      "\n",
      "        [[ 0.8610,  0.8553, -0.8872,  0.8423,  0.8773],\n",
      "         [ 0.8957,  0.9233, -0.9246,  0.9202,  0.8929]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0733 cost = 0.002383\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3790,  0.6075,  0.1320, -0.3496,  0.5966],\n",
      "         [ 0.8773, -0.9477, -0.9559,  0.7771, -0.0014]],\n",
      "\n",
      "        [[ 0.9163,  0.9102, -0.9381,  0.9116,  0.8864],\n",
      "         [-0.7012, -0.4779,  0.7096, -0.1363, -0.8765]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0734 cost = 0.001076\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4401,  0.5900,  0.0117, -0.4102,  0.6393],\n",
      "         [ 0.9016, -0.8505, -0.9244,  0.8703, -0.1305]],\n",
      "\n",
      "        [[ 0.8957,  0.9233, -0.9246,  0.9202,  0.8929],\n",
      "         [-0.7431, -0.4816,  0.7563, -0.1496, -0.9079]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0734 cost = 0.001301\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9557, -0.6492, -0.7189,  0.9380, -0.6869],\n",
      "         [-0.5108,  0.4716,  0.0991, -0.5223,  0.6763]],\n",
      "\n",
      "        [[-0.4277, -0.4937,  0.5527, -0.1492, -0.7112],\n",
      "         [ 0.8610,  0.8554, -0.8873,  0.8423,  0.8773]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0734 cost = 0.003169\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4400,  0.5903,  0.0118, -0.4099,  0.6394],\n",
      "         [ 0.9557, -0.6493, -0.7189,  0.9380, -0.6869]],\n",
      "\n",
      "        [[ 0.8957,  0.9233, -0.9246,  0.9203,  0.8930],\n",
      "         [-0.4277, -0.4938,  0.5527, -0.1492, -0.7113]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0735 cost = 0.002379\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.6079,  0.1321, -0.3491,  0.5968],\n",
      "         [-0.5107,  0.4718,  0.0992, -0.5222,  0.6764]],\n",
      "\n",
      "        [[ 0.9163,  0.9102, -0.9381,  0.9116,  0.8865],\n",
      "         [ 0.8610,  0.8554, -0.8873,  0.8423,  0.8774]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0735 cost = 0.004068\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9017, -0.8505, -0.9244,  0.8704, -0.1309],\n",
      "         [ 0.8776, -0.9478, -0.9560,  0.7776, -0.0018]],\n",
      "\n",
      "        [[-0.7432, -0.4819,  0.7565, -0.1499, -0.9079],\n",
      "         [-0.7013, -0.4782,  0.7098, -0.1367, -0.8766]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0735 cost = 0.002067\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9017, -0.8506, -0.9245,  0.8704, -0.1310],\n",
      "         [-0.5105,  0.4720,  0.0991, -0.5220,  0.6766]],\n",
      "\n",
      "        [[-0.7433, -0.4820,  0.7565, -0.1500, -0.9080],\n",
      "         [ 0.8611,  0.8555, -0.8873,  0.8424,  0.8774]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0736 cost = 0.001449\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3781,  0.6081,  0.1320, -0.3488,  0.5970],\n",
      "         [ 0.9557, -0.6497, -0.7192,  0.9380, -0.6871]],\n",
      "\n",
      "        [[ 0.9164,  0.9102, -0.9381,  0.9116,  0.8865],\n",
      "         [-0.4279, -0.4941,  0.5529, -0.1494, -0.7114]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0736 cost = 0.002052\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8778, -0.9478, -0.9560,  0.7779, -0.0020],\n",
      "         [-0.4398,  0.5908,  0.0120, -0.4095,  0.6397]],\n",
      "\n",
      "        [[-0.7014, -0.4784,  0.7099, -0.1369, -0.8767],\n",
      "         [ 0.8957,  0.9233, -0.9246,  0.9203,  0.8930]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0736 cost = 0.001183\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5104,  0.4723,  0.0991, -0.5218,  0.6768],\n",
      "         [ 0.9018, -0.8507, -0.9245,  0.8705, -0.1313]],\n",
      "\n",
      "        [[ 0.8612,  0.8555, -0.8873,  0.8424,  0.8775],\n",
      "         [-0.7434, -0.4822,  0.7566, -0.1503, -0.9080]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0737 cost = 0.001409\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9557, -0.6499, -0.7193,  0.9380, -0.6872],\n",
      "         [-0.3780,  0.6083,  0.1320, -0.3485,  0.5971]],\n",
      "\n",
      "        [[-0.4281, -0.4943,  0.5530, -0.1495, -0.7115],\n",
      "         [ 0.9164,  0.9103, -0.9381,  0.9116,  0.8866]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0737 cost = 0.002061\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4398,  0.5911,  0.0121, -0.4094,  0.6398],\n",
      "         [ 0.8779, -0.9479, -0.9561,  0.7782, -0.0022]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9246,  0.9203,  0.8931],\n",
      "         [-0.7015, -0.4786,  0.7100, -0.1372, -0.8767]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0737 cost = 0.001358\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9558, -0.6501, -0.7195,  0.9381, -0.6873],\n",
      "         [ 0.8780, -0.9479, -0.9561,  0.7783, -0.0023]],\n",
      "\n",
      "        [[-0.4282, -0.4945,  0.5530, -0.1496, -0.7115],\n",
      "         [-0.7015, -0.4786,  0.7100, -0.1372, -0.8768]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0738 cost = 0.003066\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4398,  0.5912,  0.0122, -0.4093,  0.6399],\n",
      "         [-0.3779,  0.6084,  0.1321, -0.3483,  0.5972]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9246,  0.9204,  0.8931],\n",
      "         [ 0.9164,  0.9103, -0.9381,  0.9117,  0.8866]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0738 cost = 0.001141\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5104,  0.4725,  0.0992, -0.5217,  0.6770],\n",
      "         [ 0.9020, -0.8508, -0.9246,  0.8707, -0.1318]],\n",
      "\n",
      "        [[ 0.8612,  0.8556, -0.8874,  0.8425,  0.8776],\n",
      "         [-0.7436, -0.4825,  0.7568, -0.1507, -0.9081]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0738 cost = 0.001402\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4399,  0.5914,  0.0123, -0.4092,  0.6399],\n",
      "         [ 0.9020, -0.8508, -0.9246,  0.8707, -0.1319]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9246,  0.9204,  0.8931],\n",
      "         [-0.7436, -0.4826,  0.7568, -0.1508, -0.9081]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0739 cost = 0.001280\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5105,  0.4726,  0.0992, -0.5217,  0.6771],\n",
      "         [ 0.8782, -0.9480, -0.9562,  0.7788, -0.0026]],\n",
      "\n",
      "        [[ 0.8612,  0.8556, -0.8874,  0.8425,  0.8776],\n",
      "         [-0.7016, -0.4789,  0.7101, -0.1375, -0.8768]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0739 cost = 0.001541\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9558, -0.6507, -0.7198,  0.9381, -0.6875],\n",
      "         [-0.3780,  0.6085,  0.1321, -0.3482,  0.5973]],\n",
      "\n",
      "        [[-0.4284, -0.4948,  0.5532, -0.1499, -0.7116],\n",
      "         [ 0.9164,  0.9103, -0.9381,  0.9117,  0.8867]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0739 cost = 0.002045\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8783, -0.9480, -0.9562,  0.7790, -0.0027],\n",
      "         [-0.5106,  0.4727,  0.0992, -0.5217,  0.6772]],\n",
      "\n",
      "        [[-0.7017, -0.4790,  0.7102, -0.1377, -0.8768],\n",
      "         [ 0.8613,  0.8556, -0.8874,  0.8425,  0.8776]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0740 cost = 0.001472\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4401,  0.5916,  0.0125, -0.4091,  0.6400],\n",
      "         [ 0.9021, -0.8509, -0.9247,  0.8708, -0.1323]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9247,  0.9204,  0.8932],\n",
      "         [-0.7437, -0.4829,  0.7570, -0.1511, -0.9081]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0740 cost = 0.001274\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9558, -0.6510, -0.7199,  0.9382, -0.6877],\n",
      "         [-0.3781,  0.6086,  0.1321, -0.3481,  0.5973]],\n",
      "\n",
      "        [[-0.4285, -0.4950,  0.5533, -0.1501, -0.7117],\n",
      "         [ 0.9164,  0.9103, -0.9381,  0.9117,  0.8867]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0740 cost = 0.002038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4402,  0.5917,  0.0126, -0.4090,  0.6400],\n",
      "         [-0.3782,  0.6086,  0.1322, -0.3481,  0.5973]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9247,  0.9204,  0.8932],\n",
      "         [ 0.9164,  0.9103, -0.9381,  0.9117,  0.8867]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0741 cost = 0.001132\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9022, -0.8510, -0.9248,  0.8709, -0.1326],\n",
      "         [-0.5109,  0.4727,  0.0992, -0.5218,  0.6773]],\n",
      "\n",
      "        [[-0.7438, -0.4830,  0.7571, -0.1514, -0.9082],\n",
      "         [ 0.8613,  0.8557, -0.8874,  0.8425,  0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0741 cost = 0.001426\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9559, -0.6514, -0.7201,  0.9382, -0.6878],\n",
      "         [ 0.8786, -0.9480, -0.9563,  0.7794, -0.0030]],\n",
      "\n",
      "        [[-0.4286, -0.4951,  0.5533, -0.1503, -0.7118],\n",
      "         [-0.7018, -0.4791,  0.7103, -0.1380, -0.8769]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0741 cost = 0.003027\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4404,  0.5919,  0.0128, -0.4089,  0.6401],\n",
      "         [ 0.9559, -0.6516, -0.7202,  0.9382, -0.6879]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9247,  0.9204,  0.8932],\n",
      "         [-0.4286, -0.4952,  0.5534, -0.1504, -0.7118]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0742 cost = 0.002323\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5111,  0.4728,  0.0992, -0.5218,  0.6774],\n",
      "         [ 0.8787, -0.9481, -0.9563,  0.7796, -0.0032]],\n",
      "\n",
      "        [[ 0.8613,  0.8557, -0.8874,  0.8425,  0.8777],\n",
      "         [-0.7019, -0.4792,  0.7104, -0.1382, -0.8769]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0742 cost = 0.001525\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9023, -0.8511, -0.9248,  0.8710, -0.1331],\n",
      "         [-0.3784,  0.6087,  0.1322, -0.3480,  0.5974]],\n",
      "\n",
      "        [[-0.7440, -0.4832,  0.7572, -0.1517, -0.9082],\n",
      "         [ 0.9164,  0.9103, -0.9381,  0.9117,  0.8868]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0742 cost = 0.000738\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4407,  0.5920,  0.0130, -0.4088,  0.6401],\n",
      "         [ 0.9559, -0.6520, -0.7204,  0.9383, -0.6880]],\n",
      "\n",
      "        [[ 0.8958,  0.9234, -0.9246,  0.9205,  0.8932],\n",
      "         [-0.4287, -0.4954,  0.5534, -0.1506, -0.7119]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0743 cost = 0.002314\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9024, -0.8512, -0.9249,  0.8711, -0.1333],\n",
      "         [ 0.8788, -0.9481, -0.9564,  0.7799, -0.0034]],\n",
      "\n",
      "        [[-0.7440, -0.4833,  0.7573, -0.1519, -0.9082],\n",
      "         [-0.7020, -0.4793,  0.7105, -0.1384, -0.8770]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0743 cost = 0.002008\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3785,  0.6087,  0.1323, -0.3480,  0.5974],\n",
      "         [-0.5113,  0.4728,  0.0993, -0.5219,  0.6775]],\n",
      "\n",
      "        [[ 0.9164,  0.9103, -0.9381,  0.9117,  0.8868],\n",
      "         [ 0.8613,  0.8557, -0.8873,  0.8424,  0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0743 cost = 0.004019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9024, -0.8512, -0.9249,  0.8712, -0.1335],\n",
      "         [-0.3786,  0.6087,  0.1323, -0.3480,  0.5974]],\n",
      "\n",
      "        [[-0.7441, -0.4835,  0.7574, -0.1521, -0.9082],\n",
      "         [ 0.9165,  0.9103, -0.9381,  0.9117,  0.8868]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0744 cost = 0.000735\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8790, -0.9482, -0.9564,  0.7803, -0.0036],\n",
      "         [ 0.9560, -0.6524, -0.7206,  0.9383, -0.6882]],\n",
      "\n",
      "        [[-0.7020, -0.4795,  0.7106, -0.1386, -0.8770],\n",
      "         [-0.4288, -0.4957,  0.5536, -0.1508, -0.7120]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0744 cost = 0.002357\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4409,  0.5923,  0.0133, -0.4088,  0.6402],\n",
      "         [-0.5114,  0.4729,  0.0993, -0.5220,  0.6776]],\n",
      "\n",
      "        [[ 0.8959,  0.9235, -0.9246,  0.9205,  0.8933],\n",
      "         [ 0.8614,  0.8557, -0.8874,  0.8425,  0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0744 cost = 0.004119\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5115,  0.4730,  0.0993, -0.5220,  0.6777],\n",
      "         [-0.3787,  0.6088,  0.1323, -0.3479,  0.5975]],\n",
      "\n",
      "        [[ 0.8614,  0.8557, -0.8874,  0.8425,  0.8778],\n",
      "         [ 0.9165,  0.9103, -0.9381,  0.9117,  0.8869]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0745 cost = 0.001551\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4410,  0.5925,  0.0133, -0.4086,  0.6403],\n",
      "         [ 0.9025, -0.8513, -0.9250,  0.8713, -0.1339]],\n",
      "\n",
      "        [[ 0.8959,  0.9235, -0.9247,  0.9205,  0.8933],\n",
      "         [-0.7443, -0.4838,  0.7575, -0.1524, -0.9083]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0745 cost = 0.001252\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8791, -0.9482, -0.9565,  0.7805, -0.0038],\n",
      "         [ 0.9560, -0.6526, -0.7208,  0.9383, -0.6883]],\n",
      "\n",
      "        [[-0.7021, -0.4797,  0.7107, -0.1389, -0.8771],\n",
      "         [-0.4290, -0.4960,  0.5537, -0.1509, -0.7121]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0745 cost = 0.002349\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8792, -0.9482, -0.9565,  0.7806, -0.0039],\n",
      "         [-0.4410,  0.5928,  0.0134, -0.4085,  0.6404]],\n",
      "\n",
      "        [[-0.7021, -0.4798,  0.7107, -0.1389, -0.8771],\n",
      "         [ 0.8959,  0.9235, -0.9247,  0.9206,  0.8934]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0746 cost = 0.001147\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3785,  0.6092,  0.1323, -0.3476,  0.5977],\n",
      "         [ 0.9560, -0.6527, -0.7209,  0.9384, -0.6884]],\n",
      "\n",
      "        [[ 0.9165,  0.9104, -0.9382,  0.9118,  0.8870],\n",
      "         [-0.4291, -0.4962,  0.5538, -0.1510, -0.7121]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0746 cost = 0.001985\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5115,  0.4735,  0.0992, -0.5217,  0.6781],\n",
      "         [ 0.9026, -0.8514, -0.9250,  0.8714, -0.1342]],\n",
      "\n",
      "        [[ 0.8616,  0.8559, -0.8875,  0.8427,  0.8780],\n",
      "         [-0.7444, -0.4840,  0.7577, -0.1527, -0.9083]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0746 cost = 0.001366\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9560, -0.6528, -0.7210,  0.9384, -0.6884],\n",
      "         [ 0.8793, -0.9483, -0.9565,  0.7808, -0.0040]],\n",
      "\n",
      "        [[-0.4292, -0.4963,  0.5539, -0.1511, -0.7122],\n",
      "         [-0.7022, -0.4799,  0.7108, -0.1391, -0.8771]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0747 cost = 0.002975\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3784,  0.6093,  0.1323, -0.3475,  0.5978],\n",
      "         [ 0.9027, -0.8514, -0.9251,  0.8714, -0.1343]],\n",
      "\n",
      "        [[ 0.9166,  0.9104, -0.9382,  0.9118,  0.8870],\n",
      "         [-0.7445, -0.4842,  0.7577, -0.1528, -0.9084]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0747 cost = 0.000949\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5115,  0.4736,  0.0992, -0.5216,  0.6782],\n",
      "         [-0.4411,  0.5932,  0.0136, -0.4082,  0.6406]],\n",
      "\n",
      "        [[ 0.8616,  0.8559, -0.8875,  0.8427,  0.8780],\n",
      "         [ 0.8960,  0.9235, -0.9247,  0.9206,  0.8934]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0747 cost = 0.002285\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9027, -0.8514, -0.9251,  0.8715, -0.1345],\n",
      "         [-0.3784,  0.6094,  0.1323, -0.3474,  0.5979]],\n",
      "\n",
      "        [[-0.7445, -0.4844,  0.7578, -0.1530, -0.9084],\n",
      "         [ 0.9166,  0.9104, -0.9382,  0.9118,  0.8871]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0748 cost = 0.000725\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9560, -0.6532, -0.7212,  0.9384, -0.6886],\n",
      "         [-0.5115,  0.4738,  0.0992, -0.5215,  0.6783]],\n",
      "\n",
      "        [[-0.4294, -0.4967,  0.5540, -0.1513, -0.7123],\n",
      "         [ 0.8616,  0.8560, -0.8875,  0.8427,  0.8781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0748 cost = 0.003049\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8795, -0.9483, -0.9566,  0.7812, -0.0044],\n",
      "         [-0.4411,  0.5935,  0.0137, -0.4080,  0.6407]],\n",
      "\n",
      "        [[-0.7023, -0.4802,  0.7110, -0.1395, -0.8772],\n",
      "         [ 0.8960,  0.9235, -0.9247,  0.9206,  0.8935]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0748 cost = 0.001138\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9028, -0.8515, -0.9252,  0.8716, -0.1348],\n",
      "         [-0.5115,  0.4739,  0.0992, -0.5214,  0.6784]],\n",
      "\n",
      "        [[-0.7446, -0.4845,  0.7579, -0.1532, -0.9084],\n",
      "         [ 0.8617,  0.8560, -0.8876,  0.8428,  0.8781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0749 cost = 0.001394\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4411,  0.5937,  0.0138, -0.4079,  0.6407],\n",
      "         [ 0.8796, -0.9483, -0.9566,  0.7813, -0.0045]],\n",
      "\n",
      "        [[ 0.8960,  0.9236, -0.9247,  0.9207,  0.8935],\n",
      "         [-0.7024, -0.4803,  0.7110, -0.1396, -0.8772]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0749 cost = 0.001300\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3782,  0.6096,  0.1324, -0.3470,  0.5980],\n",
      "         [ 0.9560, -0.6535, -0.7214,  0.9384, -0.6887]],\n",
      "\n",
      "        [[ 0.9166,  0.9104, -0.9382,  0.9119,  0.8872],\n",
      "         [-0.4296, -0.4969,  0.5542, -0.1514, -0.7124]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0749 cost = 0.001965\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4412,  0.5938,  0.0139, -0.4077,  0.6408],\n",
      "         [ 0.9029, -0.8515, -0.9252,  0.8716, -0.1351]],\n",
      "\n",
      "        [[ 0.8960,  0.9236, -0.9247,  0.9207,  0.8935],\n",
      "         [-0.7447, -0.4847,  0.7580, -0.1535, -0.9085]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0750 cost = 0.001232\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5116,  0.4740,  0.0992, -0.5213,  0.6786],\n",
      "         [-0.3783,  0.6096,  0.1324, -0.3470,  0.5981]],\n",
      "\n",
      "        [[ 0.8618,  0.8561, -0.8876,  0.8428,  0.8782],\n",
      "         [ 0.9166,  0.9104, -0.9382,  0.9119,  0.8872]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0750 cost = 0.001519\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9561, -0.6537, -0.7215,  0.9385, -0.6888],\n",
      "         [ 0.8797, -0.9484, -0.9566,  0.7816, -0.0047]],\n",
      "\n",
      "        [[-0.4297, -0.4970,  0.5543, -0.1516, -0.7125],\n",
      "         [-0.7025, -0.4804,  0.7111, -0.1399, -0.8773]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0750 cost = 0.002942\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3781,  0.6098,  0.1324, -0.3468,  0.5982],\n",
      "         [-0.5116,  0.4742,  0.0993, -0.5212,  0.6787]],\n",
      "\n",
      "        [[ 0.9166,  0.9104, -0.9382,  0.9119,  0.8872],\n",
      "         [ 0.8618,  0.8561, -0.8876,  0.8428,  0.8782]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0751 cost = 0.003956\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9561, -0.6539, -0.7216,  0.9385, -0.6889],\n",
      "         [ 0.9030, -0.8516, -0.9252,  0.8717, -0.1354]],\n",
      "\n",
      "        [[-0.4298, -0.4971,  0.5543, -0.1516, -0.7125],\n",
      "         [-0.7449, -0.4849,  0.7581, -0.1538, -0.9085]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0751 cost = 0.002418\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4412,  0.5943,  0.0141, -0.4073,  0.6410],\n",
      "         [ 0.8798, -0.9484, -0.9567,  0.7818, -0.0048]],\n",
      "\n",
      "        [[ 0.8961,  0.9236, -0.9247,  0.9207,  0.8936],\n",
      "         [-0.7025, -0.4805,  0.7112, -0.1401, -0.8773]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0751 cost = 0.001289\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5114,  0.4745,  0.0992, -0.5210,  0.6790],\n",
      "         [ 0.9030, -0.8516, -0.9253,  0.8718, -0.1355]],\n",
      "\n",
      "        [[ 0.8619,  0.8562, -0.8877,  0.8429,  0.8783],\n",
      "         [-0.7449, -0.4850,  0.7582, -0.1540, -0.9085]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0752 cost = 0.001342\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3778,  0.6101,  0.1324, -0.3464,  0.5984],\n",
      "         [ 0.9561, -0.6542, -0.7218,  0.9385, -0.6890]],\n",
      "\n",
      "        [[ 0.9167,  0.9105, -0.9382,  0.9120,  0.8873],\n",
      "         [-0.4299, -0.4973,  0.5544, -0.1518, -0.7126]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0752 cost = 0.001949\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8800, -0.9484, -0.9567,  0.7821, -0.0050],\n",
      "         [-0.4412,  0.5946,  0.0142, -0.4072,  0.6412]],\n",
      "\n",
      "        [[-0.7026, -0.4807,  0.7113, -0.1402, -0.8773],\n",
      "         [ 0.8961,  0.9236, -0.9247,  0.9207,  0.8937]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0752 cost = 0.001123\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5114,  0.4748,  0.0992, -0.5209,  0.6792],\n",
      "         [-0.3778,  0.6102,  0.1324, -0.3463,  0.5984]],\n",
      "\n",
      "        [[ 0.8620,  0.8563, -0.8877,  0.8430,  0.8784],\n",
      "         [ 0.9167,  0.9105, -0.9382,  0.9120,  0.8874]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0753 cost = 0.001502\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4412,  0.5948,  0.0143, -0.4070,  0.6413],\n",
      "         [ 0.8800, -0.9485, -0.9567,  0.7822, -0.0051]],\n",
      "\n",
      "        [[ 0.8961,  0.9236, -0.9247,  0.9208,  0.8937],\n",
      "         [-0.7026, -0.4807,  0.7113, -0.1404, -0.8773]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0753 cost = 0.001282\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9561, -0.6545, -0.7220,  0.9385, -0.6891],\n",
      "         [ 0.9031, -0.8517, -0.9254,  0.8719, -0.1359]],\n",
      "\n",
      "        [[-0.4300, -0.4976,  0.5545, -0.1519, -0.7127],\n",
      "         [-0.7451, -0.4853,  0.7584, -0.1544, -0.9086]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0753 cost = 0.002401\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9561, -0.6546, -0.7220,  0.9385, -0.6892],\n",
      "         [-0.5113,  0.4751,  0.0992, -0.5206,  0.6794]],\n",
      "\n",
      "        [[-0.4301, -0.4976,  0.5546, -0.1520, -0.7127],\n",
      "         [ 0.8620,  0.8563, -0.8877,  0.8430,  0.8784]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0754 cost = 0.003002\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9032, -0.8517, -0.9254,  0.8719, -0.1360],\n",
      "         [-0.3774,  0.6105,  0.1324, -0.3458,  0.5987]],\n",
      "\n",
      "        [[-0.7451, -0.4854,  0.7584, -0.1545, -0.9086],\n",
      "         [ 0.9167,  0.9105, -0.9382,  0.9120,  0.8874]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0754 cost = 0.000711\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8802, -0.9485, -0.9568,  0.7825, -0.0053],\n",
      "         [-0.4412,  0.5952,  0.0144, -0.4067,  0.6414]],\n",
      "\n",
      "        [[-0.7027, -0.4809,  0.7114, -0.1406, -0.8774],\n",
      "         [ 0.8962,  0.9237, -0.9247,  0.9208,  0.8938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0754 cost = 0.001116\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8802, -0.9485, -0.9568,  0.7826, -0.0054],\n",
      "         [-0.3772,  0.6107,  0.1324, -0.3457,  0.5988]],\n",
      "\n",
      "        [[-0.7027, -0.4809,  0.7115, -0.1407, -0.8774],\n",
      "         [ 0.9168,  0.9105, -0.9382,  0.9120,  0.8875]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0755 cost = 0.000694\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9562, -0.6551, -0.7223,  0.9386, -0.6893],\n",
      "         [-0.4412,  0.5953,  0.0145, -0.4066,  0.6415]],\n",
      "\n",
      "        [[-0.4302, -0.4978,  0.5547, -0.1522, -0.7128],\n",
      "         [ 0.8962,  0.9237, -0.9247,  0.9208,  0.8938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0755 cost = 0.002442\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9033, -0.8518, -0.9254,  0.8720, -0.1364],\n",
      "         [-0.5112,  0.4754,  0.0993, -0.5203,  0.6796]],\n",
      "\n",
      "        [[-0.7452, -0.4855,  0.7585, -0.1548, -0.9086],\n",
      "         [ 0.8621,  0.8564, -0.8878,  0.8430,  0.8785]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0755 cost = 0.001365\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4413,  0.5954,  0.0146, -0.4064,  0.6416],\n",
      "         [ 0.9562, -0.6553, -0.7224,  0.9386, -0.6894]],\n",
      "\n",
      "        [[ 0.8962,  0.9237, -0.9247,  0.9208,  0.8938],\n",
      "         [-0.4303, -0.4979,  0.5547, -0.1523, -0.7128]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0756 cost = 0.002219\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9033, -0.8518, -0.9255,  0.8721, -0.1366],\n",
      "         [ 0.8804, -0.9486, -0.9568,  0.7828, -0.0056]],\n",
      "\n",
      "        [[-0.7453, -0.4856,  0.7586, -0.1550, -0.9087],\n",
      "         [-0.7028, -0.4810,  0.7115, -0.1409, -0.8774]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0756 cost = 0.001925\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5112,  0.4755,  0.0993, -0.5203,  0.6797],\n",
      "         [-0.3771,  0.6109,  0.1325, -0.3454,  0.5989]],\n",
      "\n",
      "        [[ 0.8621,  0.8564, -0.8878,  0.8430,  0.8785],\n",
      "         [ 0.9168,  0.9105, -0.9382,  0.9120,  0.8875]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0756 cost = 0.001483\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4413,  0.5956,  0.0148, -0.4063,  0.6417],\n",
      "         [ 0.9562, -0.6557, -0.7226,  0.9387, -0.6896]],\n",
      "\n",
      "        [[ 0.8962,  0.9237, -0.9247,  0.9209,  0.8938],\n",
      "         [-0.4304, -0.4981,  0.5548, -0.1525, -0.7129]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0757 cost = 0.002211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8805, -0.9486, -0.9569,  0.7831, -0.0058],\n",
      "         [-0.3768,  0.6110,  0.1325, -0.3452,  0.5990]],\n",
      "\n",
      "        [[-0.7029, -0.4812,  0.7116, -0.1411, -0.8775],\n",
      "         [ 0.9168,  0.9105, -0.9382,  0.9120,  0.8875]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0757 cost = 0.000688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9034, -0.8520, -0.9255,  0.8722, -0.1370],\n",
      "         [-0.5110,  0.4758,  0.0993, -0.5201,  0.6799]],\n",
      "\n",
      "        [[-0.7454, -0.4858,  0.7587, -0.1553, -0.9087],\n",
      "         [ 0.8621,  0.8565, -0.8878,  0.8431,  0.8786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0757 cost = 0.001358\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8806, -0.9486, -0.9569,  0.7833, -0.0059],\n",
      "         [ 0.9562, -0.6560, -0.7227,  0.9387, -0.6897]],\n",
      "\n",
      "        [[-0.7029, -0.4812,  0.7117, -0.1413, -0.8775],\n",
      "         [-0.4305, -0.4983,  0.5549, -0.1527, -0.7130]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0758 cost = 0.002264\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6112,  0.1325, -0.3450,  0.5992],\n",
      "         [ 0.9035, -0.8520, -0.9256,  0.8723, -0.1372]],\n",
      "\n",
      "        [[ 0.9168,  0.9106, -0.9382,  0.9120,  0.8876],\n",
      "         [-0.7455, -0.4859,  0.7588, -0.1555, -0.9087]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0758 cost = 0.000917\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5108,  0.4759,  0.0993, -0.5200,  0.6800],\n",
      "         [-0.4413,  0.5959,  0.0151, -0.4059,  0.6418]],\n",
      "\n",
      "        [[ 0.8622,  0.8565, -0.8878,  0.8431,  0.8786],\n",
      "         [ 0.8962,  0.9237, -0.9247,  0.9209,  0.8938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0758 cost = 0.002211\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9035, -0.8521, -0.9256,  0.8723, -0.1374],\n",
      "         [ 0.9562, -0.6563, -0.7228,  0.9387, -0.6899]],\n",
      "\n",
      "        [[-0.7456, -0.4860,  0.7589, -0.1557, -0.9088],\n",
      "         [-0.4306, -0.4985,  0.5550, -0.1529, -0.7131]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0759 cost = 0.002404\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3763,  0.6114,  0.1325, -0.3448,  0.5993],\n",
      "         [ 0.8808, -0.9487, -0.9570,  0.7836, -0.0062]],\n",
      "\n",
      "        [[ 0.9168,  0.9106, -0.9382,  0.9121,  0.8876],\n",
      "         [-0.7030, -0.4814,  0.7118, -0.1415, -0.8776]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0759 cost = 0.000984\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4412,  0.5962,  0.0152, -0.4057,  0.6419],\n",
      "         [-0.5107,  0.4761,  0.0993, -0.5198,  0.6802]],\n",
      "\n",
      "        [[ 0.8962,  0.9237, -0.9247,  0.9209,  0.8939],\n",
      "         [ 0.8622,  0.8565, -0.8878,  0.8431,  0.8786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0759 cost = 0.004000\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4412,  0.5963,  0.0152, -0.4056,  0.6420],\n",
      "         [-0.3761,  0.6115,  0.1325, -0.3446,  0.5994]],\n",
      "\n",
      "        [[ 0.8963,  0.9237, -0.9247,  0.9209,  0.8939],\n",
      "         [ 0.9168,  0.9106, -0.9382,  0.9121,  0.8876]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0760 cost = 0.001062\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9563, -0.6566, -0.7230,  0.9388, -0.6900],\n",
      "         [ 0.9036, -0.8522, -0.9256,  0.8724, -0.1378]],\n",
      "\n",
      "        [[-0.4308, -0.4987,  0.5551, -0.1530, -0.7132],\n",
      "         [-0.7457, -0.4863,  0.7590, -0.1560, -0.9088]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0760 cost = 0.002351\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8809, -0.9487, -0.9570,  0.7838, -0.0064],\n",
      "         [-0.5107,  0.4763,  0.0993, -0.5196,  0.6804]],\n",
      "\n",
      "        [[-0.7031, -0.4816,  0.7119, -0.1418, -0.8776],\n",
      "         [ 0.8623,  0.8566, -0.8878,  0.8432,  0.8787]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0760 cost = 0.001385\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.6117,  0.1325, -0.3445,  0.5995],\n",
      "         [-0.4413,  0.5966,  0.0153, -0.4053,  0.6421]],\n",
      "\n",
      "        [[ 0.9169,  0.9106, -0.9382,  0.9121,  0.8877],\n",
      "         [ 0.8963,  0.9238, -0.9247,  0.9210,  0.8939]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0761 cost = 0.001544\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8809, -0.9487, -0.9570,  0.7839, -0.0065],\n",
      "         [-0.5107,  0.4765,  0.0993, -0.5196,  0.6806]],\n",
      "\n",
      "        [[-0.7032, -0.4817,  0.7120, -0.1419, -0.8776],\n",
      "         [ 0.8623,  0.8567, -0.8878,  0.8432,  0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0761 cost = 0.001382\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9037, -0.8522, -0.9257,  0.8725, -0.1380],\n",
      "         [ 0.9563, -0.6568, -0.7232,  0.9388, -0.6901]],\n",
      "\n",
      "        [[-0.7459, -0.4864,  0.7592, -0.1564, -0.9088],\n",
      "         [-0.4310, -0.4989,  0.5553, -0.1531, -0.7132]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0761 cost = 0.002387\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9563, -0.6568, -0.7232,  0.9388, -0.6901],\n",
      "         [-0.4414,  0.5968,  0.0155, -0.4051,  0.6423]],\n",
      "\n",
      "        [[-0.4311, -0.4990,  0.5553, -0.1532, -0.7133],\n",
      "         [ 0.8963,  0.9238, -0.9247,  0.9210,  0.8940]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0762 cost = 0.002394\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9037, -0.8523, -0.9257,  0.8725, -0.1382],\n",
      "         [-0.5108,  0.4766,  0.0993, -0.5195,  0.6808]],\n",
      "\n",
      "        [[-0.7459, -0.4865,  0.7593, -0.1565, -0.9089],\n",
      "         [ 0.8624,  0.8567, -0.8879,  0.8433,  0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0762 cost = 0.001338\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.6119,  0.1326, -0.3443,  0.5998],\n",
      "         [ 0.8810, -0.9488, -0.9570,  0.7841, -0.0067]],\n",
      "\n",
      "        [[ 0.9169,  0.9106, -0.9382,  0.9121,  0.8878],\n",
      "         [-0.7033, -0.4817,  0.7122, -0.1422, -0.8776]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0762 cost = 0.000974\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3760,  0.6119,  0.1326, -0.3442,  0.5998],\n",
      "         [ 0.9038, -0.8523, -0.9258,  0.8726, -0.1385]],\n",
      "\n",
      "        [[ 0.9169,  0.9107, -0.9382,  0.9122,  0.8878],\n",
      "         [-0.7460, -0.4866,  0.7593, -0.1567, -0.9089]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0763 cost = 0.000904\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4416,  0.5971,  0.0156, -0.4050,  0.6424],\n",
      "         [-0.5108,  0.4767,  0.0993, -0.5195,  0.6810]],\n",
      "\n",
      "        [[ 0.8963,  0.9238, -0.9247,  0.9210,  0.8941],\n",
      "         [ 0.8624,  0.8568, -0.8879,  0.8433,  0.8789]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0763 cost = 0.003965\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9563, -0.6573, -0.7234,  0.9388, -0.6903],\n",
      "         [ 0.8811, -0.9488, -0.9571,  0.7843, -0.0068]],\n",
      "\n",
      "        [[-0.4313, -0.4992,  0.5555, -0.1534, -0.7134],\n",
      "         [-0.7034, -0.4817,  0.7123, -0.1424, -0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0763 cost = 0.002832\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8812, -0.9488, -0.9571,  0.7844, -0.0069],\n",
      "         [-0.3761,  0.6119,  0.1326, -0.3442,  0.5999]],\n",
      "\n",
      "        [[-0.7034, -0.4818,  0.7123, -0.1425, -0.8777],\n",
      "         [ 0.9169,  0.9107, -0.9382,  0.9122,  0.8879]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0764 cost = 0.000674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9038, -0.8524, -0.9258,  0.8727, -0.1388],\n",
      "         [-0.4417,  0.5973,  0.0157, -0.4049,  0.6425]],\n",
      "\n",
      "        [[-0.7462, -0.4867,  0.7595, -0.1571, -0.9089],\n",
      "         [ 0.8964,  0.9239, -0.9247,  0.9210,  0.8941]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0764 cost = 0.001088\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5110,  0.4768,  0.0993, -0.5195,  0.6813],\n",
      "         [ 0.9563, -0.6575, -0.7235,  0.9389, -0.6904]],\n",
      "\n",
      "        [[ 0.8626,  0.8569, -0.8880,  0.8434,  0.8790],\n",
      "         [-0.4314, -0.4994,  0.5556, -0.1535, -0.7134]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0764 cost = 0.002343\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5111,  0.4768,  0.0993, -0.5195,  0.6813],\n",
      "         [ 0.8813, -0.9488, -0.9571,  0.7846, -0.0071]],\n",
      "\n",
      "        [[ 0.8626,  0.8569, -0.8880,  0.8434,  0.8790],\n",
      "         [-0.7035, -0.4818,  0.7124, -0.1427, -0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0765 cost = 0.001412\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3762,  0.6120,  0.1326, -0.3441,  0.6000],\n",
      "         [-0.4418,  0.5975,  0.0159, -0.4048,  0.6425]],\n",
      "\n",
      "        [[ 0.9170,  0.9107, -0.9383,  0.9122,  0.8880],\n",
      "         [ 0.8964,  0.9239, -0.9247,  0.9211,  0.8942]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0765 cost = 0.001524\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9563, -0.6576, -0.7237,  0.9389, -0.6905],\n",
      "         [ 0.9039, -0.8525, -0.9259,  0.8728, -0.1392]],\n",
      "\n",
      "        [[-0.4315, -0.4995,  0.5557, -0.1536, -0.7135],\n",
      "         [-0.7463, -0.4869,  0.7596, -0.1574, -0.9090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0765 cost = 0.002320\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4420,  0.5976,  0.0159, -0.4047,  0.6426],\n",
      "         [ 0.8814, -0.9489, -0.9571,  0.7848, -0.0072]],\n",
      "\n",
      "        [[ 0.8964,  0.9239, -0.9248,  0.9211,  0.8942],\n",
      "         [-0.7036, -0.4819,  0.7125, -0.1429, -0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0766 cost = 0.001227\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3764,  0.6120,  0.1326, -0.3440,  0.6000],\n",
      "         [ 0.9040, -0.8526, -0.9259,  0.8728, -0.1393]],\n",
      "\n",
      "        [[ 0.9170,  0.9107, -0.9383,  0.9122,  0.8881],\n",
      "         [-0.7464, -0.4870,  0.7597, -0.1576, -0.9090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0766 cost = 0.000895\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5114,  0.4769,  0.0993, -0.5196,  0.6815],\n",
      "         [ 0.9564, -0.6579, -0.7238,  0.9389, -0.6906]],\n",
      "\n",
      "        [[ 0.8627,  0.8570, -0.8880,  0.8435,  0.8792],\n",
      "         [-0.4316, -0.4997,  0.5558, -0.1538, -0.7135]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0766 cost = 0.002329\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9564, -0.6580, -0.7239,  0.9389, -0.6907],\n",
      "         [-0.5115,  0.4769,  0.0993, -0.5196,  0.6815]],\n",
      "\n",
      "        [[-0.4317, -0.4998,  0.5558, -0.1538, -0.7136],\n",
      "         [ 0.8627,  0.8570, -0.8880,  0.8435,  0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0767 cost = 0.002898\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8815, -0.9489, -0.9572,  0.7851, -0.0074],\n",
      "         [ 0.9041, -0.8526, -0.9260,  0.8729, -0.1396]],\n",
      "\n",
      "        [[-0.7037, -0.4821,  0.7126, -0.1432, -0.8777],\n",
      "         [-0.7465, -0.4871,  0.7598, -0.1578, -0.9090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0767 cost = 0.001056\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4423,  0.5979,  0.0161, -0.4046,  0.6427],\n",
      "         [-0.3766,  0.6120,  0.1326, -0.3440,  0.6000]],\n",
      "\n",
      "        [[ 0.8965,  0.9240, -0.9248,  0.9211,  0.8943],\n",
      "         [ 0.9170,  0.9107, -0.9383,  0.9123,  0.8882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0767 cost = 0.001034\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4423,  0.5979,  0.0162, -0.4046,  0.6427],\n",
      "         [ 0.9041, -0.8527, -0.9260,  0.8730, -0.1398]],\n",
      "\n",
      "        [[ 0.8965,  0.9240, -0.9248,  0.9211,  0.8943],\n",
      "         [-0.7465, -0.4872,  0.7599, -0.1580, -0.9090]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0768 cost = 0.001161\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5118,  0.4768,  0.0994, -0.5197,  0.6816],\n",
      "         [ 0.8817, -0.9489, -0.9572,  0.7853, -0.0076]],\n",
      "\n",
      "        [[ 0.8627,  0.8571, -0.8880,  0.8435,  0.8792],\n",
      "         [-0.7038, -0.4822,  0.7127, -0.1434, -0.8777]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0768 cost = 0.001396\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9564, -0.6584, -0.7242,  0.9390, -0.6908],\n",
      "         [-0.3769,  0.6120,  0.1327, -0.3439,  0.6000]],\n",
      "\n",
      "        [[-0.4319, -0.5001,  0.5560, -0.1542, -0.7137],\n",
      "         [ 0.9171,  0.9108, -0.9383,  0.9123,  0.8882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0768 cost = 0.001875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9042, -0.8527, -0.9260,  0.8731, -0.1401],\n",
      "         [ 0.9564, -0.6585, -0.7243,  0.9390, -0.6909]],\n",
      "\n",
      "        [[-0.7466, -0.4874,  0.7600, -0.1582, -0.9090],\n",
      "         [-0.4319, -0.5001,  0.5560, -0.1542, -0.7137]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0769 cost = 0.002338\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4427,  0.5981,  0.0164, -0.4045,  0.6427],\n",
      "         [ 0.8818, -0.9490, -0.9573,  0.7856, -0.0078]],\n",
      "\n",
      "        [[ 0.8965,  0.9240, -0.9248,  0.9212,  0.8944],\n",
      "         [-0.7038, -0.4823,  0.7128, -0.1436, -0.8778]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0769 cost = 0.001213\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3771,  0.6120,  0.1327, -0.3439,  0.6000],\n",
      "         [-0.5121,  0.4768,  0.0994, -0.5198,  0.6816]],\n",
      "\n",
      "        [[ 0.9171,  0.9108, -0.9383,  0.9123,  0.8882],\n",
      "         [ 0.8628,  0.8571, -0.8880,  0.8435,  0.8793]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0769 cost = 0.003826\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9043, -0.8528, -0.9261,  0.8731, -0.1404],\n",
      "         [-0.4428,  0.5982,  0.0165, -0.4044,  0.6427]],\n",
      "\n",
      "        [[-0.7467, -0.4876,  0.7601, -0.1585, -0.9091],\n",
      "         [ 0.8965,  0.9240, -0.9248,  0.9212,  0.8944]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0770 cost = 0.001069\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5122,  0.4769,  0.0994, -0.5198,  0.6817],\n",
      "         [ 0.9565, -0.6588, -0.7245,  0.9390, -0.6910]],\n",
      "\n",
      "        [[ 0.8628,  0.8571, -0.8881,  0.8435,  0.8793],\n",
      "         [-0.4321, -0.5004,  0.5561, -0.1544, -0.7138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0770 cost = 0.002302\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8819, -0.9490, -0.9573,  0.7858, -0.0080],\n",
      "         [-0.3773,  0.6120,  0.1327, -0.3439,  0.6000]],\n",
      "\n",
      "        [[-0.7039, -0.4825,  0.7129, -0.1438, -0.8778],\n",
      "         [ 0.9171,  0.9108, -0.9383,  0.9123,  0.8883]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0770 cost = 0.000660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8819, -0.9490, -0.9573,  0.7859, -0.0080],\n",
      "         [ 0.9565, -0.6589, -0.7246,  0.9391, -0.6911]],\n",
      "\n",
      "        [[-0.7040, -0.4825,  0.7129, -0.1439, -0.8778],\n",
      "         [-0.4321, -0.5005,  0.5562, -0.1545, -0.7138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0771 cost = 0.002182\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5124,  0.4770,  0.0994, -0.5198,  0.6818],\n",
      "         [-0.3774,  0.6120,  0.1327, -0.3438,  0.6000]],\n",
      "\n",
      "        [[ 0.8629,  0.8572, -0.8881,  0.8436,  0.8794],\n",
      "         [ 0.9171,  0.9108, -0.9383,  0.9123,  0.8883]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0771 cost = 0.001414\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9044, -0.8529, -0.9262,  0.8733, -0.1408],\n",
      "         [-0.4431,  0.5986,  0.0167, -0.4042,  0.6428]],\n",
      "\n",
      "        [[-0.7469, -0.4879,  0.7602, -0.1588, -0.9091],\n",
      "         [ 0.8966,  0.9240, -0.9248,  0.9212,  0.8945]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0771 cost = 0.001063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4431,  0.5987,  0.0167, -0.4041,  0.6429],\n",
      "         [-0.3772,  0.6122,  0.1328, -0.3437,  0.6001]],\n",
      "\n",
      "        [[ 0.8966,  0.9240, -0.9248,  0.9212,  0.8945],\n",
      "         [ 0.9171,  0.9108, -0.9383,  0.9123,  0.8884]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0772 cost = 0.001019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8821, -0.9491, -0.9574,  0.7861, -0.0082],\n",
      "         [ 0.9044, -0.8529, -0.9262,  0.8733, -0.1410]],\n",
      "\n",
      "        [[-0.7040, -0.4827,  0.7130, -0.1441, -0.8778],\n",
      "         [-0.7469, -0.4880,  0.7603, -0.1590, -0.9091]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0772 cost = 0.001040\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5124,  0.4773,  0.0994, -0.5195,  0.6820],\n",
      "         [ 0.9565, -0.6591, -0.7248,  0.9391, -0.6912]],\n",
      "\n",
      "        [[ 0.8630,  0.8573, -0.8881,  0.8436,  0.8795],\n",
      "         [-0.4323, -0.5009,  0.5564, -0.1547, -0.7140]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0772 cost = 0.002285\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5124,  0.4773,  0.0994, -0.5194,  0.6821],\n",
      "         [-0.4432,  0.5989,  0.0169, -0.4038,  0.6429]],\n",
      "\n",
      "        [[ 0.8630,  0.8573, -0.8881,  0.8436,  0.8795],\n",
      "         [ 0.8966,  0.9241, -0.9248,  0.9213,  0.8945]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0773 cost = 0.002115\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8821, -0.9491, -0.9574,  0.7863, -0.0083],\n",
      "         [-0.3770,  0.6125,  0.1328, -0.3434,  0.6002]],\n",
      "\n",
      "        [[-0.7041, -0.4829,  0.7131, -0.1443, -0.8779],\n",
      "         [ 0.9172,  0.9108, -0.9383,  0.9124,  0.8884]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0773 cost = 0.000655\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9045, -0.8529, -0.9262,  0.8734, -0.1411],\n",
      "         [ 0.9565, -0.6591, -0.7249,  0.9391, -0.6913]],\n",
      "\n",
      "        [[-0.7470, -0.4882,  0.7604, -0.1592, -0.9092],\n",
      "         [-0.4325, -0.5010,  0.5565, -0.1548, -0.7140]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0773 cost = 0.002307\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9045, -0.8529, -0.9262,  0.8734, -0.1412],\n",
      "         [-0.3769,  0.6126,  0.1328, -0.3432,  0.6002]],\n",
      "\n",
      "        [[-0.7471, -0.4883,  0.7604, -0.1593, -0.9092],\n",
      "         [ 0.9172,  0.9108, -0.9383,  0.9124,  0.8885]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0774 cost = 0.000669\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5123,  0.4776,  0.0994, -0.5191,  0.6822],\n",
      "         [-0.4432,  0.5993,  0.0170, -0.4034,  0.6431]],\n",
      "\n",
      "        [[ 0.8631,  0.8573, -0.8882,  0.8437,  0.8796],\n",
      "         [ 0.8966,  0.9241, -0.9248,  0.9213,  0.8946]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0774 cost = 0.002105\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8822, -0.9491, -0.9574,  0.7865, -0.0085],\n",
      "         [ 0.9565, -0.6592, -0.7250,  0.9391, -0.6914]],\n",
      "\n",
      "        [[-0.7042, -0.4831,  0.7132, -0.1445, -0.8779],\n",
      "         [-0.4326, -0.5013,  0.5566, -0.1549, -0.7141]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0774 cost = 0.002161\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3767,  0.6128,  0.1328, -0.3429,  0.6003],\n",
      "         [ 0.9046, -0.8529, -0.9263,  0.8734, -0.1414]],\n",
      "\n",
      "        [[ 0.9172,  0.9108, -0.9383,  0.9124,  0.8885],\n",
      "         [-0.7472, -0.4885,  0.7605, -0.1595, -0.9092]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0775 cost = 0.000872\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8823, -0.9491, -0.9574,  0.7866, -0.0085],\n",
      "         [ 0.9565, -0.6592, -0.7251,  0.9391, -0.6914]],\n",
      "\n",
      "        [[-0.7042, -0.4832,  0.7132, -0.1446, -0.8779],\n",
      "         [-0.4327, -0.5014,  0.5566, -0.1550, -0.7142]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0775 cost = 0.002157\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4432,  0.5998,  0.0171, -0.4029,  0.6432],\n",
      "         [-0.5122,  0.4779,  0.0994, -0.5187,  0.6824]],\n",
      "\n",
      "        [[ 0.8967,  0.9241, -0.9248,  0.9214,  0.8946],\n",
      "         [ 0.8631,  0.8574, -0.8882,  0.8438,  0.8796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0775 cost = 0.003868\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9046, -0.8529, -0.9263,  0.8735, -0.1417],\n",
      "         [-0.3765,  0.6130,  0.1329, -0.3427,  0.6004]],\n",
      "\n",
      "        [[-0.7473, -0.4888,  0.7606, -0.1597, -0.9093],\n",
      "         [ 0.9172,  0.9109, -0.9383,  0.9124,  0.8886]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0776 cost = 0.000665\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4432,  0.6000,  0.0171, -0.4028,  0.6433],\n",
      "         [ 0.8824, -0.9491, -0.9574,  0.7867, -0.0087]],\n",
      "\n",
      "        [[ 0.8967,  0.9241, -0.9248,  0.9214,  0.8947],\n",
      "         [-0.7043, -0.4834,  0.7133, -0.1447, -0.8780]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0776 cost = 0.001184\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5122,  0.4781,  0.0994, -0.5185,  0.6826],\n",
      "         [ 0.9566, -0.6593, -0.7253,  0.9392, -0.6915]],\n",
      "\n",
      "        [[ 0.8632,  0.8575, -0.8883,  0.8439,  0.8797],\n",
      "         [-0.4329, -0.5017,  0.5568, -0.1551, -0.7143]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0776 cost = 0.002258\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9047, -0.8530, -0.9264,  0.8736, -0.1419],\n",
      "         [-0.5122,  0.4782,  0.0993, -0.5185,  0.6826]],\n",
      "\n",
      "        [[-0.7473, -0.4890,  0.7607, -0.1599, -0.9093],\n",
      "         [ 0.8632,  0.8575, -0.8883,  0.8439,  0.8798]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0777 cost = 0.001281\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9566, -0.6593, -0.7254,  0.9392, -0.6916],\n",
      "         [ 0.8825, -0.9492, -0.9575,  0.7869, -0.0088]],\n",
      "\n",
      "        [[-0.4330, -0.5019,  0.5569, -0.1552, -0.7144],\n",
      "         [-0.7043, -0.4836,  0.7134, -0.1449, -0.8780]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0777 cost = 0.002730\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4433,  0.6003,  0.0172, -0.4025,  0.6434],\n",
      "         [-0.3765,  0.6133,  0.1329, -0.3425,  0.6004]],\n",
      "\n",
      "        [[ 0.8967,  0.9241, -0.9248,  0.9214,  0.8947],\n",
      "         [ 0.9173,  0.9109, -0.9383,  0.9125,  0.8887]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0777 cost = 0.000998\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9566, -0.6594, -0.7255,  0.9392, -0.6916],\n",
      "         [-0.3765,  0.6133,  0.1329, -0.3424,  0.6005]],\n",
      "\n",
      "        [[-0.4331, -0.5020,  0.5570, -0.1553, -0.7144],\n",
      "         [ 0.9173,  0.9109, -0.9383,  0.9125,  0.8887]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0778 cost = 0.001829\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5123,  0.4783,  0.0994, -0.5183,  0.6828],\n",
      "         [-0.4434,  0.6005,  0.0173, -0.4024,  0.6434]],\n",
      "\n",
      "        [[ 0.8633,  0.8575, -0.8883,  0.8439,  0.8798],\n",
      "         [ 0.8967,  0.9242, -0.9248,  0.9214,  0.8948]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0778 cost = 0.002076\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8826, -0.9492, -0.9575,  0.7871, -0.0090],\n",
      "         [ 0.9048, -0.8530, -0.9264,  0.8737, -0.1423]],\n",
      "\n",
      "        [[-0.7044, -0.4838,  0.7135, -0.1451, -0.8780],\n",
      "         [-0.7475, -0.4893,  0.7609, -0.1603, -0.9094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0778 cost = 0.001021\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4434,  0.6007,  0.0173, -0.4022,  0.6435],\n",
      "         [-0.5123,  0.4785,  0.0993, -0.5182,  0.6829]],\n",
      "\n",
      "        [[ 0.8968,  0.9242, -0.9248,  0.9215,  0.8948],\n",
      "         [ 0.8634,  0.8576, -0.8883,  0.8440,  0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0779 cost = 0.003839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8826, -0.9492, -0.9575,  0.7872, -0.0091],\n",
      "         [ 0.9048, -0.8530, -0.9265,  0.8737, -0.1424]],\n",
      "\n",
      "        [[-0.7045, -0.4838,  0.7136, -0.1452, -0.8780],\n",
      "         [-0.7476, -0.4894,  0.7610, -0.1605, -0.9094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0779 cost = 0.001019\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9566, -0.6596, -0.7257,  0.9392, -0.6918],\n",
      "         [-0.3764,  0.6136,  0.1329, -0.3421,  0.6006]],\n",
      "\n",
      "        [[-0.4333, -0.5024,  0.5571, -0.1554, -0.7145],\n",
      "         [ 0.9173,  0.9109, -0.9383,  0.9126,  0.8888]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0779 cost = 0.001821\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8827, -0.9492, -0.9575,  0.7873, -0.0092],\n",
      "         [-0.4436,  0.6011,  0.0173, -0.4019,  0.6436]],\n",
      "\n",
      "        [[-0.7045, -0.4839,  0.7136, -0.1453, -0.8780],\n",
      "         [ 0.8968,  0.9242, -0.9249,  0.9215,  0.8949]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0780 cost = 0.001029\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3764,  0.6137,  0.1329, -0.3421,  0.6007],\n",
      "         [ 0.9049, -0.8530, -0.9265,  0.8737, -0.1425]],\n",
      "\n",
      "        [[ 0.9173,  0.9109, -0.9384,  0.9126,  0.8889],\n",
      "         [-0.7477, -0.4896,  0.7611, -0.1607, -0.9094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0780 cost = 0.000859\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5125,  0.4788,  0.0993, -0.5180,  0.6832],\n",
      "         [ 0.9566, -0.6597, -0.7259,  0.9392, -0.6918]],\n",
      "\n",
      "        [[ 0.8635,  0.8577, -0.8884,  0.8441,  0.8801],\n",
      "         [-0.4334, -0.5025,  0.5572, -0.1555, -0.7146]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0780 cost = 0.002231\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4437,  0.6013,  0.0174, -0.4017,  0.6437],\n",
      "         [ 0.8828, -0.9492, -0.9576,  0.7874, -0.0093]],\n",
      "\n",
      "        [[ 0.8968,  0.9242, -0.9249,  0.9215,  0.8949],\n",
      "         [-0.7045, -0.4840,  0.7137, -0.1455, -0.8781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0781 cost = 0.001165\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9566, -0.6598, -0.7260,  0.9392, -0.6919],\n",
      "         [ 0.9049, -0.8531, -0.9265,  0.8738, -0.1428]],\n",
      "\n",
      "        [[-0.4335, -0.5027,  0.5573, -0.1556, -0.7146],\n",
      "         [-0.7477, -0.4897,  0.7612, -0.1609, -0.9094]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0781 cost = 0.002225\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6138,  0.1330, -0.3420,  0.6007],\n",
      "         [-0.5127,  0.4789,  0.0993, -0.5180,  0.6833]],\n",
      "\n",
      "        [[ 0.9174,  0.9110, -0.9384,  0.9126,  0.8889],\n",
      "         [ 0.8636,  0.8578, -0.8884,  0.8441,  0.8801]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0781 cost = 0.003730\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6139,  0.1329, -0.3419,  0.6007],\n",
      "         [-0.4439,  0.6016,  0.0175, -0.4015,  0.6438]],\n",
      "\n",
      "        [[ 0.9174,  0.9110, -0.9384,  0.9126,  0.8890],\n",
      "         [ 0.8969,  0.9243, -0.9249,  0.9216,  0.8950]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0782 cost = 0.001453\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9050, -0.8531, -0.9266,  0.8738, -0.1430],\n",
      "         [-0.5127,  0.4791,  0.0993, -0.5179,  0.6834]],\n",
      "\n",
      "        [[-0.7478, -0.4899,  0.7612, -0.1611, -0.9095],\n",
      "         [ 0.8636,  0.8578, -0.8885,  0.8442,  0.8802]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0782 cost = 0.001261\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9566, -0.6600, -0.7262,  0.9392, -0.6920],\n",
      "         [ 0.8829, -0.9493, -0.9576,  0.7876, -0.0095]],\n",
      "\n",
      "        [[-0.4336, -0.5029,  0.5574, -0.1558, -0.7147],\n",
      "         [-0.7046, -0.4842,  0.7138, -0.1458, -0.8781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0782 cost = 0.002694\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5128,  0.4792,  0.0993, -0.5178,  0.6836],\n",
      "         [ 0.9566, -0.6600, -0.7263,  0.9392, -0.6920]],\n",
      "\n",
      "        [[ 0.8637,  0.8579, -0.8885,  0.8442,  0.8803],\n",
      "         [-0.4336, -0.5029,  0.5574, -0.1558, -0.7147]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0783 cost = 0.002215\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8830, -0.9493, -0.9576,  0.7878, -0.0096],\n",
      "         [-0.3766,  0.6140,  0.1329, -0.3417,  0.6008]],\n",
      "\n",
      "        [[-0.7046, -0.4843,  0.7138, -0.1459, -0.8781],\n",
      "         [ 0.9174,  0.9110, -0.9384,  0.9127,  0.8891]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0783 cost = 0.000635\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4441,  0.6021,  0.0176, -0.4013,  0.6440],\n",
      "         [ 0.9051, -0.8531, -0.9267,  0.8739, -0.1433]],\n",
      "\n",
      "        [[ 0.8969,  0.9243, -0.9249,  0.9216,  0.8951],\n",
      "         [-0.7479, -0.4901,  0.7614, -0.1614, -0.9095]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0783 cost = 0.001102\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8830, -0.9493, -0.9577,  0.7879, -0.0097],\n",
      "         [ 0.9051, -0.8531, -0.9267,  0.8740, -0.1434]],\n",
      "\n",
      "        [[-0.7047, -0.4844,  0.7139, -0.1460, -0.8781],\n",
      "         [-0.7480, -0.4901,  0.7614, -0.1615, -0.9095]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0784 cost = 0.001005\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3768,  0.6141,  0.1329, -0.3416,  0.6009],\n",
      "         [-0.4441,  0.6022,  0.0176, -0.4012,  0.6440]],\n",
      "\n",
      "        [[ 0.9175,  0.9110, -0.9384,  0.9127,  0.8891],\n",
      "         [ 0.8970,  0.9243, -0.9249,  0.9216,  0.8952]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0784 cost = 0.001441\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9567, -0.6603, -0.7266,  0.9393, -0.6921],\n",
      "         [-0.5130,  0.4794,  0.0992, -0.5178,  0.6838]],\n",
      "\n",
      "        [[-0.4338, -0.5033,  0.5575, -0.1560, -0.7148],\n",
      "         [ 0.8638,  0.8580, -0.8886,  0.8443,  0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0784 cost = 0.002771\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4442,  0.6024,  0.0177, -0.4011,  0.6441],\n",
      "         [-0.3768,  0.6141,  0.1330, -0.3416,  0.6009]],\n",
      "\n",
      "        [[ 0.8970,  0.9243, -0.9249,  0.9217,  0.8952],\n",
      "         [ 0.9175,  0.9110, -0.9384,  0.9127,  0.8892]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0785 cost = 0.000972\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9052, -0.8532, -0.9268,  0.8740, -0.1437],\n",
      "         [ 0.9567, -0.6604, -0.7267,  0.9393, -0.6922]],\n",
      "\n",
      "        [[-0.7481, -0.4903,  0.7615, -0.1618, -0.9095],\n",
      "         [-0.4339, -0.5034,  0.5576, -0.1561, -0.7149]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0785 cost = 0.002237\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5131,  0.4795,  0.0992, -0.5178,  0.6840],\n",
      "         [ 0.8832, -0.9494, -0.9577,  0.7882, -0.0100]],\n",
      "\n",
      "        [[ 0.8638,  0.8580, -0.8886,  0.8443,  0.8804],\n",
      "         [-0.7048, -0.4846,  0.7140, -0.1463, -0.8781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0785 cost = 0.001317\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5132,  0.4795,  0.0992, -0.5178,  0.6840],\n",
      "         [-0.4444,  0.6026,  0.0178, -0.4010,  0.6442]],\n",
      "\n",
      "        [[ 0.8639,  0.8580, -0.8886,  0.8443,  0.8805],\n",
      "         [ 0.8970,  0.9244, -0.9249,  0.9217,  0.8952]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0786 cost = 0.002020\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9052, -0.8532, -0.9268,  0.8741, -0.1440],\n",
      "         [ 0.9567, -0.6606, -0.7269,  0.9393, -0.6923]],\n",
      "\n",
      "        [[-0.7482, -0.4905,  0.7616, -0.1620, -0.9096],\n",
      "         [-0.4340, -0.5035,  0.5577, -0.1563, -0.7149]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0786 cost = 0.002231\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3770,  0.6143,  0.1330, -0.3414,  0.6010],\n",
      "         [ 0.8833, -0.9494, -0.9577,  0.7884, -0.0101]],\n",
      "\n",
      "        [[ 0.9175,  0.9111, -0.9384,  0.9127,  0.8893],\n",
      "         [-0.7048, -0.4847,  0.7141, -0.1465, -0.8781]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0786 cost = 0.000903\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8833, -0.9494, -0.9577,  0.7884, -0.0102],\n",
      "         [ 0.9567, -0.6608, -0.7270,  0.9393, -0.6924]],\n",
      "\n",
      "        [[-0.7049, -0.4848,  0.7141, -0.1465, -0.8782],\n",
      "         [-0.4341, -0.5037,  0.5578, -0.1564, -0.7150]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0787 cost = 0.002091\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9053, -0.8533, -0.9269,  0.8742, -0.1443],\n",
      "         [-0.3770,  0.6144,  0.1330, -0.3413,  0.6010]],\n",
      "\n",
      "        [[-0.7483, -0.4907,  0.7617, -0.1623, -0.9096],\n",
      "         [ 0.9175,  0.9111, -0.9384,  0.9127,  0.8893]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0787 cost = 0.000642\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5132,  0.4797,  0.0993, -0.5176,  0.6842],\n",
      "         [-0.4445,  0.6031,  0.0179, -0.4007,  0.6443]],\n",
      "\n",
      "        [[ 0.8639,  0.8581, -0.8886,  0.8444,  0.8805],\n",
      "         [ 0.8970,  0.9244, -0.9249,  0.9217,  0.8953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0787 cost = 0.002010\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4445,  0.6032,  0.0179, -0.4007,  0.6443],\n",
      "         [ 0.9568, -0.6610, -0.7272,  0.9394, -0.6925]],\n",
      "\n",
      "        [[ 0.8971,  0.9244, -0.9249,  0.9217,  0.8953],\n",
      "         [-0.4342, -0.5039,  0.5579, -0.1565, -0.7151]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0788 cost = 0.002006\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9054, -0.8534, -0.9269,  0.8743, -0.1445],\n",
      "         [-0.3769,  0.6145,  0.1330, -0.3411,  0.6011]],\n",
      "\n",
      "        [[-0.7484, -0.4910,  0.7618, -0.1625, -0.9096],\n",
      "         [ 0.9176,  0.9111, -0.9384,  0.9128,  0.8894]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0788 cost = 0.000640\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8835, -0.9494, -0.9578,  0.7888, -0.0105],\n",
      "         [-0.5132,  0.4799,  0.0993, -0.5174,  0.6843]],\n",
      "\n",
      "        [[-0.7050, -0.4851,  0.7142, -0.1468, -0.8782],\n",
      "         [ 0.8640,  0.8581, -0.8886,  0.8444,  0.8806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0788 cost = 0.001278\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3769,  0.6146,  0.1330, -0.3410,  0.6011],\n",
      "         [ 0.8836, -0.9495, -0.9578,  0.7889, -0.0105]],\n",
      "\n",
      "        [[ 0.9176,  0.9111, -0.9384,  0.9128,  0.8894],\n",
      "         [-0.7050, -0.4851,  0.7143, -0.1469, -0.8782]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0789 cost = 0.000896\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9568, -0.6612, -0.7274,  0.9394, -0.6926],\n",
      "         [-0.5132,  0.4800,  0.0993, -0.5173,  0.6844]],\n",
      "\n",
      "        [[-0.4344, -0.5042,  0.5580, -0.1567, -0.7152],\n",
      "         [ 0.8640,  0.8582, -0.8887,  0.8444,  0.8806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0789 cost = 0.002738\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4445,  0.6036,  0.0180, -0.4003,  0.6445],\n",
      "         [ 0.9055, -0.8534, -0.9270,  0.8744, -0.1449]],\n",
      "\n",
      "        [[ 0.8971,  0.9244, -0.9249,  0.9218,  0.8954],\n",
      "         [-0.7485, -0.4912,  0.7620, -0.1628, -0.9097]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0789 cost = 0.001080\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8837, -0.9495, -0.9578,  0.7891, -0.0107],\n",
      "         [-0.4445,  0.6037,  0.0180, -0.4003,  0.6445]],\n",
      "\n",
      "        [[-0.7051, -0.4852,  0.7144, -0.1471, -0.8783],\n",
      "         [ 0.8971,  0.9244, -0.9249,  0.9218,  0.8954]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0790 cost = 0.000998\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5133,  0.4801,  0.0993, -0.5173,  0.6845],\n",
      "         [-0.3769,  0.6146,  0.1330, -0.3408,  0.6012]],\n",
      "\n",
      "        [[ 0.8640,  0.8582, -0.8887,  0.8445,  0.8807],\n",
      "         [ 0.9176,  0.9111, -0.9384,  0.9128,  0.8895]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0790 cost = 0.001318\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9056, -0.8535, -0.9270,  0.8744, -0.1451],\n",
      "         [ 0.9568, -0.6615, -0.7275,  0.9395, -0.6927]],\n",
      "\n",
      "        [[-0.7486, -0.4913,  0.7621, -0.1630, -0.9097],\n",
      "         [-0.4346, -0.5044,  0.5582, -0.1568, -0.7153]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0790 cost = 0.002202\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3768,  0.6148,  0.1330, -0.3406,  0.6013],\n",
      "         [ 0.9568, -0.6616, -0.7276,  0.9395, -0.6928]],\n",
      "\n",
      "        [[ 0.9176,  0.9111, -0.9384,  0.9128,  0.8895],\n",
      "         [-0.4346, -0.5044,  0.5582, -0.1569, -0.7153]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0791 cost = 0.001738\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5132,  0.4802,  0.0993, -0.5170,  0.6846],\n",
      "         [ 0.8838, -0.9495, -0.9579,  0.7894, -0.0109]],\n",
      "\n",
      "        [[ 0.8641,  0.8583, -0.8887,  0.8445,  0.8807],\n",
      "         [-0.7052, -0.4853,  0.7145, -0.1473, -0.8783]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0791 cost = 0.001293\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4445,  0.6040,  0.0182, -0.3999,  0.6446],\n",
      "         [ 0.9056, -0.8535, -0.9270,  0.8745, -0.1454]],\n",
      "\n",
      "        [[ 0.8971,  0.9244, -0.9249,  0.9218,  0.8954],\n",
      "         [-0.7487, -0.4914,  0.7622, -0.1633, -0.9097]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0791 cost = 0.001074\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9057, -0.8536, -0.9271,  0.8746, -0.1455],\n",
      "         [-0.4445,  0.6041,  0.0183, -0.3998,  0.6446]],\n",
      "\n",
      "        [[-0.7487, -0.4915,  0.7622, -0.1634, -0.9097],\n",
      "         [ 0.8971,  0.9245, -0.9249,  0.9218,  0.8954]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0792 cost = 0.000997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9568, -0.6619, -0.7277,  0.9395, -0.6929],\n",
      "         [-0.5132,  0.4803,  0.0993, -0.5169,  0.6847]],\n",
      "\n",
      "        [[-0.4348, -0.5047,  0.5583, -0.1571, -0.7154],\n",
      "         [ 0.8641,  0.8583, -0.8887,  0.8445,  0.8808]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0792 cost = 0.002717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.6150,  0.1331, -0.3403,  0.6014],\n",
      "         [ 0.8839, -0.9496, -0.9579,  0.7897, -0.0111]],\n",
      "\n",
      "        [[ 0.9176,  0.9111, -0.9384,  0.9128,  0.8896],\n",
      "         [-0.7053, -0.4855,  0.7147, -0.1476, -0.8783]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0792 cost = 0.000886\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.6150,  0.1331, -0.3403,  0.6014],\n",
      "         [ 0.9569, -0.6621, -0.7278,  0.9396, -0.6930]],\n",
      "\n",
      "        [[ 0.9176,  0.9111, -0.9384,  0.9128,  0.8896],\n",
      "         [-0.4349, -0.5048,  0.5584, -0.1572, -0.7155]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0793 cost = 0.001727\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8840, -0.9496, -0.9579,  0.7898, -0.0112],\n",
      "         [-0.4447,  0.6043,  0.0185, -0.3996,  0.6447]],\n",
      "\n",
      "        [[-0.7054, -0.4856,  0.7147, -0.1477, -0.8784],\n",
      "         [ 0.8971,  0.9245, -0.9249,  0.9219,  0.8955]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0793 cost = 0.000988\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5133,  0.4804,  0.0994, -0.5168,  0.6848],\n",
      "         [ 0.9058, -0.8537, -0.9271,  0.8747, -0.1459]],\n",
      "\n",
      "        [[ 0.8642,  0.8583, -0.8887,  0.8445,  0.8808],\n",
      "         [-0.7489, -0.4917,  0.7624, -0.1638, -0.9098]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0793 cost = 0.001177\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4447,  0.6043,  0.0186, -0.3995,  0.6447],\n",
      "         [-0.5133,  0.4804,  0.0994, -0.5168,  0.6848]],\n",
      "\n",
      "        [[ 0.8971,  0.9245, -0.9249,  0.9219,  0.8955],\n",
      "         [ 0.8642,  0.8583, -0.8887,  0.8445,  0.8808]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0794 cost = 0.003725\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8841, -0.9496, -0.9580,  0.7900, -0.0114],\n",
      "         [ 0.9569, -0.6624, -0.7280,  0.9396, -0.6931]],\n",
      "\n",
      "        [[-0.7055, -0.4856,  0.7148, -0.1479, -0.8784],\n",
      "         [-0.4351, -0.5050,  0.5586, -0.1574, -0.7155]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0794 cost = 0.002045\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3767,  0.6150,  0.1332, -0.3401,  0.6015],\n",
      "         [ 0.9058, -0.8537, -0.9271,  0.8748, -0.1462]],\n",
      "\n",
      "        [[ 0.9177,  0.9112, -0.9384,  0.9129,  0.8896],\n",
      "         [-0.7490, -0.4919,  0.7625, -0.1640, -0.9098]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0794 cost = 0.000823\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9569, -0.6625, -0.7280,  0.9396, -0.6932],\n",
      "         [-0.4449,  0.6045,  0.0187, -0.3994,  0.6448]],\n",
      "\n",
      "        [[-0.4352, -0.5051,  0.5586, -0.1575, -0.7156],\n",
      "         [ 0.8972,  0.9245, -0.9249,  0.9219,  0.8955]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0795 cost = 0.002183\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8842, -0.9496, -0.9580,  0.7902, -0.0115],\n",
      "         [-0.3768,  0.6151,  0.1332, -0.3401,  0.6015]],\n",
      "\n",
      "        [[-0.7055, -0.4858,  0.7149, -0.1481, -0.8784],\n",
      "         [ 0.9177,  0.9112, -0.9384,  0.9129,  0.8897]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0795 cost = 0.000612\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5136,  0.4804,  0.0994, -0.5168,  0.6850],\n",
      "         [ 0.9059, -0.8538, -0.9272,  0.8748, -0.1464]],\n",
      "\n",
      "        [[ 0.8643,  0.8584, -0.8887,  0.8446,  0.8809],\n",
      "         [-0.7491, -0.4920,  0.7626, -0.1642, -0.9098]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0795 cost = 0.001169\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3769,  0.6151,  0.1332, -0.3401,  0.6015],\n",
      "         [ 0.9059, -0.8538, -0.9272,  0.8749, -0.1465]],\n",
      "\n",
      "        [[ 0.9177,  0.9112, -0.9384,  0.9129,  0.8897],\n",
      "         [-0.7491, -0.4921,  0.7626, -0.1643, -0.9098]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0796 cost = 0.000820\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8843, -0.9496, -0.9580,  0.7904, -0.0116],\n",
      "         [-0.4452,  0.6047,  0.0189, -0.3993,  0.6448]],\n",
      "\n",
      "        [[-0.7056, -0.4859,  0.7150, -0.1483, -0.8784],\n",
      "         [ 0.8972,  0.9245, -0.9249,  0.9219,  0.8956]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0796 cost = 0.000979\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9569, -0.6628, -0.7282,  0.9397, -0.6934],\n",
      "         [-0.5138,  0.4804,  0.0994, -0.5169,  0.6850]],\n",
      "\n",
      "        [[-0.4354, -0.5054,  0.5588, -0.1577, -0.7157],\n",
      "         [ 0.8643,  0.8585, -0.8888,  0.8446,  0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0796 cost = 0.002687\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5139,  0.4804,  0.0994, -0.5169,  0.6850],\n",
      "         [-0.3771,  0.6150,  0.1332, -0.3400,  0.6016]],\n",
      "\n",
      "        [[ 0.8643,  0.8585, -0.8888,  0.8447,  0.8810],\n",
      "         [ 0.9177,  0.9112, -0.9384,  0.9129,  0.8898]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0797 cost = 0.001291\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9060, -0.8538, -0.9272,  0.8750, -0.1468],\n",
      "         [-0.4453,  0.6048,  0.0191, -0.3991,  0.6449]],\n",
      "\n",
      "        [[-0.7492, -0.4922,  0.7628, -0.1646, -0.9099],\n",
      "         [ 0.8972,  0.9246, -0.9249,  0.9219,  0.8956]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0797 cost = 0.000981\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9569, -0.6630, -0.7283,  0.9397, -0.6935],\n",
      "         [ 0.8844, -0.9497, -0.9580,  0.7905, -0.0118]],\n",
      "\n",
      "        [[-0.4355, -0.5055,  0.5589, -0.1578, -0.7157],\n",
      "         [-0.7057, -0.4859,  0.7151, -0.1485, -0.8784]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0797 cost = 0.002585\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8844, -0.9497, -0.9580,  0.7906, -0.0119],\n",
      "         [ 0.9060, -0.8539, -0.9272,  0.8750, -0.1470]],\n",
      "\n",
      "        [[-0.7057, -0.4860,  0.7151, -0.1485, -0.8784],\n",
      "         [-0.7493, -0.4923,  0.7628, -0.1647, -0.9099]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0798 cost = 0.000964\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4453,  0.6049,  0.0192, -0.3989,  0.6449],\n",
      "         [-0.3768,  0.6152,  0.1333, -0.3398,  0.6017]],\n",
      "\n",
      "        [[ 0.8972,  0.9246, -0.9249,  0.9220,  0.8957],\n",
      "         [ 0.9177,  0.9112, -0.9384,  0.9129,  0.8898]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0798 cost = 0.000932\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9570, -0.6634, -0.7284,  0.9397, -0.6936],\n",
      "         [-0.5139,  0.4806,  0.0995, -0.5167,  0.6852]],\n",
      "\n",
      "        [[-0.4356, -0.5056,  0.5590, -0.1580, -0.7158],\n",
      "         [ 0.8644,  0.8586, -0.8888,  0.8447,  0.8811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0798 cost = 0.002674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5139,  0.4807,  0.0995, -0.5166,  0.6853],\n",
      "         [ 0.9061, -0.8539, -0.9273,  0.8751, -0.1473]],\n",
      "\n",
      "        [[ 0.8644,  0.8586, -0.8888,  0.8447,  0.8811],\n",
      "         [-0.7494, -0.4924,  0.7629, -0.1650, -0.9099]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0799 cost = 0.001158\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9570, -0.6636, -0.7285,  0.9397, -0.6937],\n",
      "         [-0.4454,  0.6051,  0.0194, -0.3987,  0.6450]],\n",
      "\n",
      "        [[-0.4357, -0.5057,  0.5590, -0.1581, -0.7158],\n",
      "         [ 0.8972,  0.9246, -0.9249,  0.9220,  0.8957]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0799 cost = 0.002157\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3767,  0.6154,  0.1333, -0.3396,  0.6019],\n",
      "         [ 0.8846, -0.9497, -0.9581,  0.7909, -0.0121]],\n",
      "\n",
      "        [[ 0.9178,  0.9112, -0.9384,  0.9129,  0.8899],\n",
      "         [-0.7059, -0.4861,  0.7153, -0.1488, -0.8785]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0799 cost = 0.000868\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8846, -0.9497, -0.9581,  0.7910, -0.0122],\n",
      "         [ 0.9570, -0.6638, -0.7286,  0.9398, -0.6938]],\n",
      "\n",
      "        [[-0.7059, -0.4861,  0.7153, -0.1489, -0.8785],\n",
      "         [-0.4357, -0.5058,  0.5591, -0.1582, -0.7159]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0800 cost = 0.002014\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4455,  0.6052,  0.0195, -0.3986,  0.6451],\n",
      "         [-0.3768,  0.6154,  0.1333, -0.3395,  0.6019]],\n",
      "\n",
      "        [[ 0.8973,  0.9246, -0.9249,  0.9220,  0.8957],\n",
      "         [ 0.9178,  0.9112, -0.9384,  0.9129,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0800 cost = 0.000926\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9062, -0.8540, -0.9273,  0.8752, -0.1478],\n",
      "         [-0.5140,  0.4808,  0.0995, -0.5166,  0.6854]],\n",
      "\n",
      "        [[-0.7495, -0.4926,  0.7631, -0.1653, -0.9100],\n",
      "         [ 0.8645,  0.8586, -0.8888,  0.8447,  0.8811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0800 cost = 0.001197\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8847, -0.9498, -0.9581,  0.7912, -0.0124],\n",
      "         [ 0.9062, -0.8541, -0.9273,  0.8752, -0.1478]],\n",
      "\n",
      "        [[-0.7060, -0.4862,  0.7154, -0.1491, -0.8785],\n",
      "         [-0.7496, -0.4926,  0.7631, -0.1654, -0.9100]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0801 cost = 0.000956\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9570, -0.6643, -0.7288,  0.9398, -0.6939],\n",
      "         [-0.5140,  0.4808,  0.0996, -0.5166,  0.6855]],\n",
      "\n",
      "        [[-0.4359, -0.5060,  0.5592, -0.1585, -0.7159],\n",
      "         [ 0.8645,  0.8586, -0.8888,  0.8447,  0.8812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0801 cost = 0.002654\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3768,  0.6155,  0.1334, -0.3395,  0.6020],\n",
      "         [-0.4457,  0.6053,  0.0198, -0.3984,  0.6451]],\n",
      "\n",
      "        [[ 0.9178,  0.9112, -0.9384,  0.9130,  0.8899],\n",
      "         [ 0.8973,  0.9246, -0.9249,  0.9220,  0.8957]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0801 cost = 0.001380\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9570, -0.6646, -0.7289,  0.9398, -0.6940],\n",
      "         [-0.5141,  0.4808,  0.0996, -0.5166,  0.6856]],\n",
      "\n",
      "        [[-0.4359, -0.5060,  0.5593, -0.1586, -0.7160],\n",
      "         [ 0.8645,  0.8587, -0.8888,  0.8447,  0.8812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0802 cost = 0.002649\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9063, -0.8542, -0.9274,  0.8753, -0.1482],\n",
      "         [-0.3768,  0.6155,  0.1334, -0.3394,  0.6021]],\n",
      "\n",
      "        [[-0.7497, -0.4927,  0.7633, -0.1658, -0.9100],\n",
      "         [ 0.9178,  0.9113, -0.9384,  0.9130,  0.8900]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0802 cost = 0.000614\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8849, -0.9498, -0.9582,  0.7915, -0.0127],\n",
      "         [-0.4458,  0.6054,  0.0199, -0.3983,  0.6452]],\n",
      "\n",
      "        [[-0.7061, -0.4863,  0.7156, -0.1494, -0.8785],\n",
      "         [ 0.8973,  0.9246, -0.9249,  0.9220,  0.8958]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0802 cost = 0.000960\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3769,  0.6155,  0.1334, -0.3394,  0.6022],\n",
      "         [-0.5142,  0.4809,  0.0996, -0.5166,  0.6857]],\n",
      "\n",
      "        [[ 0.9178,  0.9113, -0.9384,  0.9130,  0.8900],\n",
      "         [ 0.8645,  0.8587, -0.8888,  0.8447,  0.8812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0803 cost = 0.003604\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8849, -0.9498, -0.9582,  0.7916, -0.0127],\n",
      "         [ 0.9571, -0.6650, -0.7291,  0.9399, -0.6942]],\n",
      "\n",
      "        [[-0.7061, -0.4863,  0.7156, -0.1495, -0.8785],\n",
      "         [-0.4361, -0.5061,  0.5594, -0.1588, -0.7160]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0803 cost = 0.001995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4459,  0.6056,  0.0200, -0.3982,  0.6453],\n",
      "         [ 0.9064, -0.8543, -0.9274,  0.8754, -0.1486]],\n",
      "\n",
      "        [[ 0.8973,  0.9247, -0.9249,  0.9221,  0.8958],\n",
      "         [-0.7498, -0.4928,  0.7634, -0.1661, -0.9100]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0803 cost = 0.001034\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5143,  0.4810,  0.0996, -0.5165,  0.6859],\n",
      "         [-0.3769,  0.6156,  0.1334, -0.3393,  0.6023]],\n",
      "\n",
      "        [[ 0.8646,  0.8588, -0.8889,  0.8448,  0.8813],\n",
      "         [ 0.9178,  0.9113, -0.9384,  0.9130,  0.8901]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0804 cost = 0.001264\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9571, -0.6652, -0.7292,  0.9399, -0.6942],\n",
      "         [-0.4460,  0.6057,  0.0201, -0.3981,  0.6454]],\n",
      "\n",
      "        [[-0.4362, -0.5062,  0.5595, -0.1589, -0.7161],\n",
      "         [ 0.8973,  0.9247, -0.9249,  0.9221,  0.8959]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0804 cost = 0.002125\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9064, -0.8543, -0.9275,  0.8755, -0.1488],\n",
      "         [ 0.8850, -0.9498, -0.9582,  0.7917, -0.0129]],\n",
      "\n",
      "        [[-0.7499, -0.4929,  0.7635, -0.1663, -0.9101],\n",
      "         [-0.7062, -0.4864,  0.7158, -0.1497, -0.8786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0804 cost = 0.001674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.6159,  0.1335, -0.3390,  0.6025],\n",
      "         [ 0.9064, -0.8543, -0.9275,  0.8755, -0.1489]],\n",
      "\n",
      "        [[ 0.9179,  0.9113, -0.9385,  0.9130,  0.8902],\n",
      "         [-0.7499, -0.4929,  0.7635, -0.1664, -0.9101]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0805 cost = 0.000799\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4460,  0.6060,  0.0202, -0.3978,  0.6455],\n",
      "         [ 0.9571, -0.6655, -0.7293,  0.9399, -0.6944]],\n",
      "\n",
      "        [[ 0.8974,  0.9247, -0.9249,  0.9221,  0.8959],\n",
      "         [-0.4364, -0.5064,  0.5596, -0.1590, -0.7161]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0805 cost = 0.001901\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5141,  0.4814,  0.0996, -0.5162,  0.6863],\n",
      "         [ 0.8851, -0.9499, -0.9582,  0.7919, -0.0131]],\n",
      "\n",
      "        [[ 0.8647,  0.8589, -0.8889,  0.8449,  0.8814],\n",
      "         [-0.7063, -0.4865,  0.7158, -0.1499, -0.8786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0805 cost = 0.001237\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6160,  0.1335, -0.3389,  0.6026],\n",
      "         [-0.4460,  0.6061,  0.0203, -0.3977,  0.6456]],\n",
      "\n",
      "        [[ 0.9179,  0.9113, -0.9385,  0.9130,  0.8902],\n",
      "         [ 0.8974,  0.9247, -0.9249,  0.9221,  0.8959]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0806 cost = 0.001364\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9065, -0.8544, -0.9275,  0.8756, -0.1493],\n",
      "         [ 0.9571, -0.6657, -0.7294,  0.9399, -0.6945]],\n",
      "\n",
      "        [[-0.7501, -0.4931,  0.7637, -0.1667, -0.9101],\n",
      "         [-0.4365, -0.5065,  0.5597, -0.1592, -0.7162]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0806 cost = 0.002106\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5141,  0.4815,  0.0996, -0.5161,  0.6864],\n",
      "         [ 0.8852, -0.9499, -0.9583,  0.7921, -0.0133]],\n",
      "\n",
      "        [[ 0.8648,  0.8590, -0.8889,  0.8449,  0.8815],\n",
      "         [-0.7064, -0.4867,  0.7159, -0.1501, -0.8786]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0806 cost = 0.001233\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9571, -0.6659, -0.7295,  0.9399, -0.6945],\n",
      "         [-0.5142,  0.4816,  0.0996, -0.5161,  0.6864]],\n",
      "\n",
      "        [[-0.4366, -0.5067,  0.5598, -0.1592, -0.7162],\n",
      "         [ 0.8648,  0.8590, -0.8889,  0.8449,  0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0807 cost = 0.002613\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8853, -0.9499, -0.9583,  0.7923, -0.0134],\n",
      "         [-0.3765,  0.6161,  0.1335, -0.3387,  0.6027]],\n",
      "\n",
      "        [[-0.7064, -0.4868,  0.7160, -0.1502, -0.8787],\n",
      "         [ 0.9179,  0.9114, -0.9385,  0.9131,  0.8903]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0807 cost = 0.000591\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4461,  0.6064,  0.0205, -0.3975,  0.6457],\n",
      "         [ 0.9066, -0.8545, -0.9276,  0.8757, -0.1497]],\n",
      "\n",
      "        [[ 0.8974,  0.9247, -0.9249,  0.9222,  0.8960],\n",
      "         [-0.7502, -0.4934,  0.7638, -0.1670, -0.9102]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0807 cost = 0.001020\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8853, -0.9499, -0.9583,  0.7925, -0.0135],\n",
      "         [-0.5142,  0.4816,  0.0996, -0.5161,  0.6865]],\n",
      "\n",
      "        [[-0.7065, -0.4868,  0.7161, -0.1504, -0.8787],\n",
      "         [ 0.8648,  0.8590, -0.8890,  0.8449,  0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0808 cost = 0.001209\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9571, -0.6663, -0.7297,  0.9400, -0.6947],\n",
      "         [-0.3766,  0.6162,  0.1335, -0.3386,  0.6027]],\n",
      "\n",
      "        [[-0.4368, -0.5069,  0.5599, -0.1594, -0.7163],\n",
      "         [ 0.9179,  0.9114, -0.9385,  0.9131,  0.8903]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0808 cost = 0.001679\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4462,  0.6065,  0.0206, -0.3974,  0.6457],\n",
      "         [ 0.9067, -0.8546, -0.9276,  0.8758, -0.1500]],\n",
      "\n",
      "        [[ 0.8974,  0.9248, -0.9249,  0.9222,  0.8960],\n",
      "         [-0.7503, -0.4935,  0.7639, -0.1672, -0.9102]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0808 cost = 0.001017\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9067, -0.8546, -0.9276,  0.8758, -0.1501],\n",
      "         [-0.3766,  0.6162,  0.1335, -0.3385,  0.6028]],\n",
      "\n",
      "        [[-0.7503, -0.4935,  0.7639, -0.1673, -0.9102],\n",
      "         [ 0.9179,  0.9114, -0.9385,  0.9131,  0.8903]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0809 cost = 0.000601\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9572, -0.6666, -0.7298,  0.9400, -0.6948],\n",
      "         [ 0.8855, -0.9500, -0.9584,  0.7928, -0.0138]],\n",
      "\n",
      "        [[-0.4369, -0.5070,  0.5600, -0.1596, -0.7164],\n",
      "         [-0.7066, -0.4869,  0.7162, -0.1506, -0.8787]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0809 cost = 0.002500\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5144,  0.4817,  0.0997, -0.5161,  0.6866],\n",
      "         [-0.4463,  0.6066,  0.0208, -0.3974,  0.6458]],\n",
      "\n",
      "        [[ 0.8649,  0.8591, -0.8890,  0.8449,  0.8816],\n",
      "         [ 0.8974,  0.9248, -0.9249,  0.9222,  0.8960]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0809 cost = 0.001894\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3767,  0.6162,  0.1336, -0.3385,  0.6028],\n",
      "         [-0.5144,  0.4817,  0.0997, -0.5161,  0.6866]],\n",
      "\n",
      "        [[ 0.9179,  0.9114, -0.9385,  0.9131,  0.8904],\n",
      "         [ 0.8649,  0.8591, -0.8890,  0.8449,  0.8816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0810 cost = 0.003562\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9068, -0.8547, -0.9277,  0.8759, -0.1505],\n",
      "         [-0.4463,  0.6068,  0.0208, -0.3972,  0.6458]],\n",
      "\n",
      "        [[-0.7504, -0.4937,  0.7641, -0.1676, -0.9102],\n",
      "         [ 0.8974,  0.9248, -0.9249,  0.9222,  0.8961]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0810 cost = 0.000942\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9572, -0.6670, -0.7300,  0.9401, -0.6949],\n",
      "         [ 0.8856, -0.9500, -0.9584,  0.7930, -0.0140]],\n",
      "\n",
      "        [[-0.4370, -0.5072,  0.5602, -0.1598, -0.7164],\n",
      "         [-0.7067, -0.4870,  0.7163, -0.1508, -0.8787]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0810 cost = 0.002490\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5144,  0.4820,  0.0997, -0.5159,  0.6868],\n",
      "         [-0.4463,  0.6070,  0.0208, -0.3971,  0.6459]],\n",
      "\n",
      "        [[ 0.8650,  0.8592, -0.8890,  0.8450,  0.8817],\n",
      "         [ 0.8975,  0.9248, -0.9249,  0.9222,  0.8961]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0811 cost = 0.001885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9572, -0.6671, -0.7301,  0.9401, -0.6950],\n",
      "         [ 0.9069, -0.8548, -0.9278,  0.8760, -0.1507]],\n",
      "\n",
      "        [[-0.4371, -0.5073,  0.5602, -0.1598, -0.7165],\n",
      "         [-0.7505, -0.4938,  0.7642, -0.1678, -0.9103]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0811 cost = 0.002046\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8857, -0.9500, -0.9584,  0.7932, -0.0142],\n",
      "         [-0.3765,  0.6165,  0.1335, -0.3380,  0.6030]],\n",
      "\n",
      "        [[-0.7067, -0.4871,  0.7164, -0.1510, -0.8788],\n",
      "         [ 0.9180,  0.9114, -0.9385,  0.9132,  0.8905]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0811 cost = 0.000583\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9572, -0.6673, -0.7303,  0.9401, -0.6951],\n",
      "         [-0.4462,  0.6074,  0.0209, -0.3968,  0.6460]],\n",
      "\n",
      "        [[-0.4372, -0.5074,  0.5603, -0.1599, -0.7165],\n",
      "         [ 0.8975,  0.9248, -0.9249,  0.9223,  0.8962]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0812 cost = 0.002079\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8858, -0.9501, -0.9585,  0.7934, -0.0143],\n",
      "         [-0.3764,  0.6166,  0.1335, -0.3379,  0.6030]],\n",
      "\n",
      "        [[-0.7068, -0.4872,  0.7164, -0.1511, -0.8788],\n",
      "         [ 0.9180,  0.9114, -0.9385,  0.9132,  0.8906]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0812 cost = 0.000582\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5142,  0.4824,  0.0997, -0.5156,  0.6871],\n",
      "         [ 0.9070, -0.8549, -0.9279,  0.8761, -0.1511]],\n",
      "\n",
      "        [[ 0.8651,  0.8593, -0.8891,  0.8451,  0.8818],\n",
      "         [-0.7506, -0.4940,  0.7643, -0.1680, -0.9103]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0812 cost = 0.001112\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3764,  0.6167,  0.1336, -0.3377,  0.6031],\n",
      "         [-0.4462,  0.6077,  0.0209, -0.3966,  0.6461]],\n",
      "\n",
      "        [[ 0.9180,  0.9115, -0.9385,  0.9132,  0.8906],\n",
      "         [ 0.8975,  0.9249, -0.9249,  0.9223,  0.8962]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0813 cost = 0.001338\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9070, -0.8549, -0.9279,  0.8761, -0.1513],\n",
      "         [ 0.8859, -0.9501, -0.9585,  0.7935, -0.0145]],\n",
      "\n",
      "        [[-0.7507, -0.4941,  0.7643, -0.1681, -0.9103],\n",
      "         [-0.7068, -0.4873,  0.7165, -0.1513, -0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0813 cost = 0.001629\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9573, -0.6679, -0.7306,  0.9401, -0.6952],\n",
      "         [-0.5142,  0.4826,  0.0997, -0.5154,  0.6872]],\n",
      "\n",
      "        [[-0.4373, -0.5077,  0.5604, -0.1601, -0.7166],\n",
      "         [ 0.8651,  0.8593, -0.8891,  0.8451,  0.8819]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0813 cost = 0.002568\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3763,  0.6168,  0.1336, -0.3376,  0.6032],\n",
      "         [ 0.9071, -0.8549, -0.9279,  0.8762, -0.1515]],\n",
      "\n",
      "        [[ 0.9180,  0.9115, -0.9385,  0.9132,  0.8907],\n",
      "         [-0.7507, -0.4942,  0.7644, -0.1682, -0.9103]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0814 cost = 0.000777\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5142,  0.4827,  0.0997, -0.5154,  0.6873],\n",
      "         [ 0.8860, -0.9501, -0.9585,  0.7938, -0.0147]],\n",
      "\n",
      "        [[ 0.8651,  0.8593, -0.8891,  0.8451,  0.8819],\n",
      "         [-0.7069, -0.4874,  0.7166, -0.1514, -0.8788]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0814 cost = 0.001203\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4462,  0.6080,  0.0210, -0.3964,  0.6463],\n",
      "         [ 0.9573, -0.6682, -0.7308,  0.9401, -0.6954]],\n",
      "\n",
      "        [[ 0.8976,  0.9249, -0.9250,  0.9223,  0.8963],\n",
      "         [-0.4374, -0.5078,  0.5605, -0.1603, -0.7166]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0814 cost = 0.001848\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5143,  0.4827,  0.0997, -0.5154,  0.6873],\n",
      "         [ 0.8861, -0.9501, -0.9586,  0.7940, -0.0148]],\n",
      "\n",
      "        [[ 0.8652,  0.8593, -0.8891,  0.8451,  0.8819],\n",
      "         [-0.7069, -0.4875,  0.7167, -0.1515, -0.8789]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0815 cost = 0.001200\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9072, -0.8550, -0.9280,  0.8763, -0.1519],\n",
      "         [-0.4463,  0.6082,  0.0210, -0.3963,  0.6463]],\n",
      "\n",
      "        [[-0.7509, -0.4944,  0.7645, -0.1685, -0.9104],\n",
      "         [ 0.8976,  0.9249, -0.9250,  0.9223,  0.8963]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0815 cost = 0.000928\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6168,  0.1336, -0.3373,  0.6032],\n",
      "         [ 0.9573, -0.6685, -0.7310,  0.9402, -0.6955]],\n",
      "\n",
      "        [[ 0.9181,  0.9115, -0.9385,  0.9132,  0.8907],\n",
      "         [-0.4375, -0.5080,  0.5606, -0.1604, -0.7167]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0815 cost = 0.001616\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4463,  0.6083,  0.0211, -0.3963,  0.6463],\n",
      "         [-0.5145,  0.4827,  0.0997, -0.5153,  0.6874]],\n",
      "\n",
      "        [[ 0.8976,  0.9249, -0.9250,  0.9224,  0.8963],\n",
      "         [ 0.8652,  0.8594, -0.8891,  0.8451,  0.8820]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0816 cost = 0.003585\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9072, -0.8551, -0.9280,  0.8763, -0.1521],\n",
      "         [ 0.9573, -0.6686, -0.7311,  0.9402, -0.6955]],\n",
      "\n",
      "        [[-0.7510, -0.4945,  0.7646, -0.1687, -0.9104],\n",
      "         [-0.4376, -0.5081,  0.5606, -0.1605, -0.7167]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0816 cost = 0.002047\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8863, -0.9502, -0.9586,  0.7944, -0.0151],\n",
      "         [-0.3766,  0.6168,  0.1336, -0.3373,  0.6032]],\n",
      "\n",
      "        [[-0.7070, -0.4877,  0.7168, -0.1518, -0.8789],\n",
      "         [ 0.9181,  0.9115, -0.9385,  0.9132,  0.8908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0816 cost = 0.000574\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8864, -0.9502, -0.9586,  0.7944, -0.0152],\n",
      "         [-0.3767,  0.6168,  0.1336, -0.3373,  0.6032]],\n",
      "\n",
      "        [[-0.7071, -0.4878,  0.7168, -0.1519, -0.8789],\n",
      "         [ 0.9181,  0.9115, -0.9385,  0.9132,  0.8908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0817 cost = 0.000573\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4465,  0.6085,  0.0212, -0.3961,  0.6464],\n",
      "         [ 0.9073, -0.8551, -0.9281,  0.8764, -0.1524]],\n",
      "\n",
      "        [[ 0.8976,  0.9249, -0.9250,  0.9224,  0.8964],\n",
      "         [-0.7510, -0.4947,  0.7647, -0.1689, -0.9104]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0817 cost = 0.000990\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5148,  0.4828,  0.0997, -0.5154,  0.6875],\n",
      "         [ 0.9574, -0.6688, -0.7312,  0.9402, -0.6956]],\n",
      "\n",
      "        [[ 0.8653,  0.8595, -0.8892,  0.8452,  0.8821],\n",
      "         [-0.4377, -0.5084,  0.5607, -0.1607, -0.7168]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0817 cost = 0.002002\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3769,  0.6169,  0.1336, -0.3372,  0.6032],\n",
      "         [-0.4466,  0.6086,  0.0213, -0.3961,  0.6464]],\n",
      "\n",
      "        [[ 0.9181,  0.9115, -0.9385,  0.9133,  0.8909],\n",
      "         [ 0.8976,  0.9249, -0.9250,  0.9224,  0.8964]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0818 cost = 0.001320\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9074, -0.8552, -0.9281,  0.8765, -0.1526],\n",
      "         [-0.5149,  0.4828,  0.0997, -0.5154,  0.6876]],\n",
      "\n",
      "        [[-0.7511, -0.4949,  0.7648, -0.1691, -0.9104],\n",
      "         [ 0.8653,  0.8595, -0.8892,  0.8452,  0.8821]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0818 cost = 0.001137\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8865, -0.9503, -0.9587,  0.7947, -0.0154],\n",
      "         [ 0.9574, -0.6689, -0.7314,  0.9402, -0.6957]],\n",
      "\n",
      "        [[-0.7072, -0.4880,  0.7170, -0.1522, -0.8789],\n",
      "         [-0.4378, -0.5085,  0.5608, -0.1608, -0.7169]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0818 cost = 0.001912\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9574, -0.6689, -0.7314,  0.9403, -0.6957],\n",
      "         [-0.4468,  0.6088,  0.0214, -0.3960,  0.6464]],\n",
      "\n",
      "        [[-0.4379, -0.5086,  0.5609, -0.1608, -0.7169],\n",
      "         [ 0.8977,  0.9250, -0.9250,  0.9224,  0.8965]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0819 cost = 0.002038\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9075, -0.8552, -0.9282,  0.8766, -0.1529],\n",
      "         [ 0.8866, -0.9503, -0.9587,  0.7948, -0.0155]],\n",
      "\n",
      "        [[-0.7512, -0.4951,  0.7649, -0.1694, -0.9105],\n",
      "         [-0.7072, -0.4880,  0.7170, -0.1523, -0.8790]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0819 cost = 0.001598\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5151,  0.4828,  0.0997, -0.5154,  0.6877],\n",
      "         [-0.3771,  0.6169,  0.1337, -0.3372,  0.6032]],\n",
      "\n",
      "        [[ 0.8654,  0.8595, -0.8892,  0.8452,  0.8822],\n",
      "         [ 0.9182,  0.9115, -0.9385,  0.9133,  0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0819 cost = 0.001204\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8867, -0.9503, -0.9587,  0.7950, -0.0157],\n",
      "         [-0.4469,  0.6089,  0.0215, -0.3958,  0.6465]],\n",
      "\n",
      "        [[-0.7073, -0.4882,  0.7171, -0.1524, -0.8790],\n",
      "         [ 0.8977,  0.9250, -0.9250,  0.9225,  0.8965]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0820 cost = 0.000911\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9075, -0.8553, -0.9282,  0.8766, -0.1532],\n",
      "         [-0.3770,  0.6170,  0.1337, -0.3370,  0.6032]],\n",
      "\n",
      "        [[-0.7513, -0.4953,  0.7650, -0.1696, -0.9105],\n",
      "         [ 0.9182,  0.9115, -0.9385,  0.9133,  0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0820 cost = 0.000581\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5150,  0.4830,  0.0997, -0.5152,  0.6878],\n",
      "         [ 0.9574, -0.6693, -0.7316,  0.9403, -0.6959]],\n",
      "\n",
      "        [[ 0.8654,  0.8596, -0.8892,  0.8453,  0.8822],\n",
      "         [-0.4381, -0.5089,  0.5610, -0.1611, -0.7170]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0820 cost = 0.001984\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4468,  0.6091,  0.0216, -0.3956,  0.6465],\n",
      "         [ 0.8868, -0.9503, -0.9587,  0.7952, -0.0158]],\n",
      "\n",
      "        [[ 0.8977,  0.9250, -0.9250,  0.9225,  0.8965],\n",
      "         [-0.7074, -0.4883,  0.7172, -0.1526, -0.8790]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0821 cost = 0.001020\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9076, -0.8554, -0.9283,  0.8767, -0.1535],\n",
      "         [ 0.9574, -0.6694, -0.7317,  0.9403, -0.6960]],\n",
      "\n",
      "        [[-0.7514, -0.4954,  0.7651, -0.1698, -0.9105],\n",
      "         [-0.4382, -0.5091,  0.5611, -0.1612, -0.7171]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0821 cost = 0.002018\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5150,  0.4831,  0.0998, -0.5151,  0.6878],\n",
      "         [-0.3768,  0.6171,  0.1337, -0.3368,  0.6033]],\n",
      "\n",
      "        [[ 0.8654,  0.8596, -0.8892,  0.8453,  0.8823],\n",
      "         [ 0.9182,  0.9115, -0.9385,  0.9133,  0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0821 cost = 0.001197\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5149,  0.4831,  0.0998, -0.5150,  0.6879],\n",
      "         [ 0.9076, -0.8554, -0.9283,  0.8768, -0.1537]],\n",
      "\n",
      "        [[ 0.8654,  0.8596, -0.8892,  0.8453,  0.8823],\n",
      "         [-0.7515, -0.4956,  0.7652, -0.1699, -0.9106]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0822 cost = 0.001081\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.6173,  0.1337, -0.3366,  0.6033],\n",
      "         [ 0.8869, -0.9504, -0.9588,  0.7955, -0.0161]],\n",
      "\n",
      "        [[ 0.9182,  0.9116, -0.9385,  0.9133,  0.8910],\n",
      "         [-0.7075, -0.4886,  0.7173, -0.1528, -0.8791]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0822 cost = 0.000808\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9575, -0.6696, -0.7319,  0.9404, -0.6961],\n",
      "         [-0.4468,  0.6095,  0.0218, -0.3952,  0.6466]],\n",
      "\n",
      "        [[-0.4384, -0.5094,  0.5612, -0.1613, -0.7172],\n",
      "         [ 0.8977,  0.9250, -0.9250,  0.9225,  0.8966]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0822 cost = 0.002017\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6174,  0.1337, -0.3365,  0.6034],\n",
      "         [ 0.9077, -0.8554, -0.9283,  0.8769, -0.1539]],\n",
      "\n",
      "        [[ 0.9182,  0.9116, -0.9385,  0.9133,  0.8910],\n",
      "         [-0.7516, -0.4958,  0.7653, -0.1702, -0.9106]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0823 cost = 0.000757\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8870, -0.9504, -0.9588,  0.7958, -0.0163],\n",
      "         [ 0.9575, -0.6697, -0.7320,  0.9404, -0.6962]],\n",
      "\n",
      "        [[-0.7075, -0.4887,  0.7174, -0.1530, -0.8791],\n",
      "         [-0.4384, -0.5095,  0.5613, -0.1614, -0.7172]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0823 cost = 0.001886\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5148,  0.4834,  0.0998, -0.5147,  0.6880],\n",
      "         [-0.4468,  0.6096,  0.0219, -0.3951,  0.6467]],\n",
      "\n",
      "        [[ 0.8655,  0.8596, -0.8892,  0.8453,  0.8823],\n",
      "         [ 0.8977,  0.9250, -0.9250,  0.9225,  0.8966]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0823 cost = 0.001818\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5147,  0.4835,  0.0998, -0.5146,  0.6880],\n",
      "         [ 0.9575, -0.6699, -0.7321,  0.9404, -0.6963]],\n",
      "\n",
      "        [[ 0.8655,  0.8596, -0.8892,  0.8453,  0.8823],\n",
      "         [-0.4385, -0.5096,  0.5614, -0.1615, -0.7173]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0824 cost = 0.001964\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4467,  0.6098,  0.0220, -0.3949,  0.6467],\n",
      "         [ 0.8872, -0.9504, -0.9589,  0.7960, -0.0164]],\n",
      "\n",
      "        [[ 0.8977,  0.9250, -0.9250,  0.9226,  0.8966],\n",
      "         [-0.7076, -0.4889,  0.7175, -0.1532, -0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0824 cost = 0.001009\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3762,  0.6176,  0.1338, -0.3361,  0.6034],\n",
      "         [ 0.9078, -0.8555, -0.9284,  0.8770, -0.1543]],\n",
      "\n",
      "        [[ 0.9182,  0.9116, -0.9385,  0.9134,  0.8911],\n",
      "         [-0.7517, -0.4962,  0.7655, -0.1705, -0.9106]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0824 cost = 0.000753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9079, -0.8555, -0.9284,  0.8770, -0.1544],\n",
      "         [ 0.9575, -0.6700, -0.7322,  0.9405, -0.6964]],\n",
      "\n",
      "        [[-0.7518, -0.4962,  0.7655, -0.1706, -0.9106],\n",
      "         [-0.4386, -0.5099,  0.5614, -0.1617, -0.7174]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0825 cost = 0.001995\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.6177,  0.1338, -0.3360,  0.6035],\n",
      "         [-0.5147,  0.4836,  0.0998, -0.5143,  0.6880]],\n",
      "\n",
      "        [[ 0.9182,  0.9116, -0.9385,  0.9134,  0.8911],\n",
      "         [ 0.8655,  0.8596, -0.8892,  0.8453,  0.8824]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0825 cost = 0.003475\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4467,  0.6101,  0.0221, -0.3946,  0.6468],\n",
      "         [ 0.8873, -0.9505, -0.9589,  0.7963, -0.0166]],\n",
      "\n",
      "        [[ 0.8977,  0.9250, -0.9250,  0.9226,  0.8966],\n",
      "         [-0.7077, -0.4892,  0.7176, -0.1534, -0.8792]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0825 cost = 0.001005\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.6178,  0.1338, -0.3358,  0.6035],\n",
      "         [ 0.9079, -0.8555, -0.9284,  0.8771, -0.1546]],\n",
      "\n",
      "        [[ 0.9182,  0.9116, -0.9385,  0.9134,  0.8912],\n",
      "         [-0.7518, -0.4965,  0.7656, -0.1708, -0.9107]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0826 cost = 0.000750\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8874, -0.9505, -0.9589,  0.7964, -0.0167],\n",
      "         [ 0.9576, -0.6702, -0.7324,  0.9405, -0.6965]],\n",
      "\n",
      "        [[-0.7077, -0.4893,  0.7176, -0.1535, -0.8792],\n",
      "         [-0.4388, -0.5102,  0.5616, -0.1618, -0.7175]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0826 cost = 0.001868\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5147,  0.4839,  0.0998, -0.5141,  0.6882],\n",
      "         [-0.4468,  0.6104,  0.0222, -0.3944,  0.6469]],\n",
      "\n",
      "        [[ 0.8656,  0.8597, -0.8893,  0.8454,  0.8825],\n",
      "         [ 0.8978,  0.9250, -0.9250,  0.9226,  0.8967]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0826 cost = 0.001802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5147,  0.4840,  0.0998, -0.5141,  0.6882],\n",
      "         [ 0.9576, -0.6703, -0.7325,  0.9405, -0.6965]],\n",
      "\n",
      "        [[ 0.8656,  0.8597, -0.8893,  0.8454,  0.8825],\n",
      "         [-0.4389, -0.5103,  0.5617, -0.1619, -0.7176]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0827 cost = 0.001946\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9080, -0.8556, -0.9285,  0.8772, -0.1549],\n",
      "         [ 0.8875, -0.9505, -0.9589,  0.7966, -0.0169]],\n",
      "\n",
      "        [[-0.7520, -0.4968,  0.7657, -0.1711, -0.9107],\n",
      "         [-0.7078, -0.4895,  0.7177, -0.1536, -0.8793]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0827 cost = 0.001558\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4467,  0.6107,  0.0222, -0.3942,  0.6470],\n",
      "         [-0.3760,  0.6181,  0.1338, -0.3355,  0.6036]],\n",
      "\n",
      "        [[ 0.8978,  0.9251, -0.9250,  0.9226,  0.8967],\n",
      "         [ 0.9183,  0.9116, -0.9385,  0.9134,  0.8912]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0827 cost = 0.000852\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4467,  0.6108,  0.0222, -0.3941,  0.6470],\n",
      "         [ 0.8876, -0.9505, -0.9590,  0.7968, -0.0170]],\n",
      "\n",
      "        [[ 0.8978,  0.9251, -0.9250,  0.9227,  0.8967],\n",
      "         [-0.7078, -0.4897,  0.7178, -0.1538, -0.8793]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0828 cost = 0.000997\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5147,  0.4842,  0.0998, -0.5138,  0.6884],\n",
      "         [ 0.9576, -0.6703, -0.7327,  0.9406, -0.6966]],\n",
      "\n",
      "        [[ 0.8657,  0.8598, -0.8893,  0.8455,  0.8826],\n",
      "         [-0.4391, -0.5107,  0.5618, -0.1620, -0.7177]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0828 cost = 0.001938\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9081, -0.8556, -0.9285,  0.8773, -0.1552],\n",
      "         [-0.3760,  0.6182,  0.1338, -0.3353,  0.6036]],\n",
      "\n",
      "        [[-0.7521, -0.4971,  0.7659, -0.1713, -0.9108],\n",
      "         [ 0.9183,  0.9116, -0.9385,  0.9134,  0.8913]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0828 cost = 0.000566\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9576, -0.6703, -0.7328,  0.9406, -0.6967],\n",
      "         [-0.5148,  0.4843,  0.0998, -0.5137,  0.6884]],\n",
      "\n",
      "        [[-0.4392, -0.5109,  0.5619, -0.1621, -0.7177],\n",
      "         [ 0.8657,  0.8598, -0.8893,  0.8455,  0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0829 cost = 0.002473\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4468,  0.6111,  0.0224, -0.3938,  0.6471],\n",
      "         [ 0.8877, -0.9506, -0.9590,  0.7971, -0.0172]],\n",
      "\n",
      "        [[ 0.8978,  0.9251, -0.9250,  0.9227,  0.8968],\n",
      "         [-0.7079, -0.4899,  0.7179, -0.1540, -0.8793]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0829 cost = 0.000992\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.6183,  0.1339, -0.3352,  0.6036],\n",
      "         [ 0.9082, -0.8556, -0.9286,  0.8774, -0.1554]],\n",
      "\n",
      "        [[ 0.9183,  0.9116, -0.9385,  0.9135,  0.8913],\n",
      "         [-0.7522, -0.4974,  0.7660, -0.1715, -0.9108]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0829 cost = 0.000742\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3761,  0.6183,  0.1339, -0.3352,  0.6036],\n",
      "         [-0.4469,  0.6112,  0.0224, -0.3937,  0.6471]],\n",
      "\n",
      "        [[ 0.9183,  0.9116, -0.9385,  0.9135,  0.8913],\n",
      "         [ 0.8978,  0.9251, -0.9250,  0.9227,  0.8968]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0830 cost = 0.001281\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9576, -0.6705, -0.7329,  0.9406, -0.6968],\n",
      "         [ 0.9082, -0.8556, -0.9286,  0.8774, -0.1556]],\n",
      "\n",
      "        [[-0.4393, -0.5111,  0.5620, -0.1623, -0.7178],\n",
      "         [-0.7522, -0.4975,  0.7660, -0.1717, -0.9108]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0830 cost = 0.001941\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5150,  0.4844,  0.0998, -0.5136,  0.6885],\n",
      "         [ 0.8879, -0.9506, -0.9591,  0.7974, -0.0175]],\n",
      "\n",
      "        [[ 0.8658,  0.8598, -0.8894,  0.8456,  0.8827],\n",
      "         [-0.7080, -0.4902,  0.7180, -0.1542, -0.8794]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0830 cost = 0.001139\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9083, -0.8557, -0.9286,  0.8775, -0.1558],\n",
      "         [-0.3762,  0.6184,  0.1339, -0.3351,  0.6036]],\n",
      "\n",
      "        [[-0.7523, -0.4976,  0.7661, -0.1718, -0.9108],\n",
      "         [ 0.9183,  0.9116, -0.9385,  0.9135,  0.8914]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0831 cost = 0.000562\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8879, -0.9506, -0.9591,  0.7975, -0.0176],\n",
      "         [-0.5151,  0.4844,  0.0998, -0.5136,  0.6885]],\n",
      "\n",
      "        [[-0.7081, -0.4903,  0.7181, -0.1543, -0.8794],\n",
      "         [ 0.8658,  0.8598, -0.8894,  0.8456,  0.8827]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0831 cost = 0.001136\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4471,  0.6116,  0.0226, -0.3936,  0.6472],\n",
      "         [ 0.9577, -0.6708, -0.7331,  0.9407, -0.6969]],\n",
      "\n",
      "        [[ 0.8979,  0.9251, -0.9250,  0.9227,  0.8969],\n",
      "         [-0.4395, -0.5113,  0.5621, -0.1624, -0.7179]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0831 cost = 0.001753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9084, -0.8557, -0.9287,  0.8776, -0.1560],\n",
      "         [-0.4472,  0.6116,  0.0226, -0.3936,  0.6472]],\n",
      "\n",
      "        [[-0.7524, -0.4978,  0.7662, -0.1720, -0.9109],\n",
      "         [ 0.8979,  0.9251, -0.9250,  0.9228,  0.8969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0832 cost = 0.000883\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9577, -0.6709, -0.7332,  0.9407, -0.6970],\n",
      "         [-0.3764,  0.6184,  0.1339, -0.3350,  0.6036]],\n",
      "\n",
      "        [[-0.4395, -0.5114,  0.5622, -0.1625, -0.7179],\n",
      "         [ 0.9184,  0.9116, -0.9385,  0.9135,  0.8914]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0832 cost = 0.001571\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5153,  0.4844,  0.0999, -0.5136,  0.6886],\n",
      "         [ 0.8881, -0.9507, -0.9591,  0.7978, -0.0178]],\n",
      "\n",
      "        [[ 0.8659,  0.8599, -0.8894,  0.8456,  0.8827],\n",
      "         [-0.7082, -0.4904,  0.7182, -0.1546, -0.8794]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0832 cost = 0.001132\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5154,  0.4844,  0.0999, -0.5137,  0.6886],\n",
      "         [-0.3765,  0.6184,  0.1339, -0.3350,  0.6036]],\n",
      "\n",
      "        [[ 0.8659,  0.8599, -0.8894,  0.8456,  0.8827],\n",
      "         [ 0.9184,  0.9116, -0.9385,  0.9135,  0.8915]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0833 cost = 0.001155\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9085, -0.8558, -0.9287,  0.8777, -0.1564],\n",
      "         [ 0.9577, -0.6711, -0.7334,  0.9407, -0.6971]],\n",
      "\n",
      "        [[-0.7525, -0.4979,  0.7663, -0.1723, -0.9109],\n",
      "         [-0.4396, -0.5116,  0.5623, -0.1627, -0.7180]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0833 cost = 0.001948\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8882, -0.9507, -0.9592,  0.7980, -0.0180],\n",
      "         [-0.4473,  0.6118,  0.0228, -0.3933,  0.6472]],\n",
      "\n",
      "        [[-0.7083, -0.4905,  0.7183, -0.1547, -0.8795],\n",
      "         [ 0.8979,  0.9251, -0.9250,  0.9228,  0.8969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0833 cost = 0.000875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4473,  0.6119,  0.0229, -0.3932,  0.6472],\n",
      "         [-0.5153,  0.4846,  0.0999, -0.5135,  0.6887]],\n",
      "\n",
      "        [[ 0.8979,  0.9251, -0.9250,  0.9228,  0.8969],\n",
      "         [ 0.8659,  0.8599, -0.8894,  0.8456,  0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0834 cost = 0.003483\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8882, -0.9507, -0.9592,  0.7981, -0.0181],\n",
      "         [ 0.9085, -0.8558, -0.9288,  0.8777, -0.1566]],\n",
      "\n",
      "        [[-0.7083, -0.4906,  0.7183, -0.1548, -0.8795],\n",
      "         [-0.7526, -0.4981,  0.7664, -0.1725, -0.9109]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0834 cost = 0.000865\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3763,  0.6186,  0.1340, -0.3347,  0.6037],\n",
      "         [ 0.9577, -0.6713, -0.7335,  0.9407, -0.6972]],\n",
      "\n",
      "        [[ 0.9184,  0.9116, -0.9385,  0.9135,  0.8915],\n",
      "         [-0.4398, -0.5118,  0.5624, -0.1629, -0.7181]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0834 cost = 0.001527\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9085, -0.8559, -0.9288,  0.8778, -0.1567],\n",
      "         [-0.3763,  0.6187,  0.1340, -0.3346,  0.6037]],\n",
      "\n",
      "        [[-0.7526, -0.4982,  0.7665, -0.1727, -0.9109],\n",
      "         [ 0.9184,  0.9117, -0.9385,  0.9135,  0.8916]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0835 cost = 0.000556\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5154,  0.4848,  0.0999, -0.5133,  0.6889],\n",
      "         [ 0.9577, -0.6714, -0.7336,  0.9408, -0.6972]],\n",
      "\n",
      "        [[ 0.8660,  0.8600, -0.8894,  0.8457,  0.8829],\n",
      "         [-0.4399, -0.5119,  0.5625, -0.1629, -0.7181]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0835 cost = 0.001898\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4475,  0.6122,  0.0231, -0.3929,  0.6473],\n",
      "         [ 0.8883, -0.9508, -0.9592,  0.7983, -0.0182]],\n",
      "\n",
      "        [[ 0.8979,  0.9252, -0.9250,  0.9228,  0.8970],\n",
      "         [-0.7084, -0.4907,  0.7184, -0.1551, -0.8795]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0835 cost = 0.000972\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4475,  0.6123,  0.0231, -0.3929,  0.6473],\n",
      "         [-0.5155,  0.4848,  0.0999, -0.5133,  0.6889]],\n",
      "\n",
      "        [[ 0.8979,  0.9252, -0.9250,  0.9228,  0.8970],\n",
      "         [ 0.8660,  0.8600, -0.8894,  0.8457,  0.8829]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0836 cost = 0.003468\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3764,  0.6188,  0.1340, -0.3345,  0.6037],\n",
      "         [ 0.9086, -0.8559, -0.9288,  0.8778, -0.1570]],\n",
      "\n",
      "        [[ 0.9184,  0.9117, -0.9386,  0.9136,  0.8916],\n",
      "         [-0.7527, -0.4984,  0.7666, -0.1729, -0.9110]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0836 cost = 0.000728\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9577, -0.6714, -0.7337,  0.9408, -0.6973],\n",
      "         [ 0.8884, -0.9508, -0.9592,  0.7984, -0.0183]],\n",
      "\n",
      "        [[-0.4400, -0.5122,  0.5626, -0.1631, -0.7182],\n",
      "         [-0.7085, -0.4908,  0.7185, -0.1552, -0.8795]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0836 cost = 0.002317\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5157,  0.4850,  0.0999, -0.5132,  0.6891],\n",
      "         [ 0.9086, -0.8559, -0.9289,  0.8779, -0.1571]],\n",
      "\n",
      "        [[ 0.8661,  0.8601, -0.8895,  0.8458,  0.8830],\n",
      "         [-0.7528, -0.4985,  0.7666, -0.1731, -0.9110]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0837 cost = 0.001033\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3765,  0.6189,  0.1340, -0.3345,  0.6037],\n",
      "         [ 0.8885, -0.9508, -0.9592,  0.7986, -0.0184]],\n",
      "\n",
      "        [[ 0.9185,  0.9117, -0.9386,  0.9136,  0.8917],\n",
      "         [-0.7085, -0.4909,  0.7186, -0.1553, -0.8795]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0837 cost = 0.000770\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9578, -0.6715, -0.7338,  0.9408, -0.6974],\n",
      "         [-0.4478,  0.6126,  0.0233, -0.3927,  0.6474]],\n",
      "\n",
      "        [[-0.4402, -0.5124,  0.5627, -0.1632, -0.7183],\n",
      "         [ 0.8980,  0.9252, -0.9250,  0.9229,  0.8971]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0837 cost = 0.001940\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3766,  0.6189,  0.1341, -0.3345,  0.6037],\n",
      "         [-0.5159,  0.4850,  0.0999, -0.5132,  0.6891]],\n",
      "\n",
      "        [[ 0.9185,  0.9117, -0.9386,  0.9136,  0.8917],\n",
      "         [ 0.8662,  0.8602, -0.8895,  0.8458,  0.8831]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0838 cost = 0.003397\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4479,  0.6128,  0.0233, -0.3926,  0.6475],\n",
      "         [ 0.9087, -0.8559, -0.9289,  0.8779, -0.1574]],\n",
      "\n",
      "        [[ 0.8980,  0.9252, -0.9250,  0.9229,  0.8972],\n",
      "         [-0.7529, -0.4987,  0.7668, -0.1733, -0.9110]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0838 cost = 0.000926\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8886, -0.9508, -0.9593,  0.7988, -0.0186],\n",
      "         [ 0.9578, -0.6716, -0.7339,  0.9408, -0.6974]],\n",
      "\n",
      "        [[-0.7086, -0.4910,  0.7187, -0.1555, -0.8795],\n",
      "         [-0.4403, -0.5125,  0.5628, -0.1633, -0.7183]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0838 cost = 0.001808\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4480,  0.6129,  0.0234, -0.3925,  0.6475],\n",
      "         [-0.5160,  0.4852,  0.0998, -0.5132,  0.6893]],\n",
      "\n",
      "        [[ 0.8980,  0.9253, -0.9250,  0.9229,  0.8972],\n",
      "         [ 0.8663,  0.8603, -0.8896,  0.8459,  0.8832]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0839 cost = 0.003439\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9088, -0.8559, -0.9290,  0.8780, -0.1576],\n",
      "         [-0.3769,  0.6190,  0.1341, -0.3344,  0.6038]],\n",
      "\n",
      "        [[-0.7530, -0.4989,  0.7668, -0.1735, -0.9110],\n",
      "         [ 0.9186,  0.9117, -0.9386,  0.9137,  0.8919]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0839 cost = 0.000549\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8886, -0.9508, -0.9593,  0.7989, -0.0187],\n",
      "         [ 0.9578, -0.6716, -0.7341,  0.9408, -0.6975]],\n",
      "\n",
      "        [[-0.7086, -0.4911,  0.7187, -0.1557, -0.8795],\n",
      "         [-0.4404, -0.5127,  0.5629, -0.1633, -0.7183]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0839 cost = 0.001805\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4483,  0.6132,  0.0235, -0.3924,  0.6476],\n",
      "         [-0.5162,  0.4854,  0.0998, -0.5132,  0.6895]],\n",
      "\n",
      "        [[ 0.8981,  0.9253, -0.9250,  0.9230,  0.8973],\n",
      "         [ 0.8664,  0.8604, -0.8897,  0.8460,  0.8833]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0840 cost = 0.003425\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3770,  0.6191,  0.1341, -0.3344,  0.6038],\n",
      "         [ 0.9088, -0.8559, -0.9290,  0.8780, -0.1578]],\n",
      "\n",
      "        [[ 0.9186,  0.9118, -0.9386,  0.9137,  0.8920],\n",
      "         [-0.7531, -0.4991,  0.7669, -0.1737, -0.9111]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0840 cost = 0.000720\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8887, -0.9508, -0.9593,  0.7989, -0.0188],\n",
      "         [ 0.9578, -0.6716, -0.7341,  0.9408, -0.6975]],\n",
      "\n",
      "        [[-0.7086, -0.4912,  0.7188, -0.1558, -0.8795],\n",
      "         [-0.4405, -0.5129,  0.5630, -0.1633, -0.7184]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0840 cost = 0.001802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4485,  0.6135,  0.0235, -0.3923,  0.6477],\n",
      "         [ 0.8887, -0.9508, -0.9593,  0.7989, -0.0189]],\n",
      "\n",
      "        [[ 0.8981,  0.9253, -0.9251,  0.9230,  0.8974],\n",
      "         [-0.7087, -0.4913,  0.7188, -0.1559, -0.8796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0841 cost = 0.000954\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9088, -0.8559, -0.9290,  0.8780, -0.1579],\n",
      "         [-0.3773,  0.6192,  0.1340, -0.3344,  0.6039]],\n",
      "\n",
      "        [[-0.7532, -0.4992,  0.7670, -0.1739, -0.9111],\n",
      "         [ 0.9187,  0.9118, -0.9386,  0.9138,  0.8921]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0841 cost = 0.000546\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5166,  0.4857,  0.0997, -0.5132,  0.6899],\n",
      "         [ 0.9578, -0.6716, -0.7342,  0.9408, -0.6975]],\n",
      "\n",
      "        [[ 0.8666,  0.8606, -0.8898,  0.8462,  0.8836],\n",
      "         [-0.4407, -0.5131,  0.5632, -0.1634, -0.7185]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0841 cost = 0.001867\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8887, -0.9509, -0.9593,  0.7991, -0.0190],\n",
      "         [ 0.9089, -0.8559, -0.9291,  0.8780, -0.1581]],\n",
      "\n",
      "        [[-0.7087, -0.4914,  0.7189, -0.1560, -0.8796],\n",
      "         [-0.7532, -0.4994,  0.7671, -0.1740, -0.9111]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0842 cost = 0.000847\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4487,  0.6137,  0.0236, -0.3922,  0.6478],\n",
      "         [-0.3775,  0.6193,  0.1340, -0.3344,  0.6039]],\n",
      "\n",
      "        [[ 0.8982,  0.9254, -0.9251,  0.9230,  0.8975],\n",
      "         [ 0.9187,  0.9118, -0.9386,  0.9138,  0.8922]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0842 cost = 0.000810\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9578, -0.6716, -0.7344,  0.9408, -0.6976],\n",
      "         [-0.5169,  0.4858,  0.0997, -0.5132,  0.6900]],\n",
      "\n",
      "        [[-0.4408, -0.5133,  0.5633, -0.1635, -0.7185],\n",
      "         [ 0.8667,  0.8607, -0.8899,  0.8463,  0.8837]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0842 cost = 0.002395\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9578, -0.6717, -0.7344,  0.9408, -0.6976],\n",
      "         [-0.4489,  0.6139,  0.0237, -0.3921,  0.6479]],\n",
      "\n",
      "        [[-0.4409, -0.5134,  0.5633, -0.1635, -0.7185],\n",
      "         [ 0.8982,  0.9254, -0.9251,  0.9231,  0.8976]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0843 cost = 0.001917\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5170,  0.4858,  0.0997, -0.5133,  0.6901],\n",
      "         [-0.3776,  0.6193,  0.1341, -0.3344,  0.6039]],\n",
      "\n",
      "        [[ 0.8667,  0.8607, -0.8899,  0.8463,  0.8837],\n",
      "         [ 0.9187,  0.9119, -0.9386,  0.9138,  0.8923]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0843 cost = 0.001112\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8888, -0.9509, -0.9594,  0.7992, -0.0192],\n",
      "         [ 0.9089, -0.8560, -0.9291,  0.8781, -0.1584]],\n",
      "\n",
      "        [[-0.7088, -0.4916,  0.7190, -0.1563, -0.8796],\n",
      "         [-0.7534, -0.4996,  0.7673, -0.1744, -0.9111]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0843 cost = 0.000843\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9089, -0.8560, -0.9291,  0.8781, -0.1585],\n",
      "         [-0.4490,  0.6141,  0.0238, -0.3919,  0.6479]],\n",
      "\n",
      "        [[-0.7534, -0.4997,  0.7673, -0.1745, -0.9112],\n",
      "         [ 0.8982,  0.9254, -0.9251,  0.9231,  0.8976]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0844 cost = 0.000850\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.6195,  0.1341, -0.3343,  0.6040],\n",
      "         [-0.5170,  0.4860,  0.0997, -0.5131,  0.6903]],\n",
      "\n",
      "        [[ 0.9187,  0.9119, -0.9387,  0.9138,  0.8923],\n",
      "         [ 0.8668,  0.8608, -0.8899,  0.8464,  0.8838]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0844 cost = 0.003332\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8889, -0.9509, -0.9594,  0.7993, -0.0193],\n",
      "         [ 0.9578, -0.6719, -0.7346,  0.9408, -0.6977]],\n",
      "\n",
      "        [[-0.7088, -0.4917,  0.7191, -0.1564, -0.8796],\n",
      "         [-0.4411, -0.5136,  0.5635, -0.1637, -0.7186]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0844 cost = 0.001785\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5169,  0.4862,  0.0996, -0.5130,  0.6904],\n",
      "         [ 0.9578, -0.6720, -0.7346,  0.9408, -0.6977]],\n",
      "\n",
      "        [[ 0.8669,  0.8608, -0.8900,  0.8464,  0.8839],\n",
      "         [-0.4411, -0.5137,  0.5635, -0.1637, -0.7186]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0845 cost = 0.001850\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.6197,  0.1341, -0.3341,  0.6041],\n",
      "         [ 0.8889, -0.9509, -0.9594,  0.7994, -0.0194]],\n",
      "\n",
      "        [[ 0.9188,  0.9119, -0.9387,  0.9139,  0.8924],\n",
      "         [-0.7089, -0.4918,  0.7192, -0.1565, -0.8796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0845 cost = 0.000752\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4491,  0.6145,  0.0240, -0.3916,  0.6481],\n",
      "         [ 0.9090, -0.8560, -0.9292,  0.8782, -0.1588]],\n",
      "\n",
      "        [[ 0.8983,  0.9255, -0.9251,  0.9231,  0.8977],\n",
      "         [-0.7535, -0.4999,  0.7674, -0.1748, -0.9112]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0845 cost = 0.000904\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9090, -0.8560, -0.9292,  0.8782, -0.1589],\n",
      "         [-0.5170,  0.4864,  0.0996, -0.5128,  0.6906]],\n",
      "\n",
      "        [[-0.7536, -0.5000,  0.7675, -0.1749, -0.9112],\n",
      "         [ 0.8670,  0.8609, -0.8900,  0.8465,  0.8839]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0846 cost = 0.001051\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.6199,  0.1341, -0.3340,  0.6042],\n",
      "         [ 0.8890, -0.9509, -0.9594,  0.7995, -0.0195]],\n",
      "\n",
      "        [[ 0.9188,  0.9119, -0.9387,  0.9139,  0.8925],\n",
      "         [-0.7089, -0.4919,  0.7192, -0.1567, -0.8796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0846 cost = 0.000750\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4492,  0.6147,  0.0241, -0.3914,  0.6482],\n",
      "         [ 0.9578, -0.6721, -0.7348,  0.9408, -0.6978]],\n",
      "\n",
      "        [[ 0.8983,  0.9255, -0.9251,  0.9231,  0.8978],\n",
      "         [-0.4414, -0.5140,  0.5637, -0.1639, -0.7187]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0846 cost = 0.001681\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9578, -0.6721, -0.7348,  0.9408, -0.6979],\n",
      "         [ 0.8890, -0.9509, -0.9594,  0.7996, -0.0196]],\n",
      "\n",
      "        [[-0.4414, -0.5140,  0.5638, -0.1639, -0.7188],\n",
      "         [-0.7090, -0.4919,  0.7193, -0.1568, -0.8796]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0847 cost = 0.002266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4493,  0.6148,  0.0242, -0.3914,  0.6482],\n",
      "         [ 0.9091, -0.8561, -0.9292,  0.8783, -0.1592]],\n",
      "\n",
      "        [[ 0.8983,  0.9255, -0.9251,  0.9232,  0.8978],\n",
      "         [-0.7537, -0.5002,  0.7676, -0.1752, -0.9112]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0847 cost = 0.000900\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.6199,  0.1341, -0.3339,  0.6042],\n",
      "         [-0.5171,  0.4865,  0.0996, -0.5128,  0.6908]],\n",
      "\n",
      "        [[ 0.9188,  0.9119, -0.9387,  0.9139,  0.8925],\n",
      "         [ 0.8670,  0.8610, -0.8901,  0.8466,  0.8840]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0847 cost = 0.003306\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.6200,  0.1341, -0.3339,  0.6043],\n",
      "         [-0.5172,  0.4866,  0.0996, -0.5127,  0.6908]],\n",
      "\n",
      "        [[ 0.9188,  0.9120, -0.9387,  0.9139,  0.8926],\n",
      "         [ 0.8671,  0.8610, -0.8901,  0.8466,  0.8841]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0848 cost = 0.003303\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9579, -0.6723, -0.7350,  0.9408, -0.6979],\n",
      "         [ 0.9091, -0.8561, -0.9293,  0.8784, -0.1594]],\n",
      "\n",
      "        [[-0.4416, -0.5143,  0.5639, -0.1641, -0.7188],\n",
      "         [-0.7538, -0.5004,  0.7677, -0.1755, -0.9113]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0848 cost = 0.001864\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8892, -0.9510, -0.9595,  0.7999, -0.0199],\n",
      "         [-0.4494,  0.6151,  0.0243, -0.3912,  0.6483]],\n",
      "\n",
      "        [[-0.7090, -0.4921,  0.7194, -0.1571, -0.8797],\n",
      "         [ 0.8984,  0.9255, -0.9251,  0.9232,  0.8979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0848 cost = 0.000835\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9579, -0.6724, -0.7351,  0.9408, -0.6980],\n",
      "         [ 0.8892, -0.9510, -0.9595,  0.7999, -0.0200]],\n",
      "\n",
      "        [[-0.4416, -0.5144,  0.5640, -0.1641, -0.7189],\n",
      "         [-0.7091, -0.4922,  0.7194, -0.1572, -0.8797]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0849 cost = 0.002255\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5172,  0.4870,  0.0996, -0.5126,  0.6912],\n",
      "         [-0.4495,  0.6153,  0.0243, -0.3912,  0.6484]],\n",
      "\n",
      "        [[ 0.8672,  0.8612, -0.8902,  0.8467,  0.8842],\n",
      "         [ 0.8984,  0.9255, -0.9251,  0.9232,  0.8979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0849 cost = 0.001673\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3776,  0.6202,  0.1341, -0.3336,  0.6044],\n",
      "         [ 0.9092, -0.8562, -0.9294,  0.8784, -0.1597]],\n",
      "\n",
      "        [[ 0.9189,  0.9120, -0.9387,  0.9140,  0.8927],\n",
      "         [-0.7539, -0.5006,  0.7678, -0.1757, -0.9113]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0849 cost = 0.000701\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8893, -0.9510, -0.9595,  0.8001, -0.0201],\n",
      "         [-0.5172,  0.4872,  0.0995, -0.5124,  0.6913]],\n",
      "\n",
      "        [[-0.7091, -0.4923,  0.7195, -0.1573, -0.8797],\n",
      "         [ 0.8673,  0.8612, -0.8902,  0.8468,  0.8843]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0850 cost = 0.001074\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9092, -0.8562, -0.9294,  0.8785, -0.1598],\n",
      "         [ 0.9579, -0.6726, -0.7353,  0.9408, -0.6981]],\n",
      "\n",
      "        [[-0.7540, -0.5007,  0.7679, -0.1759, -0.9113],\n",
      "         [-0.4418, -0.5147,  0.5641, -0.1643, -0.7189]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0850 cost = 0.001873\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.6204,  0.1341, -0.3334,  0.6045],\n",
      "         [-0.4494,  0.6157,  0.0244, -0.3909,  0.6486]],\n",
      "\n",
      "        [[ 0.9190,  0.9120, -0.9387,  0.9140,  0.8928],\n",
      "         [ 0.8985,  0.9256, -0.9251,  0.9233,  0.8980]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0850 cost = 0.001204\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9579, -0.6728, -0.7354,  0.9408, -0.6981],\n",
      "         [-0.4494,  0.6158,  0.0244, -0.3909,  0.6486]],\n",
      "\n",
      "        [[-0.4419, -0.5148,  0.5642, -0.1643, -0.7190],\n",
      "         [ 0.8985,  0.9256, -0.9251,  0.9233,  0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0851 cost = 0.001879\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9093, -0.8562, -0.9294,  0.8785, -0.1601],\n",
      "         [ 0.8894, -0.9510, -0.9595,  0.8003, -0.0204]],\n",
      "\n",
      "        [[-0.7540, -0.5008,  0.7680, -0.1761, -0.9113],\n",
      "         [-0.7092, -0.4924,  0.7196, -0.1576, -0.8797]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0851 cost = 0.001457\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.6205,  0.1340, -0.3333,  0.6046],\n",
      "         [-0.5172,  0.4875,  0.0995, -0.5123,  0.6916]],\n",
      "\n",
      "        [[ 0.9190,  0.9121, -0.9387,  0.9140,  0.8929],\n",
      "         [ 0.8674,  0.8614, -0.8903,  0.8469,  0.8844]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0851 cost = 0.003266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8895, -0.9511, -0.9596,  0.8004, -0.0205],\n",
      "         [-0.3775,  0.6205,  0.1340, -0.3332,  0.6047]],\n",
      "\n",
      "        [[-0.7092, -0.4925,  0.7197, -0.1577, -0.8797],\n",
      "         [ 0.9190,  0.9121, -0.9387,  0.9141,  0.8929]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0852 cost = 0.000516\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9579, -0.6730, -0.7356,  0.9409, -0.6982],\n",
      "         [-0.5172,  0.4877,  0.0995, -0.5122,  0.6918]],\n",
      "\n",
      "        [[-0.4420, -0.5150,  0.5643, -0.1645, -0.7190],\n",
      "         [ 0.8675,  0.8614, -0.8903,  0.8469,  0.8845]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0852 cost = 0.002337\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4494,  0.6163,  0.0244, -0.3907,  0.6488],\n",
      "         [ 0.9094, -0.8563, -0.9295,  0.8786, -0.1605]],\n",
      "\n",
      "        [[ 0.8985,  0.9256, -0.9252,  0.9233,  0.8982],\n",
      "         [-0.7542, -0.5010,  0.7681, -0.1764, -0.9114]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0852 cost = 0.000884\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4494,  0.6164,  0.0245, -0.3906,  0.6489],\n",
      "         [-0.3775,  0.6206,  0.1340, -0.3330,  0.6048]],\n",
      "\n",
      "        [[ 0.8985,  0.9256, -0.9252,  0.9233,  0.8982],\n",
      "         [ 0.9190,  0.9121, -0.9387,  0.9141,  0.8930]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0853 cost = 0.000780\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8896, -0.9511, -0.9596,  0.8006, -0.0207],\n",
      "         [ 0.9094, -0.8563, -0.9295,  0.8786, -0.1607]],\n",
      "\n",
      "        [[-0.7093, -0.4925,  0.7198, -0.1579, -0.8797],\n",
      "         [-0.7542, -0.5011,  0.7682, -0.1765, -0.9114]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0853 cost = 0.000820\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9579, -0.6733, -0.7358,  0.9409, -0.6983],\n",
      "         [-0.5172,  0.4880,  0.0995, -0.5121,  0.6921]],\n",
      "\n",
      "        [[-0.4422, -0.5152,  0.5645, -0.1646, -0.7191],\n",
      "         [ 0.8676,  0.8616, -0.8904,  0.8470,  0.8847]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0853 cost = 0.002329\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9095, -0.8564, -0.9296,  0.8787, -0.1608],\n",
      "         [-0.3776,  0.6208,  0.1340, -0.3329,  0.6049]],\n",
      "\n",
      "        [[-0.7543, -0.5011,  0.7682, -0.1767, -0.9114],\n",
      "         [ 0.9191,  0.9121, -0.9388,  0.9141,  0.8931]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0854 cost = 0.000525\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9579, -0.6734, -0.7359,  0.9409, -0.6984],\n",
      "         [-0.5173,  0.4881,  0.0995, -0.5120,  0.6922]],\n",
      "\n",
      "        [[-0.4423, -0.5153,  0.5645, -0.1646, -0.7191],\n",
      "         [ 0.8676,  0.8616, -0.8904,  0.8471,  0.8847]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0854 cost = 0.002325\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8896, -0.9511, -0.9596,  0.8008, -0.0209],\n",
      "         [-0.4496,  0.6167,  0.0246, -0.3904,  0.6490]],\n",
      "\n",
      "        [[-0.7094, -0.4926,  0.7199, -0.1582, -0.8797],\n",
      "         [ 0.8986,  0.9257, -0.9252,  0.9234,  0.8983]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0854 cost = 0.000819\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5173,  0.4881,  0.0995, -0.5120,  0.6924],\n",
      "         [ 0.9095, -0.8565, -0.9296,  0.8787, -0.1611]],\n",
      "\n",
      "        [[ 0.8677,  0.8617, -0.8904,  0.8471,  0.8848],\n",
      "         [-0.7544, -0.5012,  0.7683, -0.1769, -0.9114]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0855 cost = 0.000976\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4497,  0.6168,  0.0247, -0.3903,  0.6491],\n",
      "         [ 0.9580, -0.6737, -0.7360,  0.9409, -0.6985]],\n",
      "\n",
      "        [[ 0.8986,  0.9257, -0.9252,  0.9234,  0.8983],\n",
      "         [-0.4424, -0.5153,  0.5646, -0.1648, -0.7191]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0855 cost = 0.001642\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3777,  0.6208,  0.1340, -0.3328,  0.6050],\n",
      "         [ 0.8897, -0.9511, -0.9596,  0.8009, -0.0211]],\n",
      "\n",
      "        [[ 0.9191,  0.9122, -0.9388,  0.9142,  0.8932],\n",
      "         [-0.7095, -0.4926,  0.7200, -0.1583, -0.8798]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0855 cost = 0.000729\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9580, -0.6739, -0.7361,  0.9409, -0.6986],\n",
      "         [-0.5175,  0.4882,  0.0995, -0.5120,  0.6925]],\n",
      "\n",
      "        [[-0.4425, -0.5154,  0.5647, -0.1649, -0.7192],\n",
      "         [ 0.8677,  0.8617, -0.8905,  0.8471,  0.8848]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0856 cost = 0.002315\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8898, -0.9512, -0.9597,  0.8010, -0.0212],\n",
      "         [ 0.9096, -0.8565, -0.9297,  0.8788, -0.1615]],\n",
      "\n",
      "        [[-0.7095, -0.4927,  0.7201, -0.1585, -0.8798],\n",
      "         [-0.7545, -0.5013,  0.7685, -0.1772, -0.9115]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0856 cost = 0.000814\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3778,  0.6208,  0.1341, -0.3328,  0.6051],\n",
      "         [-0.4498,  0.6169,  0.0249, -0.3902,  0.6491]],\n",
      "\n",
      "        [[ 0.9191,  0.9122, -0.9388,  0.9142,  0.8933],\n",
      "         [ 0.8986,  0.9257, -0.9252,  0.9234,  0.8984]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0856 cost = 0.001182\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8898, -0.9512, -0.9597,  0.8011, -0.0213],\n",
      "         [-0.4499,  0.6169,  0.0249, -0.3902,  0.6492]],\n",
      "\n",
      "        [[-0.7096, -0.4927,  0.7202, -0.1586, -0.8798],\n",
      "         [ 0.8986,  0.9257, -0.9252,  0.9234,  0.8984]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0857 cost = 0.000813\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5177,  0.4882,  0.0995, -0.5120,  0.6926],\n",
      "         [ 0.9096, -0.8566, -0.9297,  0.8789, -0.1618]],\n",
      "\n",
      "        [[ 0.8678,  0.8618, -0.8905,  0.8471,  0.8849],\n",
      "         [-0.7546, -0.5014,  0.7686, -0.1774, -0.9115]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0857 cost = 0.000969\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6208,  0.1341, -0.3328,  0.6051],\n",
      "         [ 0.9580, -0.6743, -0.7363,  0.9409, -0.6987]],\n",
      "\n",
      "        [[ 0.9191,  0.9122, -0.9388,  0.9142,  0.8933],\n",
      "         [-0.4427, -0.5156,  0.5649, -0.1652, -0.7192]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0857 cost = 0.001443\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6208,  0.1341, -0.3327,  0.6051],\n",
      "         [ 0.9580, -0.6744, -0.7363,  0.9409, -0.6987]],\n",
      "\n",
      "        [[ 0.9192,  0.9122, -0.9388,  0.9142,  0.8934],\n",
      "         [-0.4427, -0.5156,  0.5649, -0.1652, -0.7192]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0858 cost = 0.001441\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4501,  0.6171,  0.0251, -0.3901,  0.6492],\n",
      "         [ 0.8899, -0.9512, -0.9597,  0.8013, -0.0215]],\n",
      "\n",
      "        [[ 0.8987,  0.9258, -0.9252,  0.9234,  0.8984],\n",
      "         [-0.7097, -0.4928,  0.7203, -0.1588, -0.8798]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0858 cost = 0.000901\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5179,  0.4882,  0.0995, -0.5121,  0.6927],\n",
      "         [ 0.9097, -0.8566, -0.9297,  0.8790, -0.1621]],\n",
      "\n",
      "        [[ 0.8678,  0.8618, -0.8905,  0.8471,  0.8849],\n",
      "         [-0.7547, -0.5015,  0.7687, -0.1777, -0.9115]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0858 cost = 0.000966\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8900, -0.9512, -0.9597,  0.8015, -0.0216],\n",
      "         [-0.4502,  0.6171,  0.0252, -0.3901,  0.6493]],\n",
      "\n",
      "        [[-0.7097, -0.4928,  0.7203, -0.1589, -0.8798],\n",
      "         [ 0.8987,  0.9258, -0.9252,  0.9234,  0.8985]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0859 cost = 0.000808\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.6208,  0.1341, -0.3327,  0.6052],\n",
      "         [ 0.9580, -0.6747, -0.7365,  0.9410, -0.6989]],\n",
      "\n",
      "        [[ 0.9192,  0.9122, -0.9388,  0.9142,  0.8934],\n",
      "         [-0.4428, -0.5158,  0.5650, -0.1655, -0.7193]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0859 cost = 0.001435\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9098, -0.8567, -0.9298,  0.8790, -0.1623],\n",
      "         [-0.5181,  0.4881,  0.0996, -0.5122,  0.6928]],\n",
      "\n",
      "        [[-0.7548, -0.5017,  0.7688, -0.1779, -0.9115],\n",
      "         [ 0.8678,  0.8618, -0.8905,  0.8471,  0.8849]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0859 cost = 0.001008\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9581, -0.6748, -0.7366,  0.9410, -0.6989],\n",
      "         [-0.4504,  0.6172,  0.0253, -0.3901,  0.6493]],\n",
      "\n",
      "        [[-0.4429, -0.5159,  0.5651, -0.1656, -0.7193],\n",
      "         [ 0.8987,  0.9258, -0.9252,  0.9234,  0.8985]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0860 cost = 0.001834\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3785,  0.6207,  0.1342, -0.3327,  0.6052],\n",
      "         [-0.5182,  0.4881,  0.0996, -0.5122,  0.6928]],\n",
      "\n",
      "        [[ 0.9192,  0.9122, -0.9388,  0.9142,  0.8934],\n",
      "         [ 0.8678,  0.8619, -0.8905,  0.8471,  0.8850]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0860 cost = 0.003213\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8901, -0.9512, -0.9598,  0.8017, -0.0219],\n",
      "         [ 0.9098, -0.8567, -0.9298,  0.8791, -0.1626]],\n",
      "\n",
      "        [[-0.7099, -0.4930,  0.7205, -0.1592, -0.8798],\n",
      "         [-0.7549, -0.5017,  0.7689, -0.1782, -0.9115]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0860 cost = 0.000804\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8901, -0.9513, -0.9598,  0.8018, -0.0219],\n",
      "         [-0.5183,  0.4881,  0.0996, -0.5122,  0.6929]],\n",
      "\n",
      "        [[-0.7099, -0.4930,  0.7205, -0.1593, -0.8798],\n",
      "         [ 0.8679,  0.8619, -0.8905,  0.8472,  0.8850]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0861 cost = 0.001041\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9581, -0.6750, -0.7367,  0.9410, -0.6990],\n",
      "         [-0.3787,  0.6207,  0.1342, -0.3327,  0.6053]],\n",
      "\n",
      "        [[-0.4431, -0.5160,  0.5652, -0.1658, -0.7194],\n",
      "         [ 0.9192,  0.9123, -0.9388,  0.9142,  0.8935]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0861 cost = 0.001466\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4506,  0.6174,  0.0255, -0.3899,  0.6493],\n",
      "         [ 0.9099, -0.8567, -0.9298,  0.8792, -0.1628]],\n",
      "\n",
      "        [[ 0.8987,  0.9258, -0.9252,  0.9235,  0.8986],\n",
      "         [-0.7550, -0.5018,  0.7690, -0.1784, -0.9116]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0861 cost = 0.000860\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5184,  0.4881,  0.0996, -0.5122,  0.6930],\n",
      "         [-0.3788,  0.6207,  0.1342, -0.3326,  0.6053]],\n",
      "\n",
      "        [[ 0.8680,  0.8620, -0.8905,  0.8472,  0.8851],\n",
      "         [ 0.9192,  0.9123, -0.9388,  0.9142,  0.8936]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0862 cost = 0.001046\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9581, -0.6752, -0.7368,  0.9411, -0.6991],\n",
      "         [ 0.9099, -0.8568, -0.9299,  0.8792, -0.1629]],\n",
      "\n",
      "        [[-0.4432, -0.5161,  0.5653, -0.1659, -0.7194],\n",
      "         [-0.7550, -0.5018,  0.7690, -0.1786, -0.9116]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0862 cost = 0.001800\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8902, -0.9513, -0.9598,  0.8020, -0.0221],\n",
      "         [-0.4507,  0.6175,  0.0256, -0.3898,  0.6494]],\n",
      "\n",
      "        [[-0.7100, -0.4930,  0.7206, -0.1596, -0.8798],\n",
      "         [ 0.8987,  0.9258, -0.9252,  0.9235,  0.8986]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0862 cost = 0.000800\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9100, -0.8568, -0.9299,  0.8792, -0.1631],\n",
      "         [-0.4507,  0.6176,  0.0257, -0.3897,  0.6494]],\n",
      "\n",
      "        [[-0.7551, -0.5019,  0.7691, -0.1788, -0.9116],\n",
      "         [ 0.8987,  0.9259, -0.9252,  0.9235,  0.8986]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0863 cost = 0.000802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9581, -0.6755, -0.7369,  0.9411, -0.6992],\n",
      "         [ 0.8903, -0.9513, -0.9598,  0.8020, -0.0222]],\n",
      "\n",
      "        [[-0.4432, -0.5162,  0.5653, -0.1661, -0.7195],\n",
      "         [-0.7100, -0.4930,  0.7207, -0.1597, -0.8798]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0863 cost = 0.002174\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5183,  0.4884,  0.0996, -0.5120,  0.6933],\n",
      "         [-0.3786,  0.6210,  0.1342, -0.3324,  0.6055]],\n",
      "\n",
      "        [[ 0.8680,  0.8621, -0.8906,  0.8472,  0.8851],\n",
      "         [ 0.9193,  0.9123, -0.9388,  0.9142,  0.8936]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0863 cost = 0.001040\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9100, -0.8569, -0.9299,  0.8793, -0.1634],\n",
      "         [ 0.9581, -0.6757, -0.7370,  0.9411, -0.6993]],\n",
      "\n",
      "        [[-0.7552, -0.5020,  0.7692, -0.1790, -0.9116],\n",
      "         [-0.4433, -0.5163,  0.5654, -0.1662, -0.7195]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0864 cost = 0.001809\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.6211,  0.1342, -0.3322,  0.6056],\n",
      "         [ 0.8904, -0.9513, -0.9598,  0.8022, -0.0224]],\n",
      "\n",
      "        [[ 0.9193,  0.9123, -0.9388,  0.9142,  0.8936],\n",
      "         [-0.7101, -0.4931,  0.7208, -0.1599, -0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0864 cost = 0.000711\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4506,  0.6179,  0.0259, -0.3894,  0.6496],\n",
      "         [-0.5181,  0.4887,  0.0996, -0.5118,  0.6934]],\n",
      "\n",
      "        [[ 0.8987,  0.9259, -0.9252,  0.9235,  0.8986],\n",
      "         [ 0.8680,  0.8621, -0.8906,  0.8472,  0.8852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0864 cost = 0.003233\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4506,  0.6180,  0.0259, -0.3893,  0.6496],\n",
      "         [-0.3782,  0.6213,  0.1342, -0.3321,  0.6056]],\n",
      "\n",
      "        [[ 0.8988,  0.9259, -0.9252,  0.9235,  0.8987],\n",
      "         [ 0.9193,  0.9123, -0.9388,  0.9143,  0.8937]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0865 cost = 0.000753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9101, -0.8569, -0.9300,  0.8794, -0.1637],\n",
      "         [-0.5180,  0.4888,  0.0996, -0.5116,  0.6935]],\n",
      "\n",
      "        [[-0.7553, -0.5022,  0.7693, -0.1793, -0.9116],\n",
      "         [ 0.8681,  0.8621, -0.8906,  0.8473,  0.8852]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0865 cost = 0.000992\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9581, -0.6761, -0.7372,  0.9411, -0.6994],\n",
      "         [ 0.8905, -0.9513, -0.9599,  0.8024, -0.0226]],\n",
      "\n",
      "        [[-0.4435, -0.5166,  0.5655, -0.1664, -0.7196],\n",
      "         [-0.7102, -0.4932,  0.7209, -0.1601, -0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0865 cost = 0.002161\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9101, -0.8570, -0.9300,  0.8794, -0.1638],\n",
      "         [-0.4506,  0.6182,  0.0260, -0.3891,  0.6498]],\n",
      "\n",
      "        [[-0.7553, -0.5022,  0.7693, -0.1794, -0.9117],\n",
      "         [ 0.8988,  0.9259, -0.9252,  0.9236,  0.8987]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0866 cost = 0.000795\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9581, -0.6762, -0.7373,  0.9411, -0.6995],\n",
      "         [-0.5180,  0.4891,  0.0996, -0.5115,  0.6937]],\n",
      "\n",
      "        [[-0.4436, -0.5167,  0.5656, -0.1664, -0.7196],\n",
      "         [ 0.8682,  0.8622, -0.8906,  0.8473,  0.8853]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0866 cost = 0.002255\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8905, -0.9514, -0.9599,  0.8026, -0.0228],\n",
      "         [-0.3779,  0.6216,  0.1342, -0.3318,  0.6059]],\n",
      "\n",
      "        [[-0.7103, -0.4933,  0.7210, -0.1603, -0.8799],\n",
      "         [ 0.9193,  0.9124, -0.9388,  0.9143,  0.8938]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0866 cost = 0.000495\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3779,  0.6216,  0.1342, -0.3318,  0.6059],\n",
      "         [ 0.9102, -0.8571, -0.9300,  0.8795, -0.1641]],\n",
      "\n",
      "        [[ 0.9193,  0.9124, -0.9388,  0.9143,  0.8938],\n",
      "         [-0.7554, -0.5023,  0.7694, -0.1797, -0.9117]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0867 cost = 0.000668\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4507,  0.6184,  0.0262, -0.3889,  0.6499],\n",
      "         [ 0.8906, -0.9514, -0.9599,  0.8027, -0.0229]],\n",
      "\n",
      "        [[ 0.8988,  0.9259, -0.9252,  0.9236,  0.8988],\n",
      "         [-0.7103, -0.4933,  0.7210, -0.1604, -0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0867 cost = 0.000877\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5180,  0.4892,  0.0996, -0.5114,  0.6939],\n",
      "         [ 0.9582, -0.6766, -0.7375,  0.9412, -0.6996]],\n",
      "\n",
      "        [[ 0.8682,  0.8623, -0.8907,  0.8474,  0.8854],\n",
      "         [-0.4437, -0.5168,  0.5657, -0.1666, -0.7197]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0867 cost = 0.001741\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9102, -0.8571, -0.9301,  0.8795, -0.1643],\n",
      "         [ 0.8907, -0.9514, -0.9599,  0.8028, -0.0230]],\n",
      "\n",
      "        [[-0.7555, -0.5024,  0.7695, -0.1799, -0.9117],\n",
      "         [-0.7103, -0.4934,  0.7211, -0.1606, -0.8799]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0868 cost = 0.001392\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3778,  0.6217,  0.1343, -0.3317,  0.6060],\n",
      "         [-0.4507,  0.6186,  0.0263, -0.3888,  0.6499]],\n",
      "\n",
      "        [[ 0.9193,  0.9124, -0.9388,  0.9143,  0.8938],\n",
      "         [ 0.8988,  0.9259, -0.9252,  0.9236,  0.8988]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0868 cost = 0.001149\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5181,  0.4893,  0.0996, -0.5114,  0.6940],\n",
      "         [ 0.9582, -0.6768, -0.7376,  0.9412, -0.6997]],\n",
      "\n",
      "        [[ 0.8682,  0.8623, -0.8907,  0.8474,  0.8854],\n",
      "         [-0.4438, -0.5170,  0.5658, -0.1668, -0.7197]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0868 cost = 0.001736\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8908, -0.9514, -0.9600,  0.8031, -0.0232],\n",
      "         [ 0.9582, -0.6769, -0.7376,  0.9412, -0.6997]],\n",
      "\n",
      "        [[-0.7104, -0.4935,  0.7212, -0.1607, -0.8800],\n",
      "         [-0.4438, -0.5171,  0.5658, -0.1668, -0.7198]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0869 cost = 0.001680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4508,  0.6187,  0.0264, -0.3887,  0.6500],\n",
      "         [-0.5181,  0.4893,  0.0996, -0.5114,  0.6941]],\n",
      "\n",
      "        [[ 0.8988,  0.9260, -0.9251,  0.9236,  0.8988],\n",
      "         [ 0.8683,  0.8623, -0.8907,  0.8474,  0.8854]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0869 cost = 0.003204\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9103, -0.8572, -0.9301,  0.8797, -0.1648],\n",
      "         [-0.3779,  0.6218,  0.1343, -0.3316,  0.6060]],\n",
      "\n",
      "        [[-0.7557, -0.5027,  0.7697, -0.1803, -0.9118],\n",
      "         [ 0.9194,  0.9124, -0.9388,  0.9143,  0.8939]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0869 cost = 0.000502\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4509,  0.6188,  0.0264, -0.3886,  0.6500],\n",
      "         [ 0.9582, -0.6770, -0.7378,  0.9412, -0.6998]],\n",
      "\n",
      "        [[ 0.8988,  0.9260, -0.9252,  0.9236,  0.8989],\n",
      "         [-0.4440, -0.5173,  0.5660, -0.1670, -0.7198]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0870 cost = 0.001574\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6218,  0.1343, -0.3316,  0.6060],\n",
      "         [-0.5183,  0.4894,  0.0996, -0.5114,  0.6942]],\n",
      "\n",
      "        [[ 0.9194,  0.9124, -0.9388,  0.9143,  0.8939],\n",
      "         [ 0.8683,  0.8624, -0.8907,  0.8474,  0.8855]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0870 cost = 0.003156\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9104, -0.8573, -0.9302,  0.8797, -0.1650],\n",
      "         [ 0.8909, -0.9515, -0.9600,  0.8033, -0.0235]],\n",
      "\n",
      "        [[-0.7558, -0.5029,  0.7698, -0.1805, -0.9118],\n",
      "         [-0.7105, -0.4937,  0.7213, -0.1610, -0.8800]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0870 cost = 0.001380\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4510,  0.6191,  0.0265, -0.3885,  0.6501],\n",
      "         [-0.3781,  0.6219,  0.1343, -0.3315,  0.6061]],\n",
      "\n",
      "        [[ 0.8989,  0.9260, -0.9252,  0.9236,  0.8989],\n",
      "         [ 0.9194,  0.9124, -0.9388,  0.9144,  0.8940]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0871 cost = 0.000739\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9582, -0.6770, -0.7379,  0.9412, -0.6999],\n",
      "         [ 0.8910, -0.9515, -0.9600,  0.8034, -0.0236]],\n",
      "\n",
      "        [[-0.4442, -0.5176,  0.5661, -0.1671, -0.7199],\n",
      "         [-0.7106, -0.4938,  0.7214, -0.1611, -0.8800]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0871 cost = 0.002128\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5184,  0.4896,  0.0996, -0.5113,  0.6945],\n",
      "         [ 0.9104, -0.8573, -0.9302,  0.8797, -0.1652]],\n",
      "\n",
      "        [[ 0.8684,  0.8625, -0.8908,  0.8475,  0.8856],\n",
      "         [-0.7559, -0.5031,  0.7699, -0.1807, -0.9118]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0871 cost = 0.000930\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8911, -0.9515, -0.9601,  0.8036, -0.0237],\n",
      "         [-0.4511,  0.6193,  0.0266, -0.3884,  0.6502]],\n",
      "\n",
      "        [[-0.7106, -0.4940,  0.7214, -0.1612, -0.8800],\n",
      "         [ 0.8989,  0.9260, -0.9252,  0.9237,  0.8990]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0872 cost = 0.000778\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5185,  0.4897,  0.0995, -0.5113,  0.6946],\n",
      "         [ 0.9105, -0.8573, -0.9303,  0.8798, -0.1654]],\n",
      "\n",
      "        [[ 0.8685,  0.8625, -0.8908,  0.8476,  0.8857],\n",
      "         [-0.7559, -0.5033,  0.7700, -0.1808, -0.9118]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0872 cost = 0.000928\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9582, -0.6771, -0.7381,  0.9412, -0.7000],\n",
      "         [-0.3782,  0.6221,  0.1343, -0.3314,  0.6062]],\n",
      "\n",
      "        [[-0.4443, -0.5179,  0.5663, -0.1672, -0.7200],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9144,  0.8941]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0872 cost = 0.001424\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5186,  0.4898,  0.0995, -0.5113,  0.6946],\n",
      "         [ 0.9105, -0.8573, -0.9303,  0.8798, -0.1655]],\n",
      "\n",
      "        [[ 0.8685,  0.8626, -0.8908,  0.8476,  0.8857],\n",
      "         [-0.7560, -0.5034,  0.7700, -0.1809, -0.9118]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0873 cost = 0.000927\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9583, -0.6773, -0.7382,  0.9413, -0.7000],\n",
      "         [-0.3783,  0.6221,  0.1343, -0.3314,  0.6062]],\n",
      "\n",
      "        [[-0.4444, -0.5180,  0.5663, -0.1673, -0.7200],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9144,  0.8942]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0873 cost = 0.001422\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8912, -0.9516, -0.9601,  0.8039, -0.0240],\n",
      "         [-0.4513,  0.6196,  0.0267, -0.3883,  0.6503]],\n",
      "\n",
      "        [[-0.7107, -0.4942,  0.7216, -0.1615, -0.8801],\n",
      "         [ 0.8989,  0.9260, -0.9252,  0.9237,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0873 cost = 0.000774\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4513,  0.6196,  0.0268, -0.3882,  0.6503],\n",
      "         [ 0.9583, -0.6774, -0.7383,  0.9413, -0.7001]],\n",
      "\n",
      "        [[ 0.8989,  0.9260, -0.9252,  0.9237,  0.8991],\n",
      "         [-0.4444, -0.5181,  0.5664, -0.1674, -0.7201]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0874 cost = 0.001557\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5188,  0.4899,  0.0996, -0.5113,  0.6947],\n",
      "         [-0.3784,  0.6221,  0.1343, -0.3314,  0.6062]],\n",
      "\n",
      "        [[ 0.8686,  0.8626, -0.8908,  0.8476,  0.8858],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9144,  0.8942]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0874 cost = 0.001006\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8913, -0.9516, -0.9601,  0.8041, -0.0242],\n",
      "         [ 0.9106, -0.8574, -0.9304,  0.8799, -0.1659]],\n",
      "\n",
      "        [[-0.7108, -0.4944,  0.7216, -0.1617, -0.8801],\n",
      "         [-0.7561, -0.5037,  0.7702, -0.1812, -0.9119]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0874 cost = 0.000773\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9106, -0.8574, -0.9304,  0.8800, -0.1660],\n",
      "         [ 0.9583, -0.6777, -0.7384,  0.9413, -0.7002]],\n",
      "\n",
      "        [[-0.7561, -0.5038,  0.7702, -0.1813, -0.9119],\n",
      "         [-0.4445, -0.5183,  0.5664, -0.1675, -0.7201]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0875 cost = 0.001757\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8914, -0.9516, -0.9602,  0.8042, -0.0243],\n",
      "         [-0.4514,  0.6199,  0.0269, -0.3880,  0.6503]],\n",
      "\n",
      "        [[-0.7108, -0.4945,  0.7217, -0.1618, -0.8801],\n",
      "         [ 0.8989,  0.9261, -0.9252,  0.9237,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0875 cost = 0.000771\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5187,  0.4901,  0.0996, -0.5111,  0.6948],\n",
      "         [-0.3782,  0.6224,  0.1343, -0.3312,  0.6063]],\n",
      "\n",
      "        [[ 0.8686,  0.8626, -0.8908,  0.8476,  0.8858],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9144,  0.8942]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0875 cost = 0.001002\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8914, -0.9516, -0.9602,  0.8043, -0.0244],\n",
      "         [-0.4513,  0.6200,  0.0270, -0.3878,  0.6504]],\n",
      "\n",
      "        [[-0.7109, -0.4946,  0.7217, -0.1619, -0.8802],\n",
      "         [ 0.8990,  0.9261, -0.9252,  0.9237,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0876 cost = 0.000769\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9583, -0.6779, -0.7386,  0.9413, -0.7003],\n",
      "         [-0.5185,  0.4903,  0.0996, -0.5109,  0.6949]],\n",
      "\n",
      "        [[-0.4447, -0.5186,  0.5665, -0.1677, -0.7202],\n",
      "         [ 0.8686,  0.8626, -0.8908,  0.8476,  0.8859]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0876 cost = 0.002201\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3779,  0.6226,  0.1344, -0.3309,  0.6064],\n",
      "         [ 0.9107, -0.8575, -0.9304,  0.8801, -0.1664]],\n",
      "\n",
      "        [[ 0.9195,  0.9125, -0.9388,  0.9145,  0.8943],\n",
      "         [-0.7563, -0.5040,  0.7704, -0.1816, -0.9119]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0876 cost = 0.000650\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8915, -0.9517, -0.9602,  0.8044, -0.0246],\n",
      "         [ 0.9583, -0.6781, -0.7387,  0.9414, -0.7003]],\n",
      "\n",
      "        [[-0.7110, -0.4947,  0.7218, -0.1620, -0.8802],\n",
      "         [-0.4448, -0.5186,  0.5666, -0.1678, -0.7203]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0877 cost = 0.001645\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9107, -0.8575, -0.9304,  0.8801, -0.1666],\n",
      "         [-0.5184,  0.4905,  0.0996, -0.5107,  0.6950]],\n",
      "\n",
      "        [[-0.7563, -0.5041,  0.7704, -0.1817, -0.9119],\n",
      "         [ 0.8686,  0.8627, -0.8908,  0.8476,  0.8859]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0877 cost = 0.000960\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4512,  0.6203,  0.0272, -0.3874,  0.6505],\n",
      "         [-0.3776,  0.6228,  0.1344, -0.3308,  0.6065]],\n",
      "\n",
      "        [[ 0.8990,  0.9261, -0.9251,  0.9238,  0.8992],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9145,  0.8943]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0877 cost = 0.000724\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9108, -0.8575, -0.9305,  0.8802, -0.1667],\n",
      "         [-0.5183,  0.4906,  0.0996, -0.5106,  0.6951]],\n",
      "\n",
      "        [[-0.7564, -0.5042,  0.7705, -0.1819, -0.9120],\n",
      "         [ 0.8686,  0.8627, -0.8908,  0.8476,  0.8859]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0878 cost = 0.000958\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3775,  0.6229,  0.1344, -0.3307,  0.6066],\n",
      "         [-0.4512,  0.6204,  0.0273, -0.3873,  0.6506]],\n",
      "\n",
      "        [[ 0.9195,  0.9125, -0.9388,  0.9145,  0.8943],\n",
      "         [ 0.8990,  0.9261, -0.9251,  0.9238,  0.8992]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0878 cost = 0.001120\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9583, -0.6784, -0.7388,  0.9414, -0.7005],\n",
      "         [ 0.8916, -0.9517, -0.9602,  0.8047, -0.0248]],\n",
      "\n",
      "        [[-0.4450, -0.5189,  0.5668, -0.1680, -0.7204],\n",
      "         [-0.7111, -0.4948,  0.7220, -0.1623, -0.8802]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0878 cost = 0.002087\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.6230,  0.1344, -0.3306,  0.6067],\n",
      "         [-0.4513,  0.6205,  0.0274, -0.3871,  0.6506]],\n",
      "\n",
      "        [[ 0.9195,  0.9125, -0.9388,  0.9145,  0.8943],\n",
      "         [ 0.8990,  0.9261, -0.9251,  0.9238,  0.8992]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0879 cost = 0.001119\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8917, -0.9517, -0.9603,  0.8048, -0.0249],\n",
      "         [-0.5183,  0.4908,  0.0996, -0.5104,  0.6953]],\n",
      "\n",
      "        [[-0.7111, -0.4949,  0.7220, -0.1624, -0.8802],\n",
      "         [ 0.8687,  0.8627, -0.8909,  0.8477,  0.8860]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0879 cost = 0.000990\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9109, -0.8576, -0.9305,  0.8803, -0.1671],\n",
      "         [ 0.9584, -0.6787, -0.7389,  0.9414, -0.7006]],\n",
      "\n",
      "        [[-0.7565, -0.5044,  0.7706, -0.1823, -0.9120],\n",
      "         [-0.4451, -0.5190,  0.5669, -0.1681, -0.7204]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0879 cost = 0.001735\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9584, -0.6788, -0.7389,  0.9414, -0.7006],\n",
      "         [ 0.9109, -0.8576, -0.9305,  0.8803, -0.1672]],\n",
      "\n",
      "        [[-0.4451, -0.5190,  0.5669, -0.1682, -0.7204],\n",
      "         [-0.7566, -0.5044,  0.7707, -0.1824, -0.9120]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0880 cost = 0.001719\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4513,  0.6207,  0.0275, -0.3870,  0.6508],\n",
      "         [-0.3773,  0.6231,  0.1344, -0.3304,  0.6069]],\n",
      "\n",
      "        [[ 0.8990,  0.9261, -0.9251,  0.9238,  0.8992],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9145,  0.8944]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0880 cost = 0.000718\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8918, -0.9517, -0.9603,  0.8050, -0.0252],\n",
      "         [-0.5182,  0.4909,  0.0996, -0.5103,  0.6954]],\n",
      "\n",
      "        [[-0.7113, -0.4950,  0.7222, -0.1627, -0.8803],\n",
      "         [ 0.8687,  0.8628, -0.8909,  0.8477,  0.8860]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0880 cost = 0.000987\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8918, -0.9518, -0.9603,  0.8051, -0.0253],\n",
      "         [ 0.9584, -0.6791, -0.7391,  0.9415, -0.7008]],\n",
      "\n",
      "        [[-0.7113, -0.4950,  0.7222, -0.1628, -0.8803],\n",
      "         [-0.4452, -0.5192,  0.5670, -0.1684, -0.7205]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0881 cost = 0.001627\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4513,  0.6208,  0.0276, -0.3869,  0.6509],\n",
      "         [-0.3773,  0.6232,  0.1344, -0.3304,  0.6070]],\n",
      "\n",
      "        [[ 0.8990,  0.9261, -0.9251,  0.9238,  0.8993],\n",
      "         [ 0.9195,  0.9125, -0.9388,  0.9145,  0.8944]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0881 cost = 0.000716\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5182,  0.4910,  0.0996, -0.5103,  0.6956],\n",
      "         [ 0.9110, -0.8578, -0.9306,  0.8804, -0.1677]],\n",
      "\n",
      "        [[ 0.8687,  0.8628, -0.8908,  0.8476,  0.8860],\n",
      "         [-0.7568, -0.5046,  0.7709, -0.1828, -0.9121]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0881 cost = 0.000905\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9110, -0.8578, -0.9306,  0.8804, -0.1678],\n",
      "         [-0.5182,  0.4910,  0.0996, -0.5103,  0.6956]],\n",
      "\n",
      "        [[-0.7568, -0.5046,  0.7709, -0.1829, -0.9121],\n",
      "         [ 0.8687,  0.8628, -0.8908,  0.8476,  0.8860]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0882 cost = 0.000948\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3773,  0.6233,  0.1345, -0.3303,  0.6071],\n",
      "         [ 0.8919, -0.9518, -0.9603,  0.8053, -0.0255]],\n",
      "\n",
      "        [[ 0.9195,  0.9125, -0.9388,  0.9145,  0.8944],\n",
      "         [-0.7115, -0.4951,  0.7224, -0.1631, -0.8803]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0882 cost = 0.000674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4514,  0.6210,  0.0277, -0.3867,  0.6510],\n",
      "         [ 0.9584, -0.6795, -0.7393,  0.9415, -0.7009]],\n",
      "\n",
      "        [[ 0.8990,  0.9261, -0.9251,  0.9238,  0.8993],\n",
      "         [-0.4454, -0.5195,  0.5672, -0.1686, -0.7206]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0882 cost = 0.001517\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4515,  0.6210,  0.0278, -0.3867,  0.6510],\n",
      "         [-0.3773,  0.6234,  0.1345, -0.3303,  0.6072]],\n",
      "\n",
      "        [[ 0.8990,  0.9261, -0.9251,  0.9238,  0.8993],\n",
      "         [ 0.9195,  0.9126, -0.9388,  0.9145,  0.8944]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0883 cost = 0.000713\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8920, -0.9518, -0.9604,  0.8055, -0.0256],\n",
      "         [ 0.9111, -0.8579, -0.9306,  0.8805, -0.1681]],\n",
      "\n",
      "        [[-0.7116, -0.4952,  0.7225, -0.1633, -0.8803],\n",
      "         [-0.7569, -0.5048,  0.7710, -0.1832, -0.9121]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0883 cost = 0.000755\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5184,  0.4910,  0.0997, -0.5103,  0.6958],\n",
      "         [ 0.9584, -0.6797, -0.7393,  0.9416, -0.7010]],\n",
      "\n",
      "        [[ 0.8688,  0.8628, -0.8908,  0.8477,  0.8861],\n",
      "         [-0.4455, -0.5196,  0.5673, -0.1688, -0.7207]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0883 cost = 0.001666\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9111, -0.8579, -0.9307,  0.8806, -0.1683],\n",
      "         [-0.4516,  0.6211,  0.0279, -0.3866,  0.6510]],\n",
      "\n",
      "        [[-0.7570, -0.5049,  0.7711, -0.1834, -0.9121],\n",
      "         [ 0.8990,  0.9262, -0.9251,  0.9238,  0.8993]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0884 cost = 0.000754\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9585, -0.6798, -0.7394,  0.9416, -0.7011],\n",
      "         [-0.5185,  0.4910,  0.0997, -0.5103,  0.6959]],\n",
      "\n",
      "        [[-0.4456, -0.5197,  0.5673, -0.1689, -0.7207],\n",
      "         [ 0.8688,  0.8629, -0.8908,  0.8476,  0.8861]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0884 cost = 0.002160\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3774,  0.6235,  0.1345, -0.3303,  0.6073],\n",
      "         [ 0.8921, -0.9518, -0.9604,  0.8057, -0.0258]],\n",
      "\n",
      "        [[ 0.9195,  0.9126, -0.9388,  0.9145,  0.8945],\n",
      "         [-0.7117, -0.4953,  0.7226, -0.1635, -0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0884 cost = 0.000669\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5186,  0.4910,  0.0997, -0.5104,  0.6959],\n",
      "         [ 0.9585, -0.6800, -0.7395,  0.9416, -0.7012]],\n",
      "\n",
      "        [[ 0.8688,  0.8629, -0.8908,  0.8476,  0.8861],\n",
      "         [-0.4457, -0.5198,  0.5674, -0.1690, -0.7208]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0885 cost = 0.001660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4519,  0.6212,  0.0281, -0.3864,  0.6510],\n",
      "         [ 0.9112, -0.8580, -0.9307,  0.8807, -0.1686]],\n",
      "\n",
      "        [[ 0.8990,  0.9262, -0.9251,  0.9238,  0.8993],\n",
      "         [-0.7571, -0.5050,  0.7712, -0.1837, -0.9121]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0885 cost = 0.000802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3776,  0.6234,  0.1346, -0.3303,  0.6073],\n",
      "         [ 0.8922, -0.9519, -0.9604,  0.8059, -0.0260]],\n",
      "\n",
      "        [[ 0.9196,  0.9126, -0.9388,  0.9145,  0.8945],\n",
      "         [-0.7118, -0.4954,  0.7227, -0.1637, -0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0885 cost = 0.000668\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3776,  0.6234,  0.1346, -0.3303,  0.6073],\n",
      "         [-0.4520,  0.6212,  0.0282, -0.3864,  0.6510]],\n",
      "\n",
      "        [[ 0.9196,  0.9126, -0.9388,  0.9145,  0.8945],\n",
      "         [ 0.8990,  0.9262, -0.9251,  0.9238,  0.8993]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0886 cost = 0.001103\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9585, -0.6802, -0.7396,  0.9417, -0.7013],\n",
      "         [ 0.8922, -0.9519, -0.9604,  0.8060, -0.0261]],\n",
      "\n",
      "        [[-0.4458, -0.5200,  0.5675, -0.1692, -0.7209],\n",
      "         [-0.7118, -0.4955,  0.7228, -0.1638, -0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0886 cost = 0.002044\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9113, -0.8581, -0.9307,  0.8808, -0.1689],\n",
      "         [-0.5188,  0.4910,  0.0997, -0.5104,  0.6960]],\n",
      "\n",
      "        [[-0.7572, -0.5052,  0.7713, -0.1840, -0.9122],\n",
      "         [ 0.8688,  0.8629, -0.8908,  0.8476,  0.8861]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0886 cost = 0.000937\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5189,  0.4909,  0.0997, -0.5105,  0.6960],\n",
      "         [ 0.8923, -0.9519, -0.9604,  0.8062, -0.0262]],\n",
      "\n",
      "        [[ 0.8688,  0.8629, -0.8908,  0.8476,  0.8861],\n",
      "         [-0.7119, -0.4956,  0.7228, -0.1640, -0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0887 cost = 0.000958\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3779,  0.6234,  0.1346, -0.3303,  0.6073],\n",
      "         [ 0.9113, -0.8581, -0.9308,  0.8808, -0.1691]],\n",
      "\n",
      "        [[ 0.9196,  0.9126, -0.9388,  0.9145,  0.8945],\n",
      "         [-0.7573, -0.5053,  0.7714, -0.1842, -0.9122]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0887 cost = 0.000631\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4522,  0.6214,  0.0284, -0.3863,  0.6511],\n",
      "         [ 0.9585, -0.6805, -0.7398,  0.9417, -0.7014]],\n",
      "\n",
      "        [[ 0.8990,  0.9262, -0.9251,  0.9239,  0.8994],\n",
      "         [-0.4459, -0.5202,  0.5676, -0.1694, -0.7209]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0887 cost = 0.001494\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9114, -0.8581, -0.9308,  0.8809, -0.1693],\n",
      "         [ 0.8924, -0.9520, -0.9605,  0.8064, -0.0264]],\n",
      "\n",
      "        [[-0.7573, -0.5054,  0.7715, -0.1844, -0.9122],\n",
      "         [-0.7119, -0.4957,  0.7229, -0.1642, -0.8804]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0888 cost = 0.001313\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4523,  0.6214,  0.0285, -0.3863,  0.6511],\n",
      "         [ 0.9586, -0.6806, -0.7399,  0.9417, -0.7015]],\n",
      "\n",
      "        [[ 0.8990,  0.9262, -0.9251,  0.9239,  0.8994],\n",
      "         [-0.4460, -0.5203,  0.5676, -0.1696, -0.7210]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0888 cost = 0.001491\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5192,  0.4908,  0.0998, -0.5106,  0.6961],\n",
      "         [-0.3781,  0.6233,  0.1346, -0.3303,  0.6073]],\n",
      "\n",
      "        [[ 0.8688,  0.8629, -0.8908,  0.8476,  0.8861],\n",
      "         [ 0.9196,  0.9126, -0.9388,  0.9145,  0.8946]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0888 cost = 0.000969\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5192,  0.4909,  0.0998, -0.5106,  0.6961],\n",
      "         [-0.4523,  0.6215,  0.0286, -0.3863,  0.6511]],\n",
      "\n",
      "        [[ 0.8688,  0.8629, -0.8908,  0.8476,  0.8861],\n",
      "         [ 0.8990,  0.9262, -0.9251,  0.9239,  0.8994]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0889 cost = 0.001506\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9586, -0.6808, -0.7400,  0.9418, -0.7016],\n",
      "         [ 0.9115, -0.8582, -0.9308,  0.8810, -0.1696]],\n",
      "\n",
      "        [[-0.4461, -0.5205,  0.5677, -0.1697, -0.7211],\n",
      "         [-0.7575, -0.5057,  0.7716, -0.1847, -0.9122]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0889 cost = 0.001675\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8927, -0.9520, -0.9605,  0.8069, -0.0268],\n",
      "         [-0.3779,  0.6235,  0.1347, -0.3301,  0.6074]],\n",
      "\n",
      "        [[-0.7121, -0.4960,  0.7231, -0.1645, -0.8805],\n",
      "         [ 0.9196,  0.9126, -0.9388,  0.9145,  0.8946]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0889 cost = 0.000463\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4522,  0.6218,  0.0287, -0.3861,  0.6512],\n",
      "         [ 0.9115, -0.8582, -0.9309,  0.8810, -0.1698]],\n",
      "\n",
      "        [[ 0.8990,  0.9262, -0.9251,  0.9239,  0.8994],\n",
      "         [-0.7575, -0.5058,  0.7717, -0.1848, -0.9122]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0890 cost = 0.000791\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8927, -0.9520, -0.9606,  0.8070, -0.0269],\n",
      "         [-0.5190,  0.4911,  0.0998, -0.5103,  0.6962]],\n",
      "\n",
      "        [[-0.7121, -0.4961,  0.7231, -0.1646, -0.8805],\n",
      "         [ 0.8688,  0.8629, -0.8908,  0.8476,  0.8862]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0890 cost = 0.000964\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9586, -0.6811, -0.7402,  0.9418, -0.7017],\n",
      "         [-0.3778,  0.6237,  0.1347, -0.3299,  0.6075]],\n",
      "\n",
      "        [[-0.4462, -0.5208,  0.5678, -0.1699, -0.7211],\n",
      "         [ 0.9196,  0.9126, -0.9388,  0.9146,  0.8946]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0890 cost = 0.001354\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8928, -0.9521, -0.9606,  0.8071, -0.0270],\n",
      "         [ 0.9586, -0.6812, -0.7403,  0.9418, -0.7017]],\n",
      "\n",
      "        [[-0.7122, -0.4962,  0.7232, -0.1647, -0.8806],\n",
      "         [-0.4462, -0.5208,  0.5678, -0.1700, -0.7212]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0891 cost = 0.001581\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5190,  0.4913,  0.0998, -0.5102,  0.6963],\n",
      "         [ 0.9116, -0.8583, -0.9309,  0.8811, -0.1701]],\n",
      "\n",
      "        [[ 0.8688,  0.8629, -0.8908,  0.8476,  0.8862],\n",
      "         [-0.7576, -0.5060,  0.7718, -0.1851, -0.9123]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0891 cost = 0.000882\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3777,  0.6238,  0.1347, -0.3298,  0.6075],\n",
      "         [-0.4522,  0.6221,  0.0288, -0.3858,  0.6513]],\n",
      "\n",
      "        [[ 0.9196,  0.9126, -0.9388,  0.9146,  0.8946],\n",
      "         [ 0.8990,  0.9262, -0.9250,  0.9239,  0.8994]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0891 cost = 0.001090\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9116, -0.8583, -0.9309,  0.8812, -0.1703],\n",
      "         [-0.3777,  0.6238,  0.1347, -0.3297,  0.6076]],\n",
      "\n",
      "        [[-0.7577, -0.5062,  0.7718, -0.1853, -0.9123],\n",
      "         [ 0.9196,  0.9126, -0.9387,  0.9146,  0.8946]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0892 cost = 0.000471\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4523,  0.6222,  0.0289, -0.3857,  0.6514],\n",
      "         [-0.5190,  0.4913,  0.0998, -0.5101,  0.6963]],\n",
      "\n",
      "        [[ 0.8990,  0.9262, -0.9250,  0.9239,  0.8994],\n",
      "         [ 0.8688,  0.8629, -0.8908,  0.8476,  0.8862]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0892 cost = 0.003105\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8930, -0.9521, -0.9606,  0.8074, -0.0273],\n",
      "         [ 0.9587, -0.6816, -0.7405,  0.9419, -0.7019]],\n",
      "\n",
      "        [[-0.7123, -0.4964,  0.7233, -0.1650, -0.8806],\n",
      "         [-0.4464, -0.5211,  0.5680, -0.1702, -0.7213]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0892 cost = 0.001574\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3777,  0.6239,  0.1347, -0.3297,  0.6076],\n",
      "         [ 0.9117, -0.8584, -0.9310,  0.8812, -0.1705]],\n",
      "\n",
      "        [[ 0.9196,  0.9126, -0.9388,  0.9146,  0.8947],\n",
      "         [-0.7578, -0.5063,  0.7719, -0.1854, -0.9123]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0893 cost = 0.000621\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5191,  0.4915,  0.0998, -0.5101,  0.6965],\n",
      "         [ 0.9587, -0.6816, -0.7406,  0.9419, -0.7019]],\n",
      "\n",
      "        [[ 0.8689,  0.8630, -0.8908,  0.8476,  0.8863],\n",
      "         [-0.4465, -0.5212,  0.5680, -0.1702, -0.7213]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0893 cost = 0.001621\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4524,  0.6224,  0.0289, -0.3855,  0.6515],\n",
      "         [ 0.8930, -0.9521, -0.9606,  0.8076, -0.0274]],\n",
      "\n",
      "        [[ 0.8991,  0.9263, -0.9251,  0.9239,  0.8995],\n",
      "         [-0.7124, -0.4965,  0.7234, -0.1652, -0.8806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0893 cost = 0.000808\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9118, -0.8584, -0.9310,  0.8813, -0.1707],\n",
      "         [-0.4524,  0.6225,  0.0290, -0.3855,  0.6515]],\n",
      "\n",
      "        [[-0.7578, -0.5065,  0.7720, -0.1856, -0.9123],\n",
      "         [ 0.8991,  0.9263, -0.9251,  0.9240,  0.8995]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0894 cost = 0.000734\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5192,  0.4915,  0.0998, -0.5101,  0.6966],\n",
      "         [ 0.9587, -0.6817, -0.7407,  0.9419, -0.7020]],\n",
      "\n",
      "        [[ 0.8690,  0.8630, -0.8908,  0.8477,  0.8863],\n",
      "         [-0.4466, -0.5214,  0.5681, -0.1704, -0.7214]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0894 cost = 0.001616\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3779,  0.6240,  0.1347, -0.3296,  0.6076],\n",
      "         [ 0.8931, -0.9521, -0.9607,  0.8077, -0.0276]],\n",
      "\n",
      "        [[ 0.9196,  0.9126, -0.9388,  0.9146,  0.8948],\n",
      "         [-0.7124, -0.4966,  0.7235, -0.1653, -0.8806]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0894 cost = 0.000649\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3780,  0.6240,  0.1347, -0.3296,  0.6076],\n",
      "         [ 0.9118, -0.8584, -0.9311,  0.8814, -0.1709]],\n",
      "\n",
      "        [[ 0.9197,  0.9126, -0.9388,  0.9146,  0.8948],\n",
      "         [-0.7579, -0.5066,  0.7721, -0.1858, -0.9124]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0895 cost = 0.000618\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5194,  0.4915,  0.0998, -0.5101,  0.6966],\n",
      "         [-0.4526,  0.6227,  0.0291, -0.3854,  0.6515]],\n",
      "\n",
      "        [[ 0.8690,  0.8630, -0.8908,  0.8477,  0.8864],\n",
      "         [ 0.8991,  0.9263, -0.9251,  0.9240,  0.8995]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0895 cost = 0.001482\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9587, -0.6818, -0.7408,  0.9419, -0.7021],\n",
      "         [ 0.8932, -0.9522, -0.9607,  0.8079, -0.0277]],\n",
      "\n",
      "        [[-0.4467, -0.5217,  0.5682, -0.1705, -0.7215],\n",
      "         [-0.7125, -0.4968,  0.7235, -0.1655, -0.8807]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0895 cost = 0.001993\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8932, -0.9522, -0.9607,  0.8080, -0.0278],\n",
      "         [-0.4527,  0.6228,  0.0291, -0.3853,  0.6515]],\n",
      "\n",
      "        [[-0.7125, -0.4968,  0.7236, -0.1655, -0.8807],\n",
      "         [ 0.8991,  0.9263, -0.9251,  0.9240,  0.8996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0896 cost = 0.000727\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3781,  0.6241,  0.1348, -0.3295,  0.6076],\n",
      "         [ 0.9587, -0.6819, -0.7409,  0.9420, -0.7021]],\n",
      "\n",
      "        [[ 0.9197,  0.9126, -0.9388,  0.9146,  0.8948],\n",
      "         [-0.4468, -0.5218,  0.5683, -0.1706, -0.7215]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0896 cost = 0.001294\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5195,  0.4916,  0.0999, -0.5100,  0.6966],\n",
      "         [ 0.9119, -0.8584, -0.9311,  0.8815, -0.1713]],\n",
      "\n",
      "        [[ 0.8690,  0.8631, -0.8909,  0.8477,  0.8864],\n",
      "         [-0.7581, -0.5069,  0.7722, -0.1862, -0.9124]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0896 cost = 0.000869\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9587, -0.6820, -0.7410,  0.9420, -0.7022],\n",
      "         [-0.5195,  0.4917,  0.0999, -0.5099,  0.6967]],\n",
      "\n",
      "        [[-0.4469, -0.5219,  0.5683, -0.1707, -0.7215],\n",
      "         [ 0.8690,  0.8631, -0.8909,  0.8477,  0.8864]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0897 cost = 0.002098\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3782,  0.6241,  0.1348, -0.3294,  0.6076],\n",
      "         [-0.4527,  0.6230,  0.0292, -0.3851,  0.6516]],\n",
      "\n",
      "        [[ 0.9197,  0.9126, -0.9388,  0.9146,  0.8949],\n",
      "         [ 0.8991,  0.9263, -0.9251,  0.9240,  0.8996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0897 cost = 0.001075\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8934, -0.9522, -0.9608,  0.8083, -0.0280],\n",
      "         [ 0.9120, -0.8584, -0.9311,  0.8815, -0.1715]],\n",
      "\n",
      "        [[-0.7126, -0.4970,  0.7237, -0.1658, -0.8807],\n",
      "         [-0.7581, -0.5071,  0.7723, -0.1864, -0.9124]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0897 cost = 0.000724\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.6241,  0.1348, -0.3293,  0.6077],\n",
      "         [-0.4528,  0.6232,  0.0293, -0.3850,  0.6516]],\n",
      "\n",
      "        [[ 0.9197,  0.9126, -0.9388,  0.9146,  0.8949],\n",
      "         [ 0.8991,  0.9263, -0.9251,  0.9240,  0.8996]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0898 cost = 0.001073\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9588, -0.6822, -0.7412,  0.9420, -0.7023],\n",
      "         [ 0.9120, -0.8585, -0.9312,  0.8816, -0.1716]],\n",
      "\n",
      "        [[-0.4470, -0.5221,  0.5684, -0.1709, -0.7216],\n",
      "         [-0.7582, -0.5072,  0.7724, -0.1865, -0.9124]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0898 cost = 0.001636\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5197,  0.4917,  0.0999, -0.5099,  0.6968],\n",
      "         [ 0.8935, -0.9522, -0.9608,  0.8085, -0.0282]],\n",
      "\n",
      "        [[ 0.8691,  0.8631, -0.8909,  0.8477,  0.8865],\n",
      "         [-0.7127, -0.4971,  0.7238, -0.1659, -0.8807]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0898 cost = 0.000925\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9121, -0.8585, -0.9312,  0.8816, -0.1718],\n",
      "         [-0.3784,  0.6241,  0.1348, -0.3292,  0.6077]],\n",
      "\n",
      "        [[-0.7582, -0.5073,  0.7724, -0.1866, -0.9124],\n",
      "         [ 0.9197,  0.9126, -0.9388,  0.9146,  0.8949]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0899 cost = 0.000462\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4529,  0.6234,  0.0293, -0.3849,  0.6517],\n",
      "         [-0.5198,  0.4918,  0.0999, -0.5099,  0.6968]],\n",
      "\n",
      "        [[ 0.8991,  0.9263, -0.9251,  0.9241,  0.8997],\n",
      "         [ 0.8691,  0.8631, -0.8909,  0.8477,  0.8865]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0899 cost = 0.003071\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8936, -0.9522, -0.9608,  0.8086, -0.0283],\n",
      "         [ 0.9588, -0.6825, -0.7414,  0.9421, -0.7024]],\n",
      "\n",
      "        [[-0.7127, -0.4973,  0.7238, -0.1661, -0.8807],\n",
      "         [-0.4471, -0.5223,  0.5685, -0.1711, -0.7217]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0899 cost = 0.001545\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5199,  0.4918,  0.0999, -0.5099,  0.6969],\n",
      "         [ 0.9588, -0.6825, -0.7415,  0.9421, -0.7024]],\n",
      "\n",
      "        [[ 0.8691,  0.8632, -0.8909,  0.8478,  0.8866],\n",
      "         [-0.4471, -0.5224,  0.5686, -0.1711, -0.7217]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0900 cost = 0.001591\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4531,  0.6236,  0.0294, -0.3848,  0.6518],\n",
      "         [-0.3786,  0.6242,  0.1348, -0.3291,  0.6077]],\n",
      "\n",
      "        [[ 0.8992,  0.9264, -0.9251,  0.9241,  0.8997],\n",
      "         [ 0.9198,  0.9127, -0.9388,  0.9147,  0.8950]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0900 cost = 0.000680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9121, -0.8585, -0.9313,  0.8817, -0.1721],\n",
      "         [ 0.8936, -0.9523, -0.9608,  0.8087, -0.0285]],\n",
      "\n",
      "        [[-0.7584, -0.5075,  0.7726, -0.1869, -0.9125],\n",
      "         [-0.7128, -0.4974,  0.7239, -0.1662, -0.8807]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0900 cost = 0.001266\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9588, -0.6826, -0.7416,  0.9421, -0.7024],\n",
      "         [-0.4532,  0.6237,  0.0294, -0.3847,  0.6518]],\n",
      "\n",
      "        [[-0.4472, -0.5226,  0.5687, -0.1712, -0.7217],\n",
      "         [ 0.8992,  0.9264, -0.9251,  0.9241,  0.8998]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0901 cost = 0.001645\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8937, -0.9523, -0.9608,  0.8089, -0.0286],\n",
      "         [ 0.9122, -0.8585, -0.9313,  0.8818, -0.1723]],\n",
      "\n",
      "        [[-0.7128, -0.4975,  0.7240, -0.1663, -0.8808],\n",
      "         [-0.7584, -0.5077,  0.7726, -0.1871, -0.9125]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0901 cost = 0.000717\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3788,  0.6243,  0.1348, -0.3291,  0.6077],\n",
      "         [-0.5201,  0.4919,  0.0999, -0.5099,  0.6970]],\n",
      "\n",
      "        [[ 0.9198,  0.9127, -0.9388,  0.9147,  0.8951],\n",
      "         [ 0.8692,  0.8632, -0.8909,  0.8478,  0.8867]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0901 cost = 0.003028\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5202,  0.4920,  0.0999, -0.5098,  0.6970],\n",
      "         [-0.3789,  0.6243,  0.1348, -0.3291,  0.6077]],\n",
      "\n",
      "        [[ 0.8692,  0.8632, -0.8909,  0.8478,  0.8867],\n",
      "         [ 0.9198,  0.9127, -0.9388,  0.9147,  0.8951]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0902 cost = 0.000934\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9588, -0.6827, -0.7418,  0.9421, -0.7025],\n",
      "         [-0.4532,  0.6240,  0.0295, -0.3845,  0.6519]],\n",
      "\n",
      "        [[-0.4474, -0.5228,  0.5688, -0.1714, -0.7218],\n",
      "         [ 0.8992,  0.9264, -0.9251,  0.9241,  0.8998]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0902 cost = 0.001639\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9122, -0.8585, -0.9314,  0.8818, -0.1726],\n",
      "         [ 0.8938, -0.9523, -0.9609,  0.8090, -0.0288]],\n",
      "\n",
      "        [[-0.7585, -0.5079,  0.7727, -0.1873, -0.9125],\n",
      "         [-0.7129, -0.4977,  0.7241, -0.1665, -0.8808]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0902 cost = 0.001259\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5201,  0.4923,  0.0999, -0.5096,  0.6972],\n",
      "         [-0.4532,  0.6242,  0.0295, -0.3844,  0.6520]],\n",
      "\n",
      "        [[ 0.8693,  0.8633, -0.8910,  0.8479,  0.8868],\n",
      "         [ 0.8992,  0.9264, -0.9251,  0.9242,  0.8999]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0903 cost = 0.001449\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9588, -0.6828, -0.7419,  0.9421, -0.7026],\n",
      "         [ 0.9123, -0.8586, -0.9314,  0.8818, -0.1727]],\n",
      "\n",
      "        [[-0.4475, -0.5230,  0.5689, -0.1715, -0.7218],\n",
      "         [-0.7586, -0.5080,  0.7728, -0.1874, -0.9125]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0903 cost = 0.001616\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8939, -0.9523, -0.9609,  0.8092, -0.0290],\n",
      "         [-0.3785,  0.6247,  0.1348, -0.3287,  0.6079]],\n",
      "\n",
      "        [[-0.7129, -0.4978,  0.7241, -0.1666, -0.8808],\n",
      "         [ 0.9198,  0.9127, -0.9388,  0.9148,  0.8953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0903 cost = 0.000445\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4531,  0.6246,  0.0296, -0.3841,  0.6521],\n",
      "         [ 0.9123, -0.8586, -0.9314,  0.8819, -0.1728]],\n",
      "\n",
      "        [[ 0.8993,  0.9264, -0.9251,  0.9242,  0.8999],\n",
      "         [-0.7586, -0.5082,  0.7729, -0.1875, -0.9126]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0904 cost = 0.000759\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5199,  0.4927,  0.0998, -0.5092,  0.6974],\n",
      "         [ 0.9589, -0.6830, -0.7421,  0.9421, -0.7026]],\n",
      "\n",
      "        [[ 0.8694,  0.8634, -0.8910,  0.8480,  0.8869],\n",
      "         [-0.4476, -0.5232,  0.5690, -0.1716, -0.7219]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0904 cost = 0.001572\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8940, -0.9524, -0.9609,  0.8094, -0.0291],\n",
      "         [-0.3783,  0.6249,  0.1348, -0.3285,  0.6080]],\n",
      "\n",
      "        [[-0.7130, -0.4980,  0.7242, -0.1668, -0.8808],\n",
      "         [ 0.9199,  0.9127, -0.9388,  0.9148,  0.8953]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0904 cost = 0.000444\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.6249,  0.1348, -0.3284,  0.6080],\n",
      "         [ 0.9589, -0.6831, -0.7422,  0.9421, -0.7027]],\n",
      "\n",
      "        [[ 0.9199,  0.9127, -0.9388,  0.9148,  0.8953],\n",
      "         [-0.4477, -0.5233,  0.5690, -0.1717, -0.7219]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0905 cost = 0.001264\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8940, -0.9524, -0.9609,  0.8095, -0.0292],\n",
      "         [-0.5199,  0.4929,  0.0998, -0.5091,  0.6975]],\n",
      "\n",
      "        [[-0.7130, -0.4981,  0.7242, -0.1669, -0.8809],\n",
      "         [ 0.8694,  0.8634, -0.8910,  0.8480,  0.8869]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0905 cost = 0.000928\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9124, -0.8586, -0.9315,  0.8820, -0.1732],\n",
      "         [-0.4531,  0.6250,  0.0296, -0.3837,  0.6522]],\n",
      "\n",
      "        [[-0.7587, -0.5085,  0.7730, -0.1878, -0.9126],\n",
      "         [ 0.8993,  0.9264, -0.9251,  0.9242,  0.9000]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0905 cost = 0.000709\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9589, -0.6833, -0.7423,  0.9421, -0.7028],\n",
      "         [-0.4531,  0.6250,  0.0297, -0.3837,  0.6522]],\n",
      "\n",
      "        [[-0.4478, -0.5235,  0.5691, -0.1718, -0.7220],\n",
      "         [ 0.8993,  0.9264, -0.9251,  0.9242,  0.9000]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0906 cost = 0.001624\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9124, -0.8587, -0.9315,  0.8820, -0.1734],\n",
      "         [-0.5199,  0.4930,  0.0998, -0.5090,  0.6976]],\n",
      "\n",
      "        [[-0.7588, -0.5085,  0.7731, -0.1880, -0.9126],\n",
      "         [ 0.8695,  0.8635, -0.8911,  0.8480,  0.8870]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0906 cost = 0.000891\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8941, -0.9524, -0.9610,  0.8097, -0.0295],\n",
      "         [-0.3782,  0.6251,  0.1349, -0.3282,  0.6080]],\n",
      "\n",
      "        [[-0.7131, -0.4982,  0.7244, -0.1671, -0.8809],\n",
      "         [ 0.9199,  0.9128, -0.9388,  0.9148,  0.8954]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0906 cost = 0.000441\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3782,  0.6251,  0.1349, -0.3282,  0.6081],\n",
      "         [ 0.9125, -0.8587, -0.9315,  0.8821, -0.1736]],\n",
      "\n",
      "        [[ 0.9199,  0.9128, -0.9388,  0.9148,  0.8954],\n",
      "         [-0.7589, -0.5086,  0.7731, -0.1881, -0.9126]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0907 cost = 0.000598\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9589, -0.6835, -0.7425,  0.9422, -0.7029],\n",
      "         [-0.4532,  0.6252,  0.0298, -0.3835,  0.6522]],\n",
      "\n",
      "        [[-0.4479, -0.5236,  0.5692, -0.1720, -0.7221],\n",
      "         [ 0.8993,  0.9265, -0.9251,  0.9242,  0.9000]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0907 cost = 0.001618\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5199,  0.4930,  0.0999, -0.5089,  0.6977],\n",
      "         [ 0.8942, -0.9524, -0.9610,  0.8098, -0.0296]],\n",
      "\n",
      "        [[ 0.8695,  0.8635, -0.8911,  0.8480,  0.8870],\n",
      "         [-0.7132, -0.4982,  0.7244, -0.1673, -0.8809]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0907 cost = 0.000900\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9125, -0.8588, -0.9316,  0.8821, -0.1739],\n",
      "         [-0.5200,  0.4930,  0.0999, -0.5089,  0.6977]],\n",
      "\n",
      "        [[-0.7589, -0.5087,  0.7732, -0.1883, -0.9126],\n",
      "         [ 0.8695,  0.8635, -0.8911,  0.8480,  0.8870]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0908 cost = 0.000888\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3783,  0.6251,  0.1349, -0.3281,  0.6081],\n",
      "         [ 0.9589, -0.6838, -0.7426,  0.9422, -0.7030]],\n",
      "\n",
      "        [[ 0.9199,  0.9128, -0.9388,  0.9148,  0.8954],\n",
      "         [-0.4480, -0.5237,  0.5693, -0.1722, -0.7221]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0908 cost = 0.001252\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4533,  0.6252,  0.0300, -0.3833,  0.6522],\n",
      "         [ 0.8943, -0.9525, -0.9610,  0.8100, -0.0298]],\n",
      "\n",
      "        [[ 0.8993,  0.9265, -0.9251,  0.9243,  0.9001],\n",
      "         [-0.7133, -0.4983,  0.7245, -0.1674, -0.8809]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0908 cost = 0.000771\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4533,  0.6253,  0.0300, -0.3833,  0.6522],\n",
      "         [ 0.8943, -0.9525, -0.9610,  0.8101, -0.0298]],\n",
      "\n",
      "        [[ 0.8993,  0.9265, -0.9251,  0.9243,  0.9001],\n",
      "         [-0.7133, -0.4983,  0.7245, -0.1675, -0.8809]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0909 cost = 0.000770\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9126, -0.8589, -0.9316,  0.8822, -0.1742],\n",
      "         [-0.5201,  0.4929,  0.0999, -0.5089,  0.6977]],\n",
      "\n",
      "        [[-0.7591, -0.5088,  0.7733, -0.1887, -0.9127],\n",
      "         [ 0.8695,  0.8635, -0.8911,  0.8480,  0.8870]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0909 cost = 0.000885\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9590, -0.6840, -0.7427,  0.9422, -0.7031],\n",
      "         [-0.3784,  0.6250,  0.1350, -0.3280,  0.6080]],\n",
      "\n",
      "        [[-0.4481, -0.5239,  0.5694, -0.1724, -0.7222],\n",
      "         [ 0.9199,  0.9128, -0.9388,  0.9148,  0.8955]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0909 cost = 0.001290\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3785,  0.6250,  0.1350, -0.3280,  0.6080],\n",
      "         [-0.4534,  0.6253,  0.0302, -0.3833,  0.6522]],\n",
      "\n",
      "        [[ 0.9199,  0.9128, -0.9388,  0.9148,  0.8955],\n",
      "         [ 0.8993,  0.9265, -0.9251,  0.9243,  0.9001]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0910 cost = 0.001041\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9590, -0.6842, -0.7428,  0.9423, -0.7032],\n",
      "         [ 0.9127, -0.8590, -0.9316,  0.8823, -0.1745]],\n",
      "\n",
      "        [[-0.4481, -0.5239,  0.5694, -0.1725, -0.7222],\n",
      "         [-0.7591, -0.5088,  0.7734, -0.1889, -0.9127]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0910 cost = 0.001587\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5203,  0.4929,  0.0999, -0.5089,  0.6978],\n",
      "         [ 0.8945, -0.9525, -0.9611,  0.8104, -0.0301]],\n",
      "\n",
      "        [[ 0.8695,  0.8636, -0.8910,  0.8480,  0.8871],\n",
      "         [-0.7134, -0.4984,  0.7247, -0.1678, -0.8809]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0910 cost = 0.000893\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9590, -0.6844, -0.7429,  0.9423, -0.7032],\n",
      "         [ 0.8945, -0.9525, -0.9611,  0.8105, -0.0302]],\n",
      "\n",
      "        [[-0.4482, -0.5240,  0.5694, -0.1726, -0.7222],\n",
      "         [-0.7134, -0.4984,  0.7247, -0.1679, -0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0911 cost = 0.001914\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4536,  0.6254,  0.0303, -0.3832,  0.6522],\n",
      "         [-0.5203,  0.4929,  0.1000, -0.5090,  0.6978]],\n",
      "\n",
      "        [[ 0.8993,  0.9265, -0.9251,  0.9243,  0.9001],\n",
      "         [ 0.8695,  0.8636, -0.8910,  0.8479,  0.8871]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0911 cost = 0.003010\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9128, -0.8591, -0.9317,  0.8824, -0.1749],\n",
      "         [-0.3787,  0.6250,  0.1350, -0.3280,  0.6080]],\n",
      "\n",
      "        [[-0.7593, -0.5090,  0.7735, -0.1892, -0.9127],\n",
      "         [ 0.9199,  0.9128, -0.9388,  0.9148,  0.8955]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0911 cost = 0.000446\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9590, -0.6847, -0.7431,  0.9423, -0.7033],\n",
      "         [-0.5205,  0.4929,  0.1000, -0.5090,  0.6979]],\n",
      "\n",
      "        [[-0.4482, -0.5242,  0.5695, -0.1728, -0.7223],\n",
      "         [ 0.8696,  0.8636, -0.8911,  0.8480,  0.8871]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0912 cost = 0.002025\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8946, -0.9526, -0.9611,  0.8107, -0.0305],\n",
      "         [-0.3788,  0.6250,  0.1350, -0.3280,  0.6080]],\n",
      "\n",
      "        [[-0.7135, -0.4985,  0.7248, -0.1681, -0.8810],\n",
      "         [ 0.9199,  0.9128, -0.9388,  0.9148,  0.8956]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0912 cost = 0.000434\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9128, -0.8591, -0.9318,  0.8825, -0.1752],\n",
      "         [-0.4538,  0.6256,  0.0304, -0.3831,  0.6523]],\n",
      "\n",
      "        [[-0.7593, -0.5090,  0.7736, -0.1894, -0.9127],\n",
      "         [ 0.8994,  0.9265, -0.9251,  0.9243,  0.9002]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0912 cost = 0.000696\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9128, -0.8592, -0.9318,  0.8825, -0.1753],\n",
      "         [ 0.8947, -0.9526, -0.9611,  0.8108, -0.0306]],\n",
      "\n",
      "        [[-0.7594, -0.5090,  0.7736, -0.1895, -0.9127],\n",
      "         [-0.7135, -0.4985,  0.7248, -0.1682, -0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0913 cost = 0.001223\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3790,  0.6250,  0.1350, -0.3280,  0.6080],\n",
      "         [-0.4538,  0.6257,  0.0305, -0.3831,  0.6523]],\n",
      "\n",
      "        [[ 0.9200,  0.9128, -0.9388,  0.9148,  0.8956],\n",
      "         [ 0.8994,  0.9266, -0.9251,  0.9243,  0.9002]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0913 cost = 0.001033\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9591, -0.6851, -0.7433,  0.9424, -0.7035],\n",
      "         [-0.5207,  0.4930,  0.1000, -0.5090,  0.6980]],\n",
      "\n",
      "        [[-0.4484, -0.5243,  0.5696, -0.1730, -0.7223],\n",
      "         [ 0.8696,  0.8637, -0.8911,  0.8480,  0.8872]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0913 cost = 0.002016\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5207,  0.4930,  0.1000, -0.5090,  0.6980],\n",
      "         [ 0.8948, -0.9526, -0.9612,  0.8110, -0.0308]],\n",
      "\n",
      "        [[ 0.8697,  0.8637, -0.8911,  0.8480,  0.8872],\n",
      "         [-0.7136, -0.4986,  0.7249, -0.1684, -0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0914 cost = 0.000884\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4539,  0.6258,  0.0305, -0.3831,  0.6523],\n",
      "         [ 0.9591, -0.6852, -0.7434,  0.9424, -0.7035]],\n",
      "\n",
      "        [[ 0.8994,  0.9266, -0.9251,  0.9243,  0.9002],\n",
      "         [-0.4485, -0.5244,  0.5697, -0.1731, -0.7223]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0914 cost = 0.001384\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9129, -0.8593, -0.9318,  0.8826, -0.1757],\n",
      "         [-0.3791,  0.6250,  0.1350, -0.3279,  0.6080]],\n",
      "\n",
      "        [[-0.7595, -0.5092,  0.7738, -0.1899, -0.9128],\n",
      "         [ 0.9200,  0.9128, -0.9388,  0.9148,  0.8957]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0914 cost = 0.000443\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4540,  0.6258,  0.0306, -0.3830,  0.6523],\n",
      "         [ 0.8949, -0.9527, -0.9612,  0.8112, -0.0310]],\n",
      "\n",
      "        [[ 0.8994,  0.9266, -0.9251,  0.9243,  0.9002],\n",
      "         [-0.7137, -0.4986,  0.7250, -0.1686, -0.8810]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0915 cost = 0.000756\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3792,  0.6250,  0.1350, -0.3279,  0.6080],\n",
      "         [ 0.9130, -0.8593, -0.9319,  0.8826, -0.1759]],\n",
      "\n",
      "        [[ 0.9200,  0.9129, -0.9388,  0.9148,  0.8957],\n",
      "         [-0.7596, -0.5093,  0.7739, -0.1900, -0.9128]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0915 cost = 0.000585\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5210,  0.4930,  0.1000, -0.5091,  0.6982],\n",
      "         [ 0.9591, -0.6856, -0.7435,  0.9424, -0.7036]],\n",
      "\n",
      "        [[ 0.8697,  0.8638, -0.8911,  0.8480,  0.8873],\n",
      "         [-0.4486, -0.5245,  0.5698, -0.1733, -0.7224]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0915 cost = 0.001526\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3794,  0.6250,  0.1351, -0.3279,  0.6080],\n",
      "         [-0.4541,  0.6259,  0.0307, -0.3830,  0.6523]],\n",
      "\n",
      "        [[ 0.9200,  0.9129, -0.9388,  0.9148,  0.8957],\n",
      "         [ 0.8994,  0.9266, -0.9251,  0.9243,  0.9003]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0916 cost = 0.001026\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9130, -0.8594, -0.9319,  0.8827, -0.1762],\n",
      "         [-0.5211,  0.4930,  0.1000, -0.5092,  0.6982]],\n",
      "\n",
      "        [[-0.7596, -0.5094,  0.7740, -0.1902, -0.9128],\n",
      "         [ 0.8697,  0.8638, -0.8911,  0.8480,  0.8873]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0916 cost = 0.000869\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8950, -0.9527, -0.9613,  0.8116, -0.0313],\n",
      "         [ 0.9591, -0.6857, -0.7437,  0.9424, -0.7037]],\n",
      "\n",
      "        [[-0.7138, -0.4988,  0.7251, -0.1688, -0.8811],\n",
      "         [-0.4487, -0.5247,  0.5699, -0.1735, -0.7224]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0916 cost = 0.001480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.6250,  0.1351, -0.3279,  0.6079],\n",
      "         [ 0.9131, -0.8595, -0.9319,  0.8827, -0.1763]],\n",
      "\n",
      "        [[ 0.9200,  0.9129, -0.9388,  0.9148,  0.8958],\n",
      "         [-0.7597, -0.5095,  0.7740, -0.1903, -0.9128]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0917 cost = 0.000582\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9591, -0.6858, -0.7437,  0.9424, -0.7037],\n",
      "         [ 0.8951, -0.9527, -0.9613,  0.8117, -0.0314]],\n",
      "\n",
      "        [[-0.4487, -0.5248,  0.5699, -0.1736, -0.7225],\n",
      "         [-0.7138, -0.4988,  0.7252, -0.1690, -0.8811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0917 cost = 0.001881\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5213,  0.4929,  0.1001, -0.5092,  0.6982],\n",
      "         [-0.4543,  0.6261,  0.0309, -0.3829,  0.6523]],\n",
      "\n",
      "        [[ 0.8698,  0.8638, -0.8911,  0.8480,  0.8873],\n",
      "         [ 0.8994,  0.9266, -0.9251,  0.9244,  0.9003]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0917 cost = 0.001399\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9592, -0.6860, -0.7439,  0.9425, -0.7038],\n",
      "         [ 0.9132, -0.8595, -0.9320,  0.8828, -0.1766]],\n",
      "\n",
      "        [[-0.4488, -0.5249,  0.5700, -0.1737, -0.7225],\n",
      "         [-0.7598, -0.5096,  0.7741, -0.1906, -0.9128]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0918 cost = 0.001554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3797,  0.6250,  0.1351, -0.3278,  0.6079],\n",
      "         [ 0.8952, -0.9528, -0.9613,  0.8119, -0.0316]],\n",
      "\n",
      "        [[ 0.9200,  0.9129, -0.9388,  0.9149,  0.8958],\n",
      "         [-0.7139, -0.4990,  0.7253, -0.1691, -0.8811]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0918 cost = 0.000605\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5214,  0.4930,  0.1001, -0.5092,  0.6983],\n",
      "         [-0.4543,  0.6263,  0.0309, -0.3829,  0.6524]],\n",
      "\n",
      "        [[ 0.8698,  0.8638, -0.8911,  0.8480,  0.8873],\n",
      "         [ 0.8994,  0.9266, -0.9251,  0.9244,  0.9003]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0918 cost = 0.001396\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4543,  0.6263,  0.0309, -0.3828,  0.6524],\n",
      "         [-0.3797,  0.6250,  0.1351, -0.3277,  0.6080]],\n",
      "\n",
      "        [[ 0.8994,  0.9266, -0.9251,  0.9244,  0.9003],\n",
      "         [ 0.9200,  0.9129, -0.9388,  0.9149,  0.8958]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0919 cost = 0.000646\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8953, -0.9528, -0.9613,  0.8121, -0.0318],\n",
      "         [-0.5214,  0.4931,  0.1001, -0.5091,  0.6983]],\n",
      "\n",
      "        [[-0.7139, -0.4991,  0.7253, -0.1693, -0.8811],\n",
      "         [ 0.8698,  0.8638, -0.8911,  0.8480,  0.8874]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0919 cost = 0.000897\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9133, -0.8596, -0.9321,  0.8829, -0.1770],\n",
      "         [ 0.9592, -0.6864, -0.7441,  0.9425, -0.7040]],\n",
      "\n",
      "        [[-0.7599, -0.5099,  0.7742, -0.1909, -0.9129],\n",
      "         [-0.4489, -0.5251,  0.5701, -0.1739, -0.7226]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0919 cost = 0.001558\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3797,  0.6251,  0.1351, -0.3275,  0.6081],\n",
      "         [ 0.9592, -0.6864, -0.7442,  0.9425, -0.7040]],\n",
      "\n",
      "        [[ 0.9201,  0.9129, -0.9388,  0.9149,  0.8959],\n",
      "         [-0.4490, -0.5252,  0.5701, -0.1739, -0.7226]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0920 cost = 0.001212\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8954, -0.9528, -0.9614,  0.8123, -0.0320],\n",
      "         [ 0.9133, -0.8596, -0.9321,  0.8830, -0.1772]],\n",
      "\n",
      "        [[-0.7140, -0.4992,  0.7254, -0.1695, -0.8812],\n",
      "         [-0.7600, -0.5100,  0.7743, -0.1911, -0.9129]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0920 cost = 0.000680\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5214,  0.4933,  0.1001, -0.5090,  0.6985],\n",
      "         [-0.4543,  0.6267,  0.0310, -0.3825,  0.6526]],\n",
      "\n",
      "        [[ 0.8698,  0.8639, -0.8911,  0.8480,  0.8874],\n",
      "         [ 0.8995,  0.9267, -0.9250,  0.9244,  0.9004]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0920 cost = 0.001388\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3797,  0.6252,  0.1351, -0.3274,  0.6081],\n",
      "         [ 0.8955, -0.9528, -0.9614,  0.8125, -0.0321]],\n",
      "\n",
      "        [[ 0.9201,  0.9129, -0.9388,  0.9149,  0.8959],\n",
      "         [-0.7141, -0.4993,  0.7255, -0.1696, -0.8812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0921 cost = 0.000600\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9134, -0.8597, -0.9321,  0.8830, -0.1775],\n",
      "         [-0.4543,  0.6269,  0.0310, -0.3824,  0.6526]],\n",
      "\n",
      "        [[-0.7601, -0.5102,  0.7744, -0.1913, -0.9129],\n",
      "         [ 0.8995,  0.9267, -0.9250,  0.9244,  0.9004]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0921 cost = 0.000679\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9592, -0.6867, -0.7444,  0.9426, -0.7041],\n",
      "         [-0.5213,  0.4934,  0.1001, -0.5088,  0.6986]],\n",
      "\n",
      "        [[-0.4492, -0.5255,  0.5703, -0.1741, -0.7227],\n",
      "         [ 0.8698,  0.8639, -0.8911,  0.8480,  0.8874]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0921 cost = 0.001979\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8956, -0.9529, -0.9614,  0.8126, -0.0322],\n",
      "         [ 0.9592, -0.6868, -0.7444,  0.9426, -0.7041]],\n",
      "\n",
      "        [[-0.7141, -0.4994,  0.7256, -0.1698, -0.8812],\n",
      "         [-0.4492, -0.5255,  0.5703, -0.1742, -0.7227]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0922 cost = 0.001458\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4543,  0.6271,  0.0311, -0.3822,  0.6527],\n",
      "         [ 0.9134, -0.8597, -0.9322,  0.8831, -0.1777]],\n",
      "\n",
      "        [[ 0.8995,  0.9267, -0.9250,  0.9244,  0.9004],\n",
      "         [-0.7602, -0.5103,  0.7745, -0.1915, -0.9129]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0922 cost = 0.000720\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.6254,  0.1351, -0.3271,  0.6082],\n",
      "         [-0.5213,  0.4935,  0.1001, -0.5087,  0.6986]],\n",
      "\n",
      "        [[ 0.9201,  0.9129, -0.9388,  0.9149,  0.8959],\n",
      "         [ 0.8699,  0.8639, -0.8911,  0.8480,  0.8875]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0922 cost = 0.002940\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4543,  0.6272,  0.0311, -0.3821,  0.6527],\n",
      "         [ 0.9135, -0.8597, -0.9322,  0.8831, -0.1779]],\n",
      "\n",
      "        [[ 0.8995,  0.9267, -0.9250,  0.9245,  0.9004],\n",
      "         [-0.7602, -0.5104,  0.7746, -0.1917, -0.9130]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0923 cost = 0.000719\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9593, -0.6870, -0.7446,  0.9426, -0.7042],\n",
      "         [-0.3796,  0.6255,  0.1351, -0.3270,  0.6083]],\n",
      "\n",
      "        [[-0.4494, -0.5257,  0.5704, -0.1743, -0.7228],\n",
      "         [ 0.9201,  0.9129, -0.9388,  0.9149,  0.8960]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0923 cost = 0.001243\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8957, -0.9529, -0.9614,  0.8129, -0.0325],\n",
      "         [-0.5214,  0.4937,  0.1001, -0.5086,  0.6988]],\n",
      "\n",
      "        [[-0.7143, -0.4996,  0.7257, -0.1701, -0.8812],\n",
      "         [ 0.8699,  0.8640, -0.8912,  0.8481,  0.8875]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0923 cost = 0.000887\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9593, -0.6872, -0.7447,  0.9426, -0.7043],\n",
      "         [ 0.9135, -0.8598, -0.9322,  0.8832, -0.1781]],\n",
      "\n",
      "        [[-0.4494, -0.5258,  0.5705, -0.1744, -0.7228],\n",
      "         [-0.7603, -0.5105,  0.7746, -0.1919, -0.9130]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0924 cost = 0.001530\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3796,  0.6255,  0.1351, -0.3269,  0.6083],\n",
      "         [ 0.8957, -0.9529, -0.9615,  0.8130, -0.0326]],\n",
      "\n",
      "        [[ 0.9201,  0.9129, -0.9388,  0.9149,  0.8961],\n",
      "         [-0.7143, -0.4996,  0.7258, -0.1702, -0.8812]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0924 cost = 0.000595\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4544,  0.6275,  0.0312, -0.3818,  0.6529],\n",
      "         [-0.5214,  0.4938,  0.1001, -0.5085,  0.6989]],\n",
      "\n",
      "        [[ 0.8995,  0.9267, -0.9250,  0.9245,  0.9005],\n",
      "         [ 0.8700,  0.8640, -0.8912,  0.8481,  0.8876]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0924 cost = 0.002949\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4545,  0.6276,  0.0312, -0.3818,  0.6529],\n",
      "         [ 0.9136, -0.8598, -0.9323,  0.8832, -0.1784]],\n",
      "\n",
      "        [[ 0.8995,  0.9267, -0.9250,  0.9245,  0.9005],\n",
      "         [-0.7604, -0.5106,  0.7747, -0.1921, -0.9130]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0925 cost = 0.000714\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9593, -0.6874, -0.7448,  0.9426, -0.7044],\n",
      "         [-0.5215,  0.4939,  0.1001, -0.5085,  0.6990]],\n",
      "\n",
      "        [[-0.4496, -0.5260,  0.5707, -0.1746, -0.7229],\n",
      "         [ 0.8700,  0.8641, -0.8912,  0.8482,  0.8877]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0925 cost = 0.001962\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8958, -0.9529, -0.9615,  0.8131, -0.0328],\n",
      "         [-0.3798,  0.6256,  0.1351, -0.3269,  0.6084]],\n",
      "\n",
      "        [[-0.7144, -0.4996,  0.7259, -0.1704, -0.8813],\n",
      "         [ 0.9202,  0.9130, -0.9388,  0.9150,  0.8962]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0925 cost = 0.000418\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5216,  0.4939,  0.1001, -0.5085,  0.6992],\n",
      "         [ 0.8958, -0.9529, -0.9615,  0.8131, -0.0328]],\n",
      "\n",
      "        [[ 0.8701,  0.8642, -0.8912,  0.8482,  0.8877],\n",
      "         [-0.7144, -0.4996,  0.7259, -0.1705, -0.8813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0926 cost = 0.000854\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4547,  0.6278,  0.0313, -0.3817,  0.6530],\n",
      "         [ 0.9593, -0.6875, -0.7449,  0.9426, -0.7044]],\n",
      "\n",
      "        [[ 0.8996,  0.9268, -0.9250,  0.9245,  0.9006],\n",
      "         [-0.4497, -0.5260,  0.5708, -0.1747, -0.7229]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0926 cost = 0.001338\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3799,  0.6257,  0.1351, -0.3269,  0.6084],\n",
      "         [ 0.9136, -0.8599, -0.9323,  0.8833, -0.1787]],\n",
      "\n",
      "        [[ 0.9202,  0.9130, -0.9388,  0.9150,  0.8962],\n",
      "         [-0.7605, -0.5107,  0.7749, -0.1925, -0.9130]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0926 cost = 0.000567\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4548,  0.6279,  0.0314, -0.3816,  0.6530],\n",
      "         [ 0.8959, -0.9529, -0.9615,  0.8132, -0.0330]],\n",
      "\n",
      "        [[ 0.8996,  0.9268, -0.9250,  0.9245,  0.9006],\n",
      "         [-0.7145, -0.4997,  0.7260, -0.1706, -0.8813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0927 cost = 0.000729\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9593, -0.6877, -0.7450,  0.9426, -0.7045],\n",
      "         [ 0.9136, -0.8599, -0.9323,  0.8833, -0.1789]],\n",
      "\n",
      "        [[-0.4498, -0.5262,  0.5709, -0.1748, -0.7229],\n",
      "         [-0.7606, -0.5107,  0.7749, -0.1926, -0.9131]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0927 cost = 0.001519\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3801,  0.6257,  0.1352, -0.3269,  0.6084],\n",
      "         [-0.5220,  0.4940,  0.1001, -0.5085,  0.6993]],\n",
      "\n",
      "        [[ 0.9202,  0.9130, -0.9388,  0.9150,  0.8963],\n",
      "         [ 0.8702,  0.8643, -0.8913,  0.8483,  0.8878]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0927 cost = 0.002907\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5220,  0.4941,  0.1001, -0.5085,  0.6994],\n",
      "         [ 0.9137, -0.8599, -0.9324,  0.8834, -0.1790]],\n",
      "\n",
      "        [[ 0.8702,  0.8643, -0.8913,  0.8483,  0.8879],\n",
      "         [-0.7606, -0.5108,  0.7750, -0.1927, -0.9131]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0928 cost = 0.000797\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8960, -0.9530, -0.9615,  0.8134, -0.0332],\n",
      "         [ 0.9593, -0.6879, -0.7452,  0.9426, -0.7046]],\n",
      "\n",
      "        [[-0.7146, -0.4997,  0.7261, -0.1708, -0.8813],\n",
      "         [-0.4499, -0.5263,  0.5710, -0.1749, -0.7230]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0928 cost = 0.001438\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3803,  0.6257,  0.1352, -0.3268,  0.6085],\n",
      "         [-0.4550,  0.6281,  0.0315, -0.3815,  0.6531]],\n",
      "\n",
      "        [[ 0.9202,  0.9131, -0.9388,  0.9150,  0.8964],\n",
      "         [ 0.8996,  0.9268, -0.9251,  0.9246,  0.9007]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0928 cost = 0.000993\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4551,  0.6282,  0.0315, -0.3815,  0.6531],\n",
      "         [-0.3803,  0.6258,  0.1351, -0.3268,  0.6085]],\n",
      "\n",
      "        [[ 0.8996,  0.9268, -0.9251,  0.9246,  0.9007],\n",
      "         [ 0.9202,  0.9131, -0.9388,  0.9150,  0.8964]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0929 cost = 0.000626\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9137, -0.8599, -0.9324,  0.8834, -0.1793],\n",
      "         [-0.5222,  0.4943,  0.1001, -0.5085,  0.6996]],\n",
      "\n",
      "        [[-0.7607, -0.5110,  0.7751, -0.1930, -0.9131],\n",
      "         [ 0.8703,  0.8644, -0.8914,  0.8484,  0.8880]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0929 cost = 0.000839\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9593, -0.6880, -0.7453,  0.9426, -0.7046],\n",
      "         [ 0.8960, -0.9530, -0.9615,  0.8135, -0.0333]],\n",
      "\n",
      "        [[-0.4501, -0.5265,  0.5711, -0.1750, -0.7230],\n",
      "         [-0.7146, -0.4998,  0.7262, -0.1710, -0.8813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0929 cost = 0.001825\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5223,  0.4943,  0.1001, -0.5085,  0.6997],\n",
      "         [ 0.9593, -0.6881, -0.7453,  0.9426, -0.7046]],\n",
      "\n",
      "        [[ 0.8704,  0.8645, -0.8914,  0.8484,  0.8880],\n",
      "         [-0.4501, -0.5265,  0.5711, -0.1750, -0.7230]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0930 cost = 0.001471\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8961, -0.9530, -0.9616,  0.8136, -0.0334],\n",
      "         [-0.4552,  0.6285,  0.0316, -0.3814,  0.6532]],\n",
      "\n",
      "        [[-0.7147, -0.4999,  0.7263, -0.1711, -0.8813],\n",
      "         [ 0.8997,  0.9269, -0.9251,  0.9246,  0.9008]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0930 cost = 0.000660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3806,  0.6259,  0.1352, -0.3268,  0.6086],\n",
      "         [ 0.9138, -0.8600, -0.9325,  0.8835, -0.1796]],\n",
      "\n",
      "        [[ 0.9203,  0.9131, -0.9388,  0.9151,  0.8965],\n",
      "         [-0.7608, -0.5111,  0.7752, -0.1933, -0.9131]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0930 cost = 0.000561\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5224,  0.4944,  0.1001, -0.5085,  0.6998],\n",
      "         [ 0.8961, -0.9530, -0.9616,  0.8137, -0.0335]],\n",
      "\n",
      "        [[ 0.8704,  0.8645, -0.8914,  0.8484,  0.8881],\n",
      "         [-0.7147, -0.5000,  0.7263, -0.1713, -0.8813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0931 cost = 0.000841\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4554,  0.6286,  0.0317, -0.3813,  0.6533],\n",
      "         [-0.3807,  0.6259,  0.1352, -0.3267,  0.6086]],\n",
      "\n",
      "        [[ 0.8997,  0.9269, -0.9251,  0.9246,  0.9009],\n",
      "         [ 0.9203,  0.9131, -0.9389,  0.9151,  0.8965]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0931 cost = 0.000622\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9138, -0.8600, -0.9325,  0.8835, -0.1798],\n",
      "         [ 0.9594, -0.6883, -0.7455,  0.9427, -0.7048]],\n",
      "\n",
      "        [[-0.7609, -0.5113,  0.7753, -0.1935, -0.9131],\n",
      "         [-0.4502, -0.5268,  0.5713, -0.1753, -0.7231]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0931 cost = 0.001515\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.6259,  0.1352, -0.3267,  0.6086],\n",
      "         [ 0.8962, -0.9530, -0.9616,  0.8139, -0.0337]],\n",
      "\n",
      "        [[ 0.9203,  0.9131, -0.9389,  0.9151,  0.8965],\n",
      "         [-0.7148, -0.5001,  0.7264, -0.1714, -0.8813]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0932 cost = 0.000582\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5226,  0.4945,  0.1001, -0.5085,  0.6999],\n",
      "         [-0.4555,  0.6287,  0.0318, -0.3812,  0.6533]],\n",
      "\n",
      "        [[ 0.8704,  0.8645, -0.8914,  0.8485,  0.8881],\n",
      "         [ 0.8997,  0.9269, -0.9251,  0.9246,  0.9009]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0932 cost = 0.001341\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9139, -0.8600, -0.9325,  0.8836, -0.1800],\n",
      "         [ 0.9594, -0.6884, -0.7456,  0.9427, -0.7048]],\n",
      "\n",
      "        [[-0.7610, -0.5114,  0.7754, -0.1937, -0.9132],\n",
      "         [-0.4503, -0.5269,  0.5713, -0.1754, -0.7232]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0932 cost = 0.001511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4555,  0.6289,  0.0319, -0.3811,  0.6533],\n",
      "         [ 0.9594, -0.6885, -0.7457,  0.9427, -0.7049]],\n",
      "\n",
      "        [[ 0.8997,  0.9269, -0.9251,  0.9246,  0.9009],\n",
      "         [-0.4504, -0.5270,  0.5714, -0.1754, -0.7232]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0933 cost = 0.001315\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3809,  0.6260,  0.1352, -0.3266,  0.6087],\n",
      "         [-0.5227,  0.4946,  0.1001, -0.5084,  0.6999]],\n",
      "\n",
      "        [[ 0.9203,  0.9131, -0.9389,  0.9151,  0.8966],\n",
      "         [ 0.8705,  0.8646, -0.8914,  0.8485,  0.8882]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0933 cost = 0.002875\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9140, -0.8601, -0.9326,  0.8837, -0.1802],\n",
      "         [ 0.8963, -0.9531, -0.9616,  0.8142, -0.0339]],\n",
      "\n",
      "        [[-0.7611, -0.5116,  0.7755, -0.1939, -0.9132],\n",
      "         [-0.7149, -0.5003,  0.7265, -0.1717, -0.8814]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0933 cost = 0.001156\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4556,  0.6291,  0.0319, -0.3810,  0.6534],\n",
      "         [ 0.9594, -0.6886, -0.7458,  0.9427, -0.7049]],\n",
      "\n",
      "        [[ 0.8997,  0.9269, -0.9251,  0.9247,  0.9010],\n",
      "         [-0.4505, -0.5272,  0.5715, -0.1755, -0.7233]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0934 cost = 0.001311\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9140, -0.8601, -0.9326,  0.8837, -0.1804],\n",
      "         [ 0.8964, -0.9531, -0.9617,  0.8143, -0.0341]],\n",
      "\n",
      "        [[-0.7611, -0.5118,  0.7756, -0.1940, -0.9132],\n",
      "         [-0.7149, -0.5005,  0.7266, -0.1718, -0.8814]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0934 cost = 0.001153\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3809,  0.6262,  0.1352, -0.3265,  0.6087],\n",
      "         [-0.5227,  0.4948,  0.1001, -0.5083,  0.7001]],\n",
      "\n",
      "        [[ 0.9204,  0.9131, -0.9389,  0.9151,  0.8967],\n",
      "         [ 0.8706,  0.8646, -0.8915,  0.8486,  0.8883]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0934 cost = 0.002866\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9140, -0.8601, -0.9326,  0.8837, -0.1806],\n",
      "         [-0.3809,  0.6262,  0.1352, -0.3265,  0.6087]],\n",
      "\n",
      "        [[-0.7612, -0.5120,  0.7756, -0.1941, -0.9132],\n",
      "         [ 0.9204,  0.9131, -0.9389,  0.9151,  0.8967]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0935 cost = 0.000418\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8965, -0.9531, -0.9617,  0.8145, -0.0343],\n",
      "         [ 0.9594, -0.6887, -0.7460,  0.9427, -0.7050]],\n",
      "\n",
      "        [[-0.7150, -0.5007,  0.7267, -0.1720, -0.8814],\n",
      "         [-0.4507, -0.5276,  0.5717, -0.1756, -0.7234]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0935 cost = 0.001414\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4556,  0.6295,  0.0320, -0.3808,  0.6535],\n",
      "         [-0.5228,  0.4950,  0.1000, -0.5082,  0.7003]],\n",
      "\n",
      "        [[ 0.8998,  0.9269, -0.9251,  0.9247,  0.9010],\n",
      "         [ 0.8706,  0.8647, -0.8915,  0.8486,  0.8884]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0935 cost = 0.002876\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4556,  0.6296,  0.0320, -0.3808,  0.6536],\n",
      "         [-0.5228,  0.4951,  0.1000, -0.5082,  0.7004]],\n",
      "\n",
      "        [[ 0.8998,  0.9269, -0.9251,  0.9247,  0.9011],\n",
      "         [ 0.8707,  0.8648, -0.8916,  0.8487,  0.8884]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0936 cost = 0.002871\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9141, -0.8602, -0.9327,  0.8838, -0.1808],\n",
      "         [-0.3810,  0.6264,  0.1352, -0.3263,  0.6088]],\n",
      "\n",
      "        [[-0.7613, -0.5123,  0.7758, -0.1944, -0.9133],\n",
      "         [ 0.9204,  0.9132, -0.9389,  0.9152,  0.8969]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0936 cost = 0.000416\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9594, -0.6886, -0.7460,  0.9427, -0.7050],\n",
      "         [ 0.8966, -0.9531, -0.9617,  0.8146, -0.0345]],\n",
      "\n",
      "        [[-0.4509, -0.5279,  0.5719, -0.1756, -0.7234],\n",
      "         [-0.7151, -0.5009,  0.7268, -0.1722, -0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0936 cost = 0.001796\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4557,  0.6299,  0.0320, -0.3806,  0.6537],\n",
      "         [ 0.8966, -0.9531, -0.9617,  0.8146, -0.0345]],\n",
      "\n",
      "        [[ 0.8998,  0.9270, -0.9251,  0.9247,  0.9012],\n",
      "         [-0.7151, -0.5009,  0.7268, -0.1722, -0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0937 cost = 0.000706\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9594, -0.6886, -0.7461,  0.9427, -0.7050],\n",
      "         [-0.5230,  0.4955,  0.0999, -0.5081,  0.7007]],\n",
      "\n",
      "        [[-0.4510, -0.5280,  0.5720, -0.1756, -0.7235],\n",
      "         [ 0.8709,  0.8650, -0.8917,  0.8489,  0.8886]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0937 cost = 0.001909\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3811,  0.6266,  0.1352, -0.3263,  0.6088],\n",
      "         [ 0.9141, -0.8602, -0.9327,  0.8838, -0.1811]],\n",
      "\n",
      "        [[ 0.9205,  0.9132, -0.9389,  0.9153,  0.8970],\n",
      "         [-0.7614, -0.5125,  0.7759, -0.1946, -0.9133]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0937 cost = 0.000550\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8967, -0.9532, -0.9617,  0.8147, -0.0347],\n",
      "         [-0.5231,  0.4956,  0.0999, -0.5081,  0.7009]],\n",
      "\n",
      "        [[-0.7151, -0.5010,  0.7269, -0.1724, -0.8815],\n",
      "         [ 0.8709,  0.8650, -0.8917,  0.8489,  0.8887]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0938 cost = 0.000850\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9142, -0.8602, -0.9328,  0.8838, -0.1812],\n",
      "         [ 0.9594, -0.6887, -0.7462,  0.9427, -0.7051]],\n",
      "\n",
      "        [[-0.7615, -0.5126,  0.7760, -0.1948, -0.9133],\n",
      "         [-0.4512, -0.5282,  0.5721, -0.1757, -0.7235]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0938 cost = 0.001492\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4559,  0.6303,  0.0321, -0.3805,  0.6538],\n",
      "         [-0.3813,  0.6267,  0.1351, -0.3263,  0.6089]],\n",
      "\n",
      "        [[ 0.8999,  0.9270, -0.9251,  0.9248,  0.9013],\n",
      "         [ 0.9205,  0.9133, -0.9389,  0.9153,  0.8971]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0938 cost = 0.000607\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5233,  0.4957,  0.0999, -0.5081,  0.7010],\n",
      "         [-0.4559,  0.6303,  0.0321, -0.3805,  0.6538]],\n",
      "\n",
      "        [[ 0.8710,  0.8651, -0.8918,  0.8490,  0.8888],\n",
      "         [ 0.8999,  0.9270, -0.9251,  0.9248,  0.9013]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0939 cost = 0.001308\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9142, -0.8603, -0.9328,  0.8839, -0.1815],\n",
      "         [ 0.8967, -0.9532, -0.9618,  0.8149, -0.0349]],\n",
      "\n",
      "        [[-0.7616, -0.5127,  0.7761, -0.1950, -0.9133],\n",
      "         [-0.7152, -0.5011,  0.7271, -0.1726, -0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0939 cost = 0.001135\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6268,  0.1351, -0.3262,  0.6090],\n",
      "         [ 0.9594, -0.6888, -0.7463,  0.9427, -0.7051]],\n",
      "\n",
      "        [[ 0.9205,  0.9133, -0.9389,  0.9153,  0.8972],\n",
      "         [-0.4514, -0.5284,  0.5723, -0.1758, -0.7236]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0939 cost = 0.001157\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4559,  0.6306,  0.0322, -0.3803,  0.6539],\n",
      "         [ 0.9594, -0.6888, -0.7464,  0.9427, -0.7052]],\n",
      "\n",
      "        [[ 0.8999,  0.9271, -0.9251,  0.9248,  0.9014],\n",
      "         [-0.4514, -0.5285,  0.5723, -0.1758, -0.7236]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0940 cost = 0.001293\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5233,  0.4959,  0.0998, -0.5080,  0.7013],\n",
      "         [ 0.8968, -0.9532, -0.9618,  0.8151, -0.0351]],\n",
      "\n",
      "        [[ 0.8711,  0.8652, -0.8918,  0.8491,  0.8889],\n",
      "         [-0.7153, -0.5013,  0.7272, -0.1728, -0.8815]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0940 cost = 0.000815\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6269,  0.1351, -0.3261,  0.6090],\n",
      "         [ 0.9143, -0.8603, -0.9329,  0.8839, -0.1818]],\n",
      "\n",
      "        [[ 0.9206,  0.9133, -0.9389,  0.9153,  0.8972],\n",
      "         [-0.7617, -0.5130,  0.7762, -0.1952, -0.9134]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0940 cost = 0.000545\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4560,  0.6307,  0.0322, -0.3802,  0.6540],\n",
      "         [-0.5234,  0.4960,  0.0998, -0.5079,  0.7013]],\n",
      "\n",
      "        [[ 0.8999,  0.9271, -0.9251,  0.9248,  0.9014],\n",
      "         [ 0.8711,  0.8652, -0.8918,  0.8491,  0.8890]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0941 cost = 0.002824\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8969, -0.9533, -0.9618,  0.8152, -0.0353],\n",
      "         [-0.3814,  0.6269,  0.1351, -0.3261,  0.6090]],\n",
      "\n",
      "        [[-0.7154, -0.5014,  0.7273, -0.1730, -0.8816],\n",
      "         [ 0.9206,  0.9133, -0.9389,  0.9153,  0.8973]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0941 cost = 0.000400\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9143, -0.8603, -0.9329,  0.8840, -0.1820],\n",
      "         [ 0.9594, -0.6889, -0.7465,  0.9427, -0.7053]],\n",
      "\n",
      "        [[-0.7618, -0.5132,  0.7763, -0.1954, -0.9134],\n",
      "         [-0.4517, -0.5288,  0.5725, -0.1760, -0.7237]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0941 cost = 0.001481\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9143, -0.8603, -0.9329,  0.8840, -0.1821],\n",
      "         [ 0.9595, -0.6889, -0.7466,  0.9427, -0.7053]],\n",
      "\n",
      "        [[-0.7618, -0.5133,  0.7763, -0.1955, -0.9134],\n",
      "         [-0.4517, -0.5289,  0.5726, -0.1760, -0.7237]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0942 cost = 0.001479\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8970, -0.9533, -0.9619,  0.8154, -0.0354],\n",
      "         [-0.4561,  0.6310,  0.0323, -0.3801,  0.6541]],\n",
      "\n",
      "        [[-0.7155, -0.5016,  0.7274, -0.1731, -0.8816],\n",
      "         [ 0.8999,  0.9271, -0.9251,  0.9248,  0.9015]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0942 cost = 0.000636\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5236,  0.4961,  0.0998, -0.5079,  0.7016],\n",
      "         [-0.3815,  0.6270,  0.1351, -0.3261,  0.6091]],\n",
      "\n",
      "        [[ 0.8712,  0.8654, -0.8919,  0.8492,  0.8891],\n",
      "         [ 0.9206,  0.9133, -0.9389,  0.9154,  0.8973]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0942 cost = 0.000826\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9144, -0.8604, -0.9330,  0.8841, -0.1823],\n",
      "         [ 0.9595, -0.6891, -0.7467,  0.9427, -0.7054]],\n",
      "\n",
      "        [[-0.7619, -0.5135,  0.7764, -0.1957, -0.9134],\n",
      "         [-0.4519, -0.5292,  0.5727, -0.1761, -0.7238]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0943 cost = 0.001475\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5236,  0.4963,  0.0998, -0.5078,  0.7016],\n",
      "         [ 0.8971, -0.9533, -0.9619,  0.8155, -0.0356]],\n",
      "\n",
      "        [[ 0.8713,  0.8654, -0.8919,  0.8492,  0.8891],\n",
      "         [-0.7156, -0.5018,  0.7275, -0.1733, -0.8816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0943 cost = 0.000807\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6272,  0.1352, -0.3259,  0.6091],\n",
      "         [-0.4561,  0.6312,  0.0324, -0.3799,  0.6541]],\n",
      "\n",
      "        [[ 0.9206,  0.9133, -0.9389,  0.9154,  0.8974],\n",
      "         [ 0.8999,  0.9271, -0.9251,  0.9249,  0.9016]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0943 cost = 0.000949\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9595, -0.6892, -0.7468,  0.9427, -0.7054],\n",
      "         [ 0.9145, -0.8605, -0.9330,  0.8841, -0.1826]],\n",
      "\n",
      "        [[-0.4520, -0.5294,  0.5728, -0.1762, -0.7239],\n",
      "         [-0.7620, -0.5137,  0.7766, -0.1959, -0.9135]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0944 cost = 0.001468\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5235,  0.4965,  0.0998, -0.5077,  0.7017],\n",
      "         [ 0.8972, -0.9533, -0.9619,  0.8157, -0.0358]],\n",
      "\n",
      "        [[ 0.8713,  0.8654, -0.8919,  0.8493,  0.8892],\n",
      "         [-0.7156, -0.5020,  0.7276, -0.1735, -0.8816]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0944 cost = 0.000805\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4561,  0.6314,  0.0325, -0.3798,  0.6542],\n",
      "         [-0.3813,  0.6273,  0.1352, -0.3258,  0.6092]],\n",
      "\n",
      "        [[ 0.9000,  0.9271, -0.9251,  0.9249,  0.9016],\n",
      "         [ 0.9206,  0.9133, -0.9389,  0.9154,  0.8974]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0944 cost = 0.000595\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9145, -0.8605, -0.9330,  0.8842, -0.1829],\n",
      "         [ 0.9595, -0.6894, -0.7469,  0.9428, -0.7055]],\n",
      "\n",
      "        [[-0.7621, -0.5139,  0.7767, -0.1961, -0.9135],\n",
      "         [-0.4522, -0.5296,  0.5729, -0.1763, -0.7240]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0945 cost = 0.001467\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3812,  0.6274,  0.1352, -0.3258,  0.6092],\n",
      "         [-0.4561,  0.6316,  0.0325, -0.3797,  0.6542]],\n",
      "\n",
      "        [[ 0.9206,  0.9133, -0.9389,  0.9154,  0.8974],\n",
      "         [ 0.9000,  0.9271, -0.9251,  0.9249,  0.9016]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0945 cost = 0.000945\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5235,  0.4966,  0.0998, -0.5076,  0.7018],\n",
      "         [ 0.8973, -0.9534, -0.9619,  0.8159, -0.0360]],\n",
      "\n",
      "        [[ 0.8713,  0.8654, -0.8919,  0.8493,  0.8892],\n",
      "         [-0.7157, -0.5022,  0.7277, -0.1737, -0.8817]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0945 cost = 0.000802\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9595, -0.6896, -0.7470,  0.9428, -0.7056],\n",
      "         [-0.5235,  0.4967,  0.0998, -0.5075,  0.7018]],\n",
      "\n",
      "        [[-0.4523, -0.5299,  0.5730, -0.1764, -0.7240],\n",
      "         [ 0.8714,  0.8654, -0.8919,  0.8493,  0.8893]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0946 cost = 0.001871\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4561,  0.6318,  0.0326, -0.3796,  0.6543],\n",
      "         [ 0.8973, -0.9534, -0.9620,  0.8161, -0.0361]],\n",
      "\n",
      "        [[ 0.9000,  0.9271, -0.9251,  0.9249,  0.9016],\n",
      "         [-0.7158, -0.5023,  0.7278, -0.1738, -0.8817]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0946 cost = 0.000685\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9146, -0.8606, -0.9331,  0.8843, -0.1833],\n",
      "         [-0.3813,  0.6276,  0.1352, -0.3257,  0.6092]],\n",
      "\n",
      "        [[-0.7623, -0.5142,  0.7768, -0.1964, -0.9136],\n",
      "         [ 0.9206,  0.9133, -0.9389,  0.9154,  0.8975]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0946 cost = 0.000404\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9146, -0.8606, -0.9331,  0.8843, -0.1834],\n",
      "         [ 0.8974, -0.9534, -0.9620,  0.8162, -0.0363]],\n",
      "\n",
      "        [[-0.7623, -0.5143,  0.7768, -0.1964, -0.9136],\n",
      "         [-0.7158, -0.5024,  0.7279, -0.1739, -0.8817]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0947 cost = 0.001109\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4562,  0.6319,  0.0326, -0.3795,  0.6543],\n",
      "         [-0.3813,  0.6276,  0.1352, -0.3256,  0.6092]],\n",
      "\n",
      "        [[ 0.9000,  0.9271, -0.9250,  0.9249,  0.9017],\n",
      "         [ 0.9206,  0.9133, -0.9389,  0.9154,  0.8975]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0947 cost = 0.000591\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9596, -0.6899, -0.7472,  0.9428, -0.7057],\n",
      "         [-0.5236,  0.4968,  0.0998, -0.5075,  0.7019]],\n",
      "\n",
      "        [[-0.4525, -0.5302,  0.5732, -0.1766, -0.7241],\n",
      "         [ 0.8714,  0.8655, -0.8920,  0.8493,  0.8893]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0947 cost = 0.001864\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8975, -0.9534, -0.9620,  0.8164, -0.0365],\n",
      "         [-0.3813,  0.6277,  0.1352, -0.3256,  0.6092]],\n",
      "\n",
      "        [[-0.7159, -0.5026,  0.7280, -0.1740, -0.8818],\n",
      "         [ 0.9207,  0.9133, -0.9389,  0.9154,  0.8975]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0948 cost = 0.000392\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9147, -0.8607, -0.9332,  0.8844, -0.1837],\n",
      "         [ 0.9596, -0.6900, -0.7473,  0.9429, -0.7058]],\n",
      "\n",
      "        [[-0.7624, -0.5145,  0.7770, -0.1967, -0.9136],\n",
      "         [-0.4526, -0.5302,  0.5733, -0.1767, -0.7242]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0948 cost = 0.001454\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5237,  0.4969,  0.0998, -0.5075,  0.7020],\n",
      "         [-0.4563,  0.6321,  0.0328, -0.3794,  0.6544]],\n",
      "\n",
      "        [[ 0.8714,  0.8655, -0.8920,  0.8493,  0.8893],\n",
      "         [ 0.9000,  0.9271, -0.9250,  0.9249,  0.9017]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0948 cost = 0.001273\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6277,  0.1352, -0.3255,  0.6093],\n",
      "         [ 0.9596, -0.6902, -0.7473,  0.9429, -0.7059]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9389,  0.9154,  0.8976],\n",
      "         [-0.4526, -0.5304,  0.5733, -0.1767, -0.7242]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0949 cost = 0.001129\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8976, -0.9535, -0.9620,  0.8167, -0.0367],\n",
      "         [-0.5237,  0.4970,  0.0998, -0.5074,  0.7020]],\n",
      "\n",
      "        [[-0.7160, -0.5028,  0.7281, -0.1742, -0.8818],\n",
      "         [ 0.8714,  0.8655, -0.8920,  0.8493,  0.8894]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0949 cost = 0.000824\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4563,  0.6322,  0.0328, -0.3792,  0.6544],\n",
      "         [ 0.9148, -0.8608, -0.9332,  0.8845, -0.1841]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9249,  0.9017],\n",
      "         [-0.7625, -0.5147,  0.7771, -0.1969, -0.9136]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0949 cost = 0.000663\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5237,  0.4970,  0.0998, -0.5074,  0.7021],\n",
      "         [ 0.9148, -0.8609, -0.9332,  0.8845, -0.1842]],\n",
      "\n",
      "        [[ 0.8714,  0.8655, -0.8920,  0.8493,  0.8894],\n",
      "         [-0.7625, -0.5148,  0.7771, -0.1970, -0.9136]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0950 cost = 0.000748\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6278,  0.1352, -0.3254,  0.6093],\n",
      "         [-0.4563,  0.6323,  0.0329, -0.3791,  0.6545]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9389,  0.9155,  0.8976],\n",
      "         [ 0.9000,  0.9272, -0.9250,  0.9250,  0.9017]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0950 cost = 0.000935\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9596, -0.6905, -0.7475,  0.9429, -0.7060],\n",
      "         [ 0.8977, -0.9535, -0.9621,  0.8169, -0.0369]],\n",
      "\n",
      "        [[-0.4528, -0.5306,  0.5735, -0.1769, -0.7243],\n",
      "         [-0.7162, -0.5029,  0.7283, -0.1745, -0.8818]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0950 cost = 0.001736\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6279,  0.1352, -0.3254,  0.6094],\n",
      "         [ 0.8978, -0.9535, -0.9621,  0.8170, -0.0370]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9389,  0.9155,  0.8976],\n",
      "         [-0.7162, -0.5029,  0.7283, -0.1745, -0.8818]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0951 cost = 0.000549\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5238,  0.4971,  0.0998, -0.5073,  0.7022],\n",
      "         [ 0.9596, -0.6906, -0.7476,  0.9429, -0.7060]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8920,  0.8493,  0.8894],\n",
      "         [-0.4529, -0.5307,  0.5736, -0.1770, -0.7243]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0951 cost = 0.001391\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4564,  0.6325,  0.0330, -0.3790,  0.6545],\n",
      "         [ 0.9149, -0.8609, -0.9333,  0.8847, -0.1846]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018],\n",
      "         [-0.7627, -0.5150,  0.7773, -0.1973, -0.9137]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0951 cost = 0.000659\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9596, -0.6907, -0.7477,  0.9430, -0.7061],\n",
      "         [-0.3814,  0.6279,  0.1353, -0.3253,  0.6094]],\n",
      "\n",
      "        [[-0.4530, -0.5308,  0.5736, -0.1771, -0.7244],\n",
      "         [ 0.9207,  0.9134, -0.9389,  0.9155,  0.8977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0952 cost = 0.001166\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9150, -0.8610, -0.9333,  0.8847, -0.1848],\n",
      "         [ 0.8979, -0.9536, -0.9621,  0.8173, -0.0373]],\n",
      "\n",
      "        [[-0.7627, -0.5151,  0.7773, -0.1974, -0.9137],\n",
      "         [-0.7163, -0.5031,  0.7284, -0.1747, -0.8819]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0952 cost = 0.001091\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5239,  0.4971,  0.0999, -0.5074,  0.7022],\n",
      "         [-0.4564,  0.6326,  0.0330, -0.3790,  0.6546]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895],\n",
      "         [ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0952 cost = 0.001262\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5239,  0.4971,  0.0999, -0.5073,  0.7022],\n",
      "         [-0.3815,  0.6279,  0.1353, -0.3252,  0.6094]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895],\n",
      "         [ 0.9207,  0.9134, -0.9389,  0.9155,  0.8977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0953 cost = 0.000804\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9597, -0.6910, -0.7479,  0.9430, -0.7062],\n",
      "         [ 0.8980, -0.9536, -0.9622,  0.8175, -0.0375]],\n",
      "\n",
      "        [[-0.4531, -0.5311,  0.5737, -0.1773, -0.7245],\n",
      "         [-0.7164, -0.5033,  0.7285, -0.1749, -0.8819]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0953 cost = 0.001722\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9151, -0.8610, -0.9334,  0.8848, -0.1851],\n",
      "         [-0.4563,  0.6329,  0.0331, -0.3787,  0.6546]],\n",
      "\n",
      "        [[-0.7628, -0.5154,  0.7775, -0.1976, -0.9137],\n",
      "         [ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0953 cost = 0.000619\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5238,  0.4974,  0.0999, -0.5071,  0.7023],\n",
      "         [ 0.9597, -0.6912, -0.7480,  0.9430, -0.7063]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895],\n",
      "         [-0.4532, -0.5312,  0.5738, -0.1774, -0.7245]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0954 cost = 0.001380\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4563,  0.6331,  0.0331, -0.3786,  0.6547],\n",
      "         [ 0.8982, -0.9536, -0.9622,  0.8177, -0.0377]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018],\n",
      "         [-0.7165, -0.5035,  0.7286, -0.1750, -0.8820]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0954 cost = 0.000668\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3811,  0.6283,  0.1353, -0.3248,  0.6095],\n",
      "         [ 0.9151, -0.8611, -0.9334,  0.8849, -0.1853]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9389,  0.9155,  0.8977],\n",
      "         [-0.7629, -0.5156,  0.7775, -0.1978, -0.9138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0954 cost = 0.000525\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8982, -0.9536, -0.9622,  0.8179, -0.0378],\n",
      "         [ 0.9152, -0.8611, -0.9334,  0.8849, -0.1854]],\n",
      "\n",
      "        [[-0.7165, -0.5036,  0.7286, -0.1751, -0.8820],\n",
      "         [-0.7629, -0.5157,  0.7776, -0.1979, -0.9138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0955 cost = 0.000618\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5237,  0.4976,  0.0999, -0.5069,  0.7024],\n",
      "         [ 0.9597, -0.6914, -0.7482,  0.9431, -0.7064]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895],\n",
      "         [-0.4533, -0.5315,  0.5739, -0.1776, -0.7246]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0955 cost = 0.001375\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4562,  0.6333,  0.0332, -0.3784,  0.6547],\n",
      "         [-0.3811,  0.6284,  0.1353, -0.3247,  0.6095]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018],\n",
      "         [ 0.9207,  0.9134, -0.9389,  0.9155,  0.8977]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0955 cost = 0.000579\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4562,  0.6333,  0.0332, -0.3783,  0.6547],\n",
      "         [ 0.9597, -0.6915, -0.7483,  0.9431, -0.7064]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018],\n",
      "         [-0.4533, -0.5316,  0.5739, -0.1777, -0.7246]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0956 cost = 0.001235\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3810,  0.6284,  0.1353, -0.3246,  0.6095],\n",
      "         [ 0.9152, -0.8611, -0.9335,  0.8850, -0.1857]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9389,  0.9155,  0.8977],\n",
      "         [-0.7630, -0.5160,  0.7777, -0.1981, -0.9138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0956 cost = 0.000522\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5237,  0.4977,  0.0999, -0.5068,  0.7024],\n",
      "         [ 0.8984, -0.9537, -0.9623,  0.8182, -0.0381]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895],\n",
      "         [-0.7166, -0.5039,  0.7287, -0.1754, -0.8821]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0956 cost = 0.000776\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9153, -0.8611, -0.9335,  0.8850, -0.1859],\n",
      "         [ 0.9598, -0.6917, -0.7484,  0.9431, -0.7065]],\n",
      "\n",
      "        [[-0.7631, -0.5161,  0.7777, -0.1983, -0.9138],\n",
      "         [-0.4534, -0.5318,  0.5740, -0.1778, -0.7247]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0957 cost = 0.001418\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8985, -0.9537, -0.9623,  0.8184, -0.0383],\n",
      "         [-0.5238,  0.4977,  0.1000, -0.5068,  0.7024]],\n",
      "\n",
      "        [[-0.7167, -0.5041,  0.7288, -0.1755, -0.8821],\n",
      "         [ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0957 cost = 0.000810\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3811,  0.6285,  0.1354, -0.3246,  0.6095],\n",
      "         [-0.4563,  0.6336,  0.0334, -0.3782,  0.6548]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9389,  0.9155,  0.8977],\n",
      "         [ 0.9000,  0.9272, -0.9250,  0.9250,  0.9018]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0957 cost = 0.000921\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8986, -0.9537, -0.9623,  0.8185, -0.0384],\n",
      "         [ 0.9598, -0.6918, -0.7485,  0.9432, -0.7066]],\n",
      "\n",
      "        [[-0.7167, -0.5042,  0.7289, -0.1756, -0.8821],\n",
      "         [-0.4535, -0.5320,  0.5740, -0.1780, -0.7248]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0958 cost = 0.001333\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5238,  0.4977,  0.1000, -0.5067,  0.7024],\n",
      "         [ 0.9154, -0.8612, -0.9335,  0.8852, -0.1862]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895],\n",
      "         [-0.7632, -0.5164,  0.7779, -0.1985, -0.9138]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0958 cost = 0.000731\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4564,  0.6337,  0.0335, -0.3781,  0.6548],\n",
      "         [-0.3812,  0.6285,  0.1354, -0.3245,  0.6095]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9251,  0.9018],\n",
      "         [ 0.9207,  0.9134, -0.9388,  0.9155,  0.8978]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0958 cost = 0.000574\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9154, -0.8612, -0.9336,  0.8852, -0.1864],\n",
      "         [ 0.8986, -0.9538, -0.9623,  0.8187, -0.0386]],\n",
      "\n",
      "        [[-0.7632, -0.5165,  0.7779, -0.1987, -0.9139],\n",
      "         [-0.7168, -0.5044,  0.7290, -0.1758, -0.8821]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0959 cost = 0.001068\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9598, -0.6920, -0.7487,  0.9432, -0.7067],\n",
      "         [-0.5239,  0.4977,  0.1000, -0.5067,  0.7024]],\n",
      "\n",
      "        [[-0.4537, -0.5323,  0.5742, -0.1782, -0.7249],\n",
      "         [ 0.8715,  0.8656, -0.8919,  0.8493,  0.8895]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0959 cost = 0.001817\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4565,  0.6338,  0.0336, -0.3780,  0.6548],\n",
      "         [-0.3812,  0.6286,  0.1354, -0.3245,  0.6096]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9250,  0.9251,  0.9018],\n",
      "         [ 0.9207,  0.9134, -0.9388,  0.9155,  0.8978]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0959 cost = 0.000573\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9155, -0.8613, -0.9336,  0.8853, -0.1866],\n",
      "         [ 0.8987, -0.9538, -0.9624,  0.8189, -0.0388]],\n",
      "\n",
      "        [[-0.7633, -0.5167,  0.7780, -0.1989, -0.9139],\n",
      "         [-0.7169, -0.5045,  0.7291, -0.1760, -0.8822]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0960 cost = 0.001065\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5240,  0.4977,  0.1000, -0.5067,  0.7025],\n",
      "         [-0.4565,  0.6338,  0.0336, -0.3779,  0.6548]],\n",
      "\n",
      "        [[ 0.8715,  0.8656, -0.8919,  0.8492,  0.8896],\n",
      "         [ 0.9000,  0.9272, -0.9249,  0.9251,  0.9019]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0960 cost = 0.001242\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9599, -0.6922, -0.7488,  0.9433, -0.7068],\n",
      "         [-0.3812,  0.6287,  0.1355, -0.3244,  0.6096]],\n",
      "\n",
      "        [[-0.4538, -0.5326,  0.5743, -0.1783, -0.7249],\n",
      "         [ 0.9207,  0.9134, -0.9388,  0.9155,  0.8978]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0960 cost = 0.001139\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4565,  0.6340,  0.0337, -0.3778,  0.6549],\n",
      "         [ 0.9599, -0.6923, -0.7489,  0.9433, -0.7069]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9249,  0.9251,  0.9019],\n",
      "         [-0.4538, -0.5326,  0.5743, -0.1784, -0.7250]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0961 cost = 0.001217\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3812,  0.6287,  0.1355, -0.3244,  0.6097],\n",
      "         [-0.5240,  0.4979,  0.1000, -0.5066,  0.7026]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9388,  0.9155,  0.8978],\n",
      "         [ 0.8715,  0.8656, -0.8919,  0.8492,  0.8896]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0961 cost = 0.002741\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9156, -0.8613, -0.9336,  0.8854, -0.1870],\n",
      "         [ 0.8989, -0.9538, -0.9624,  0.8193, -0.0391]],\n",
      "\n",
      "        [[-0.7635, -0.5170,  0.7782, -0.1992, -0.9139],\n",
      "         [-0.7171, -0.5048,  0.7293, -0.1762, -0.8822]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0961 cost = 0.001059\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4565,  0.6342,  0.0337, -0.3776,  0.6550],\n",
      "         [ 0.9156, -0.8613, -0.9337,  0.8854, -0.1871]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9249,  0.9251,  0.9019],\n",
      "         [-0.7635, -0.5171,  0.7782, -0.1992, -0.9139]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0962 cost = 0.000640\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8990, -0.9539, -0.9624,  0.8194, -0.0392],\n",
      "         [ 0.9599, -0.6925, -0.7490,  0.9433, -0.7070]],\n",
      "\n",
      "        [[-0.7171, -0.5049,  0.7293, -0.1763, -0.8823],\n",
      "         [-0.4540, -0.5329,  0.5745, -0.1785, -0.7250]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0962 cost = 0.001317\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5240,  0.4981,  0.1000, -0.5065,  0.7028],\n",
      "         [-0.3812,  0.6289,  0.1355, -0.3242,  0.6098]],\n",
      "\n",
      "        [[ 0.8716,  0.8656, -0.8919,  0.8493,  0.8897],\n",
      "         [ 0.9207,  0.9134, -0.9388,  0.9155,  0.8979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0962 cost = 0.000785\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4565,  0.6344,  0.0338, -0.3775,  0.6551],\n",
      "         [-0.3811,  0.6290,  0.1355, -0.3242,  0.6098]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9249,  0.9251,  0.9019],\n",
      "         [ 0.9207,  0.9134, -0.9388,  0.9155,  0.8979]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0963 cost = 0.000567\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9157, -0.8614, -0.9337,  0.8855, -0.1874],\n",
      "         [-0.5240,  0.4983,  0.1000, -0.5064,  0.7029]],\n",
      "\n",
      "        [[-0.7636, -0.5174,  0.7783, -0.1994, -0.9140],\n",
      "         [ 0.8716,  0.8657, -0.8919,  0.8493,  0.8897]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0963 cost = 0.000767\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9599, -0.6926, -0.7491,  0.9433, -0.7070],\n",
      "         [ 0.8991, -0.9539, -0.9625,  0.8196, -0.0394]],\n",
      "\n",
      "        [[-0.4542, -0.5332,  0.5746, -0.1786, -0.7251],\n",
      "         [-0.7172, -0.5052,  0.7294, -0.1765, -0.8823]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0963 cost = 0.001674\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5239,  0.4985,  0.1000, -0.5062,  0.7030],\n",
      "         [ 0.9599, -0.6927, -0.7492,  0.9433, -0.7071]],\n",
      "\n",
      "        [[ 0.8716,  0.8657, -0.8919,  0.8494,  0.8897],\n",
      "         [-0.4542, -0.5332,  0.5747, -0.1787, -0.7251]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0964 cost = 0.001343\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4564,  0.6347,  0.0339, -0.3772,  0.6552],\n",
      "         [ 0.8992, -0.9539, -0.9625,  0.8197, -0.0396]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9249,  0.9251,  0.9020],\n",
      "         [-0.7172, -0.5053,  0.7295, -0.1766, -0.8823]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0964 cost = 0.000647\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.6294,  0.1355, -0.3239,  0.6100],\n",
      "         [ 0.9157, -0.8614, -0.9337,  0.8855, -0.1876]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9388,  0.9156,  0.8980],\n",
      "         [-0.7637, -0.5177,  0.7785, -0.1997, -0.9140]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0964 cost = 0.000511\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4564,  0.6349,  0.0340, -0.3771,  0.6552],\n",
      "         [ 0.9157, -0.8615, -0.9338,  0.8856, -0.1877]],\n",
      "\n",
      "        [[ 0.9000,  0.9272, -0.9249,  0.9251,  0.9020],\n",
      "         [-0.7638, -0.5177,  0.7785, -0.1998, -0.9140]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0965 cost = 0.000635\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8993, -0.9539, -0.9625,  0.8199, -0.0398],\n",
      "         [-0.3808,  0.6295,  0.1355, -0.3239,  0.6101]],\n",
      "\n",
      "        [[-0.7173, -0.5055,  0.7296, -0.1768, -0.8824],\n",
      "         [ 0.9207,  0.9134, -0.9388,  0.9156,  0.8980]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0965 cost = 0.000373\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5239,  0.4987,  0.1000, -0.5061,  0.7031],\n",
      "         [ 0.9599, -0.6929, -0.7493,  0.9433, -0.7072]],\n",
      "\n",
      "        [[ 0.8717,  0.8657, -0.8919,  0.8494,  0.8898],\n",
      "         [-0.4544, -0.5336,  0.5748, -0.1789, -0.7252]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0965 cost = 0.001337\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3808,  0.6295,  0.1355, -0.3238,  0.6101],\n",
      "         [-0.4565,  0.6350,  0.0341, -0.3770,  0.6553]],\n",
      "\n",
      "        [[ 0.9207,  0.9134, -0.9388,  0.9156,  0.8980],\n",
      "         [ 0.9000,  0.9272, -0.9249,  0.9252,  0.9020]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0966 cost = 0.000904\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9158, -0.8615, -0.9338,  0.8856, -0.1880],\n",
      "         [ 0.8993, -0.9540, -0.9625,  0.8201, -0.0399]],\n",
      "\n",
      "        [[-0.7639, -0.5180,  0.7786, -0.2000, -0.9141],\n",
      "         [-0.7174, -0.5056,  0.7297, -0.1769, -0.8824]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0966 cost = 0.001044\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9599, -0.6930, -0.7495,  0.9434, -0.7073],\n",
      "         [-0.5239,  0.4988,  0.1000, -0.5060,  0.7032]],\n",
      "\n",
      "        [[-0.4545, -0.5337,  0.5749, -0.1790, -0.7253],\n",
      "         [ 0.8717,  0.8657, -0.8920,  0.8494,  0.8898]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0966 cost = 0.001788\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5239,  0.4988,  0.1000, -0.5060,  0.7032],\n",
      "         [ 0.9159, -0.8615, -0.9338,  0.8857, -0.1882]],\n",
      "\n",
      "        [[ 0.8717,  0.8657, -0.8920,  0.8494,  0.8898],\n",
      "         [-0.7639, -0.5181,  0.7787, -0.2001, -0.9141]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0967 cost = 0.000714\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9600, -0.6931, -0.7495,  0.9434, -0.7073],\n",
      "         [-0.3808,  0.6296,  0.1355, -0.3237,  0.6101]],\n",
      "\n",
      "        [[-0.4546, -0.5339,  0.5749, -0.1791, -0.7253],\n",
      "         [ 0.9208,  0.9134, -0.9388,  0.9156,  0.8980]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0967 cost = 0.001120\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8995, -0.9540, -0.9626,  0.8204, -0.0402],\n",
      "         [-0.4565,  0.6352,  0.0342, -0.3768,  0.6553]],\n",
      "\n",
      "        [[-0.7175, -0.5059,  0.7298, -0.1772, -0.8825],\n",
      "         [ 0.9001,  0.9272, -0.9249,  0.9252,  0.9020]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0967 cost = 0.000593\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9159, -0.8616, -0.9339,  0.8858, -0.1884],\n",
      "         [-0.5240,  0.4988,  0.1000, -0.5060,  0.7033]],\n",
      "\n",
      "        [[-0.7640, -0.5183,  0.7788, -0.2003, -0.9141],\n",
      "         [ 0.8717,  0.8658, -0.8919,  0.8494,  0.8898]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0968 cost = 0.000758\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4566,  0.6353,  0.0343, -0.3768,  0.6553],\n",
      "         [ 0.8995, -0.9540, -0.9626,  0.8205, -0.0403]],\n",
      "\n",
      "        [[ 0.9001,  0.9272, -0.9249,  0.9252,  0.9021],\n",
      "         [-0.7175, -0.5059,  0.7298, -0.1773, -0.8825]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0968 cost = 0.000639\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9600, -0.6934, -0.7497,  0.9434, -0.7074],\n",
      "         [-0.3809,  0.6296,  0.1356, -0.3237,  0.6101]],\n",
      "\n",
      "        [[-0.4547, -0.5340,  0.5750, -0.1793, -0.7254],\n",
      "         [ 0.9208,  0.9134, -0.9388,  0.9156,  0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0968 cost = 0.001116\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9160, -0.8616, -0.9339,  0.8858, -0.1887],\n",
      "         [-0.3810,  0.6296,  0.1356, -0.3237,  0.6101]],\n",
      "\n",
      "        [[-0.7641, -0.5184,  0.7788, -0.2005, -0.9141],\n",
      "         [ 0.9208,  0.9134, -0.9388,  0.9156,  0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0969 cost = 0.000380\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9600, -0.6936, -0.7498,  0.9435, -0.7075],\n",
      "         [-0.5241,  0.4988,  0.1001, -0.5060,  0.7033]],\n",
      "\n",
      "        [[-0.4547, -0.5341,  0.5751, -0.1794, -0.7254],\n",
      "         [ 0.8717,  0.8658, -0.8919,  0.8494,  0.8898]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0969 cost = 0.001777\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4567,  0.6354,  0.0344, -0.3767,  0.6553],\n",
      "         [ 0.8997, -0.9541, -0.9626,  0.8207, -0.0406]],\n",
      "\n",
      "        [[ 0.9001,  0.9273, -0.9249,  0.9252,  0.9021],\n",
      "         [-0.7176, -0.5060,  0.7300, -0.1775, -0.8825]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0969 cost = 0.000637\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9160, -0.8617, -0.9339,  0.8859, -0.1890],\n",
      "         [-0.3811,  0.6296,  0.1356, -0.3237,  0.6101]],\n",
      "\n",
      "        [[-0.7642, -0.5184,  0.7789, -0.2008, -0.9141],\n",
      "         [ 0.9208,  0.9134, -0.9388,  0.9156,  0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0970 cost = 0.000379\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8997, -0.9541, -0.9627,  0.8209, -0.0407],\n",
      "         [-0.5242,  0.4988,  0.1001, -0.5060,  0.7033]],\n",
      "\n",
      "        [[-0.7177, -0.5061,  0.7300, -0.1776, -0.8825],\n",
      "         [ 0.8717,  0.8658, -0.8919,  0.8493,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0970 cost = 0.000785\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9600, -0.6939, -0.7499,  0.9435, -0.7076],\n",
      "         [-0.4568,  0.6354,  0.0345, -0.3767,  0.6553]],\n",
      "\n",
      "        [[-0.4548, -0.5342,  0.5752, -0.1796, -0.7255],\n",
      "         [ 0.9001,  0.9273, -0.9249,  0.9252,  0.9021]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0970 cost = 0.001382\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8998, -0.9541, -0.9627,  0.8210, -0.0408],\n",
      "         [-0.5243,  0.4988,  0.1001, -0.5060,  0.7034]],\n",
      "\n",
      "        [[-0.7177, -0.5061,  0.7301, -0.1777, -0.8825],\n",
      "         [ 0.8717,  0.8658, -0.8919,  0.8493,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0971 cost = 0.000783\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9600, -0.6941, -0.7500,  0.9435, -0.7077],\n",
      "         [-0.4568,  0.6354,  0.0346, -0.3766,  0.6553]],\n",
      "\n",
      "        [[-0.4549, -0.5342,  0.5752, -0.1797, -0.7255],\n",
      "         [ 0.9001,  0.9273, -0.9249,  0.9252,  0.9021]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0971 cost = 0.001380\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9161, -0.8619, -0.9340,  0.8860, -0.1895],\n",
      "         [-0.3812,  0.6296,  0.1357, -0.3236,  0.6101]],\n",
      "\n",
      "        [[-0.7643, -0.5185,  0.7791, -0.2011, -0.9142],\n",
      "         [ 0.9208,  0.9134, -0.9388,  0.9156,  0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0971 cost = 0.000377\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8998, -0.9541, -0.9627,  0.8211, -0.0410],\n",
      "         [-0.5244,  0.4987,  0.1002, -0.5061,  0.7034]],\n",
      "\n",
      "        [[-0.7179, -0.5060,  0.7302, -0.1779, -0.8825],\n",
      "         [ 0.8717,  0.8658, -0.8919,  0.8493,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0972 cost = 0.000781\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4569,  0.6354,  0.0348, -0.3765,  0.6553],\n",
      "         [-0.3813,  0.6296,  0.1357, -0.3236,  0.6101]],\n",
      "\n",
      "        [[ 0.9000,  0.9273, -0.9249,  0.9252,  0.9021],\n",
      "         [ 0.9208,  0.9134, -0.9388,  0.9156,  0.8981]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0972 cost = 0.000554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9601, -0.6945, -0.7501,  0.9435, -0.7078],\n",
      "         [ 0.9162, -0.8619, -0.9340,  0.8860, -0.1897]],\n",
      "\n",
      "        [[-0.4550, -0.5343,  0.5753, -0.1799, -0.7255],\n",
      "         [-0.7644, -0.5184,  0.7792, -0.2013, -0.9142]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0972 cost = 0.001362\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.8999, -0.9542, -0.9627,  0.8213, -0.0412],\n",
      "         [-0.5245,  0.4987,  0.1002, -0.5061,  0.7035]],\n",
      "\n",
      "        [[-0.7180, -0.5059,  0.7303, -0.1781, -0.8825],\n",
      "         [ 0.8717,  0.8659, -0.8919,  0.8493,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0973 cost = 0.000779\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3813,  0.6296,  0.1357, -0.3236,  0.6101],\n",
      "         [-0.4571,  0.6354,  0.0349, -0.3764,  0.6553]],\n",
      "\n",
      "        [[ 0.9208,  0.9134, -0.9388,  0.9156,  0.8981],\n",
      "         [ 0.9000,  0.9273, -0.9248,  0.9252,  0.9021]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0973 cost = 0.000892\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9162, -0.8620, -0.9340,  0.8861, -0.1900],\n",
      "         [ 0.9601, -0.6947, -0.7502,  0.9436, -0.7079]],\n",
      "\n",
      "        [[-0.7645, -0.5184,  0.7793, -0.2016, -0.9142],\n",
      "         [-0.4551, -0.5343,  0.5754, -0.1801, -0.7255]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0973 cost = 0.001357\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4571,  0.6354,  0.0350, -0.3764,  0.6553],\n",
      "         [-0.5245,  0.4987,  0.1002, -0.5061,  0.7036]],\n",
      "\n",
      "        [[ 0.9000,  0.9273, -0.9248,  0.9252,  0.9021],\n",
      "         [ 0.8717,  0.8659, -0.8919,  0.8493,  0.8899]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0974 cost = 0.002711\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9162, -0.8620, -0.9341,  0.8861, -0.1902],\n",
      "         [-0.3814,  0.6296,  0.1357, -0.3236,  0.6102]],\n",
      "\n",
      "        [[-0.7645, -0.5184,  0.7793, -0.2017, -0.9142],\n",
      "         [ 0.9208,  0.9135, -0.9388,  0.9156,  0.8982]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0974 cost = 0.000375\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9601, -0.6949, -0.7503,  0.9436, -0.7080],\n",
      "         [ 0.9000, -0.9542, -0.9627,  0.8214, -0.0414]],\n",
      "\n",
      "        [[-0.4552, -0.5343,  0.5755, -0.1802, -0.7256],\n",
      "         [-0.7181, -0.5058,  0.7305, -0.1784, -0.8825]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0974 cost = 0.001626\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3815,  0.6296,  0.1357, -0.3236,  0.6102],\n",
      "         [ 0.9601, -0.6950, -0.7503,  0.9436, -0.7080]],\n",
      "\n",
      "        [[ 0.9208,  0.9135, -0.9388,  0.9156,  0.8982],\n",
      "         [-0.4553, -0.5344,  0.5756, -0.1803, -0.7256]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0975 cost = 0.001051\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4573,  0.6356,  0.0351, -0.3763,  0.6553],\n",
      "         [ 0.9163, -0.8621, -0.9341,  0.8862, -0.1905]],\n",
      "\n",
      "        [[ 0.9000,  0.9274, -0.9248,  0.9252,  0.9022],\n",
      "         [-0.7646, -0.5184,  0.7794, -0.2019, -0.9142]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0975 cost = 0.000617\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5247,  0.4988,  0.1002, -0.5061,  0.7038],\n",
      "         [ 0.9000, -0.9542, -0.9628,  0.8216, -0.0416]],\n",
      "\n",
      "        [[ 0.8718,  0.8660, -0.8919,  0.8493,  0.8900],\n",
      "         [-0.7182, -0.5059,  0.7307, -0.1786, -0.8825]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0975 cost = 0.000736\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9163, -0.8622, -0.9341,  0.8862, -0.1907],\n",
      "         [-0.3816,  0.6296,  0.1357, -0.3236,  0.6102]],\n",
      "\n",
      "        [[-0.7647, -0.5185,  0.7795, -0.2020, -0.9142],\n",
      "         [ 0.9208,  0.9135, -0.9388,  0.9156,  0.8983]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0976 cost = 0.000373\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9601, -0.6953, -0.7505,  0.9436, -0.7081],\n",
      "         [-0.4574,  0.6356,  0.0352, -0.3762,  0.6554]],\n",
      "\n",
      "        [[-0.4554, -0.5345,  0.5757, -0.1805, -0.7256],\n",
      "         [ 0.9001,  0.9274, -0.9248,  0.9252,  0.9022]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0976 cost = 0.001362\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9001, -0.9542, -0.9628,  0.8217, -0.0418],\n",
      "         [-0.5249,  0.4988,  0.1002, -0.5062,  0.7038]],\n",
      "\n",
      "        [[-0.7183, -0.5059,  0.7307, -0.1787, -0.8826],\n",
      "         [ 0.8718,  0.8661, -0.8919,  0.8493,  0.8901]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0976 cost = 0.000772\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4575,  0.6357,  0.0352, -0.3762,  0.6554],\n",
      "         [-0.5249,  0.4988,  0.1002, -0.5062,  0.7038]],\n",
      "\n",
      "        [[ 0.9001,  0.9274, -0.9248,  0.9252,  0.9022],\n",
      "         [ 0.8718,  0.8661, -0.8919,  0.8493,  0.8901]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0977 cost = 0.002698\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9601, -0.6955, -0.7506,  0.9436, -0.7082],\n",
      "         [ 0.9001, -0.9542, -0.9628,  0.8218, -0.0419]],\n",
      "\n",
      "        [[-0.4555, -0.5346,  0.5758, -0.1806, -0.7257],\n",
      "         [-0.7184, -0.5059,  0.7308, -0.1789, -0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0977 cost = 0.001615\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3818,  0.6296,  0.1358, -0.3237,  0.6102],\n",
      "         [ 0.9164, -0.8623, -0.9342,  0.8863, -0.1911]],\n",
      "\n",
      "        [[ 0.9208,  0.9135, -0.9387,  0.9156,  0.8983],\n",
      "         [-0.7648, -0.5186,  0.7796, -0.2024, -0.9142]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0977 cost = 0.000494\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9164, -0.8623, -0.9342,  0.8863, -0.1912],\n",
      "         [ 0.9601, -0.6956, -0.7507,  0.9436, -0.7082]],\n",
      "\n",
      "        [[-0.7648, -0.5186,  0.7796, -0.2024, -0.9142],\n",
      "         [-0.4556, -0.5347,  0.5758, -0.1807, -0.7257]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0978 cost = 0.001343\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9002, -0.9543, -0.9628,  0.8219, -0.0421],\n",
      "         [-0.3819,  0.6296,  0.1358, -0.3237,  0.6102]],\n",
      "\n",
      "        [[-0.7185, -0.5059,  0.7309, -0.1790, -0.8826],\n",
      "         [ 0.9208,  0.9135, -0.9387,  0.9156,  0.8984]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0978 cost = 0.000360\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4577,  0.6358,  0.0354, -0.3761,  0.6554],\n",
      "         [-0.5251,  0.4988,  0.1002, -0.5063,  0.7040]],\n",
      "\n",
      "        [[ 0.9001,  0.9274, -0.9248,  0.9252,  0.9023],\n",
      "         [ 0.8719,  0.8662, -0.8919,  0.8493,  0.8902]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0978 cost = 0.002688\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5252,  0.4988,  0.1002, -0.5063,  0.7041],\n",
      "         [ 0.9164, -0.8624, -0.9342,  0.8864, -0.1915]],\n",
      "\n",
      "        [[ 0.8719,  0.8662, -0.8919,  0.8493,  0.8902],\n",
      "         [-0.7649, -0.5187,  0.7797, -0.2026, -0.9143]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0979 cost = 0.000693\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3821,  0.6297,  0.1358, -0.3237,  0.6102],\n",
      "         [ 0.9002, -0.9543, -0.9628,  0.8220, -0.0423]],\n",
      "\n",
      "        [[ 0.9208,  0.9136, -0.9387,  0.9156,  0.8984],\n",
      "         [-0.7185, -0.5060,  0.7310, -0.1792, -0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0979 cost = 0.000506\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4579,  0.6360,  0.0354, -0.3761,  0.6555],\n",
      "         [ 0.9602, -0.6959, -0.7509,  0.9436, -0.7083]],\n",
      "\n",
      "        [[ 0.9001,  0.9275, -0.9248,  0.9252,  0.9024],\n",
      "         [-0.4558, -0.5350,  0.5760, -0.1809, -0.7258]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0979 cost = 0.001158\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9602, -0.6959, -0.7509,  0.9436, -0.7083],\n",
      "         [ 0.9165, -0.8624, -0.9343,  0.8864, -0.1917]],\n",
      "\n",
      "        [[-0.4558, -0.5350,  0.5761, -0.1809, -0.7258],\n",
      "         [-0.7650, -0.5188,  0.7798, -0.2028, -0.9143]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0980 cost = 0.001338\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3823,  0.6297,  0.1358, -0.3238,  0.6102],\n",
      "         [-0.4580,  0.6361,  0.0355, -0.3761,  0.6555]],\n",
      "\n",
      "        [[ 0.9209,  0.9136, -0.9387,  0.9156,  0.8985],\n",
      "         [ 0.9001,  0.9275, -0.9248,  0.9252,  0.9024]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0980 cost = 0.000877\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5255,  0.4989,  0.1002, -0.5064,  0.7043],\n",
      "         [ 0.9003, -0.9543, -0.9629,  0.8221, -0.0425]],\n",
      "\n",
      "        [[ 0.8720,  0.8663, -0.8920,  0.8494,  0.8903],\n",
      "         [-0.7186, -0.5061,  0.7311, -0.1794, -0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0980 cost = 0.000726\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9602, -0.6961, -0.7510,  0.9437, -0.7084],\n",
      "         [-0.4581,  0.6362,  0.0355, -0.3761,  0.6555]],\n",
      "\n",
      "        [[-0.4559, -0.5352,  0.5761, -0.1810, -0.7258],\n",
      "         [ 0.9001,  0.9275, -0.9248,  0.9252,  0.9025]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0981 cost = 0.001347\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3825,  0.6297,  0.1358, -0.3238,  0.6102],\n",
      "         [-0.5256,  0.4990,  0.1001, -0.5065,  0.7043]],\n",
      "\n",
      "        [[ 0.9209,  0.9136, -0.9387,  0.9156,  0.8986],\n",
      "         [ 0.8721,  0.8664, -0.8920,  0.8494,  0.8904]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0981 cost = 0.002663\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9165, -0.8624, -0.9343,  0.8865, -0.1920],\n",
      "         [ 0.9004, -0.9543, -0.9629,  0.8223, -0.0426]],\n",
      "\n",
      "        [[-0.7651, -0.5190,  0.7800, -0.2031, -0.9143],\n",
      "         [-0.7187, -0.5061,  0.7312, -0.1795, -0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0981 cost = 0.001003\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3826,  0.6297,  0.1358, -0.3238,  0.6102],\n",
      "         [ 0.9165, -0.8624, -0.9344,  0.8865, -0.1921]],\n",
      "\n",
      "        [[ 0.9209,  0.9136, -0.9388,  0.9157,  0.8986],\n",
      "         [-0.7651, -0.5191,  0.7800, -0.2032, -0.9143]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0982 cost = 0.000488\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9602, -0.6962, -0.7512,  0.9437, -0.7085],\n",
      "         [-0.5258,  0.4991,  0.1001, -0.5065,  0.7045]],\n",
      "\n",
      "        [[-0.4560, -0.5354,  0.5763, -0.1811, -0.7258],\n",
      "         [ 0.8721,  0.8664, -0.8921,  0.8495,  0.8905]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0982 cost = 0.001723\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9004, -0.9544, -0.9629,  0.8224, -0.0428],\n",
      "         [-0.4583,  0.6365,  0.0356, -0.3760,  0.6556]],\n",
      "\n",
      "        [[-0.7187, -0.5062,  0.7313, -0.1797, -0.8826],\n",
      "         [ 0.9002,  0.9275, -0.9248,  0.9253,  0.9026]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0982 cost = 0.000570\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9602, -0.6963, -0.7513,  0.9437, -0.7085],\n",
      "         [-0.5259,  0.4992,  0.1001, -0.5065,  0.7045]],\n",
      "\n",
      "        [[-0.4561, -0.5354,  0.5763, -0.1812, -0.7259],\n",
      "         [ 0.8722,  0.8665, -0.8921,  0.8495,  0.8905]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0983 cost = 0.001720\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9166, -0.8625, -0.9344,  0.8865, -0.1924],\n",
      "         [ 0.9005, -0.9544, -0.9629,  0.8225, -0.0429]],\n",
      "\n",
      "        [[-0.7653, -0.5192,  0.7801, -0.2034, -0.9143],\n",
      "         [-0.7188, -0.5062,  0.7314, -0.1798, -0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0983 cost = 0.000998\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3828,  0.6297,  0.1358, -0.3237,  0.6103],\n",
      "         [-0.4583,  0.6366,  0.0357, -0.3759,  0.6556]],\n",
      "\n",
      "        [[ 0.9209,  0.9137, -0.9388,  0.9157,  0.8988],\n",
      "         [ 0.9002,  0.9276, -0.9248,  0.9253,  0.9026]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0983 cost = 0.000868\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3829,  0.6297,  0.1358, -0.3237,  0.6103],\n",
      "         [-0.4584,  0.6366,  0.0357, -0.3759,  0.6557]],\n",
      "\n",
      "        [[ 0.9209,  0.9137, -0.9388,  0.9157,  0.8988],\n",
      "         [ 0.9002,  0.9276, -0.9248,  0.9253,  0.9026]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0984 cost = 0.000867\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5261,  0.4992,  0.1001, -0.5065,  0.7047],\n",
      "         [ 0.9006, -0.9544, -0.9629,  0.8226, -0.0431]],\n",
      "\n",
      "        [[ 0.8722,  0.8666, -0.8921,  0.8496,  0.8906],\n",
      "         [-0.7189, -0.5063,  0.7315, -0.1800, -0.8826]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0984 cost = 0.000718\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9602, -0.6966, -0.7515,  0.9437, -0.7086],\n",
      "         [ 0.9167, -0.8626, -0.9345,  0.8866, -0.1928]],\n",
      "\n",
      "        [[-0.4563, -0.5356,  0.5765, -0.1814, -0.7259],\n",
      "         [-0.7654, -0.5193,  0.7802, -0.2036, -0.9144]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0984 cost = 0.001325\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9167, -0.8626, -0.9345,  0.8866, -0.1929],\n",
      "         [ 0.9602, -0.6967, -0.7515,  0.9437, -0.7086]],\n",
      "\n",
      "        [[-0.7654, -0.5194,  0.7803, -0.2037, -0.9144],\n",
      "         [-0.4563, -0.5356,  0.5766, -0.1815, -0.7259]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0985 cost = 0.001322\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9007, -0.9544, -0.9630,  0.8228, -0.0433],\n",
      "         [-0.5262,  0.4993,  0.1001, -0.5065,  0.7048]],\n",
      "\n",
      "        [[-0.7189, -0.5064,  0.7316, -0.1801, -0.8826],\n",
      "         [ 0.8723,  0.8666, -0.8921,  0.8496,  0.8907]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0985 cost = 0.000753\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4585,  0.6369,  0.0358, -0.3758,  0.6557],\n",
      "         [-0.3831,  0.6297,  0.1358, -0.3236,  0.6103]],\n",
      "\n",
      "        [[ 0.9002,  0.9276, -0.9248,  0.9253,  0.9027],\n",
      "         [ 0.9210,  0.9137, -0.9388,  0.9157,  0.8989]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0985 cost = 0.000533\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9603, -0.6970, -0.7517,  0.9437, -0.7087],\n",
      "         [ 0.9167, -0.8627, -0.9345,  0.8867, -0.1932]],\n",
      "\n",
      "        [[-0.4564, -0.5358,  0.5767, -0.1816, -0.7260],\n",
      "         [-0.7655, -0.5195,  0.7804, -0.2039, -0.9144]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0986 cost = 0.001320\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9007, -0.9544, -0.9630,  0.8230, -0.0435],\n",
      "         [-0.4585,  0.6369,  0.0358, -0.3758,  0.6558]],\n",
      "\n",
      "        [[-0.7190, -0.5065,  0.7317, -0.1803, -0.8826],\n",
      "         [ 0.9002,  0.9276, -0.9248,  0.9253,  0.9027]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0986 cost = 0.000563\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5263,  0.4993,  0.1001, -0.5065,  0.7049],\n",
      "         [-0.3832,  0.6298,  0.1358, -0.3236,  0.6104]],\n",
      "\n",
      "        [[ 0.8723,  0.8667, -0.8921,  0.8496,  0.8907],\n",
      "         [ 0.9210,  0.9137, -0.9388,  0.9157,  0.8989]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0986 cost = 0.000738\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9603, -0.6972, -0.7518,  0.9437, -0.7088],\n",
      "         [ 0.9008, -0.9545, -0.9630,  0.8231, -0.0436]],\n",
      "\n",
      "        [[-0.4565, -0.5359,  0.5767, -0.1817, -0.7260],\n",
      "         [-0.7191, -0.5065,  0.7318, -0.1804, -0.8827]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0987 cost = 0.001578\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5262,  0.4995,  0.1001, -0.5064,  0.7050],\n",
      "         [ 0.9168, -0.8628, -0.9346,  0.8868, -0.1935]],\n",
      "\n",
      "        [[ 0.8723,  0.8667, -0.8921,  0.8496,  0.8908],\n",
      "         [-0.7656, -0.5197,  0.7805, -0.2042, -0.9144]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0987 cost = 0.000678\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3830,  0.6299,  0.1358, -0.3234,  0.6105],\n",
      "         [-0.4585,  0.6372,  0.0359, -0.3756,  0.6559]],\n",
      "\n",
      "        [[ 0.9210,  0.9137, -0.9387,  0.9157,  0.8990],\n",
      "         [ 0.9002,  0.9276, -0.9247,  0.9253,  0.9028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0987 cost = 0.000859\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5262,  0.4996,  0.1001, -0.5064,  0.7051],\n",
      "         [ 0.9169, -0.8628, -0.9346,  0.8868, -0.1937]],\n",
      "\n",
      "        [[ 0.8723,  0.8667, -0.8921,  0.8496,  0.8908],\n",
      "         [-0.7656, -0.5198,  0.7805, -0.2043, -0.9144]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0988 cost = 0.000677\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9603, -0.6976, -0.7520,  0.9438, -0.7089],\n",
      "         [-0.3830,  0.6300,  0.1358, -0.3233,  0.6105]],\n",
      "\n",
      "        [[-0.4566, -0.5361,  0.5768, -0.1819, -0.7260],\n",
      "         [ 0.9210,  0.9137, -0.9387,  0.9157,  0.8990]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0988 cost = 0.001063\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4585,  0.6373,  0.0360, -0.3755,  0.6560],\n",
      "         [ 0.9009, -0.9545, -0.9631,  0.8234, -0.0439]],\n",
      "\n",
      "        [[ 0.9002,  0.9276, -0.9247,  0.9254,  0.9028],\n",
      "         [-0.7192, -0.5067,  0.7319, -0.1807, -0.8827]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0988 cost = 0.000602\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9603, -0.6978, -0.7521,  0.9438, -0.7090],\n",
      "         [-0.5262,  0.4997,  0.1001, -0.5063,  0.7051]],\n",
      "\n",
      "        [[-0.4567, -0.5362,  0.5769, -0.1820, -0.7261],\n",
      "         [ 0.8723,  0.8667, -0.8921,  0.8496,  0.8908]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0989 cost = 0.001696\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3830,  0.6301,  0.1358, -0.3232,  0.6106],\n",
      "         [ 0.9010, -0.9545, -0.9631,  0.8235, -0.0440]],\n",
      "\n",
      "        [[ 0.9210,  0.9137, -0.9387,  0.9157,  0.8990],\n",
      "         [-0.7193, -0.5068,  0.7319, -0.1808, -0.8827]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0989 cost = 0.000491\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9169, -0.8628, -0.9347,  0.8869, -0.1941],\n",
      "         [-0.4586,  0.6375,  0.0361, -0.3754,  0.6560]],\n",
      "\n",
      "        [[-0.7658, -0.5199,  0.7807, -0.2045, -0.9144],\n",
      "         [ 0.9002,  0.9276, -0.9247,  0.9254,  0.9028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0989 cost = 0.000560\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9170, -0.8628, -0.9347,  0.8869, -0.1941],\n",
      "         [-0.4586,  0.6375,  0.0361, -0.3753,  0.6560]],\n",
      "\n",
      "        [[-0.7658, -0.5200,  0.7807, -0.2046, -0.9145],\n",
      "         [ 0.9002,  0.9276, -0.9247,  0.9254,  0.9028]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0990 cost = 0.000560\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9011, -0.9545, -0.9631,  0.8236, -0.0442],\n",
      "         [-0.5262,  0.4998,  0.1002, -0.5062,  0.7052]],\n",
      "\n",
      "        [[-0.7193, -0.5068,  0.7320, -0.1809, -0.8827],\n",
      "         [ 0.8724,  0.8667, -0.8921,  0.8496,  0.8909]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0990 cost = 0.000745\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9603, -0.6982, -0.7522,  0.9438, -0.7091],\n",
      "         [-0.3830,  0.6302,  0.1358, -0.3232,  0.6106]],\n",
      "\n",
      "        [[-0.4568, -0.5364,  0.5770, -0.1823, -0.7261],\n",
      "         [ 0.9210,  0.9137, -0.9387,  0.9157,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0990 cost = 0.001056\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5263,  0.4998,  0.1002, -0.5062,  0.7053],\n",
      "         [-0.3831,  0.6302,  0.1358, -0.3231,  0.6107]],\n",
      "\n",
      "        [[ 0.8724,  0.8668, -0.8921,  0.8496,  0.8909],\n",
      "         [ 0.9210,  0.9137, -0.9387,  0.9157,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0991 cost = 0.000730\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9011, -0.9546, -0.9631,  0.8238, -0.0443],\n",
      "         [ 0.9603, -0.6984, -0.7523,  0.9438, -0.7092]],\n",
      "\n",
      "        [[-0.7194, -0.5068,  0.7321, -0.1811, -0.8827],\n",
      "         [-0.4569, -0.5364,  0.5770, -0.1823, -0.7261]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0991 cost = 0.001228\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9170, -0.8629, -0.9347,  0.8870, -0.1945],\n",
      "         [-0.4586,  0.6377,  0.0363, -0.3751,  0.6561]],\n",
      "\n",
      "        [[-0.7659, -0.5200,  0.7808, -0.2049, -0.9145],\n",
      "         [ 0.9002,  0.9277, -0.9247,  0.9254,  0.9029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0991 cost = 0.000557\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4586,  0.6377,  0.0363, -0.3750,  0.6561],\n",
      "         [-0.3828,  0.6303,  0.1359, -0.3230,  0.6107]],\n",
      "\n",
      "        [[ 0.9002,  0.9277, -0.9247,  0.9254,  0.9029],\n",
      "         [ 0.9210,  0.9138, -0.9387,  0.9157,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0992 cost = 0.000525\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9604, -0.6986, -0.7524,  0.9439, -0.7092],\n",
      "         [ 0.9171, -0.8630, -0.9347,  0.8871, -0.1946]],\n",
      "\n",
      "        [[-0.4570, -0.5365,  0.5771, -0.1825, -0.7262],\n",
      "         [-0.7660, -0.5201,  0.7809, -0.2051, -0.9145]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0992 cost = 0.001297\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9012, -0.9546, -0.9631,  0.8240, -0.0445],\n",
      "         [-0.5261,  0.5001,  0.1002, -0.5060,  0.7054]],\n",
      "\n",
      "        [[-0.7195, -0.5069,  0.7322, -0.1813, -0.8827],\n",
      "         [ 0.8724,  0.8668, -0.8921,  0.8495,  0.8909]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0992 cost = 0.000740\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5261,  0.5001,  0.1002, -0.5059,  0.7054],\n",
      "         [ 0.9012, -0.9546, -0.9631,  0.8240, -0.0446]],\n",
      "\n",
      "        [[ 0.8724,  0.8668, -0.8921,  0.8495,  0.8909],\n",
      "         [-0.7196, -0.5069,  0.7323, -0.1813, -0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0993 cost = 0.000701\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9604, -0.6989, -0.7525,  0.9439, -0.7093],\n",
      "         [-0.4586,  0.6379,  0.0364, -0.3748,  0.6562]],\n",
      "\n",
      "        [[-0.4570, -0.5366,  0.5772, -0.1826, -0.7262],\n",
      "         [ 0.9002,  0.9277, -0.9247,  0.9254,  0.9029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0993 cost = 0.001304\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3826,  0.6305,  0.1359, -0.3228,  0.6109],\n",
      "         [ 0.9171, -0.8630, -0.9348,  0.8871, -0.1950]],\n",
      "\n",
      "        [[ 0.9210,  0.9138, -0.9387,  0.9157,  0.8991],\n",
      "         [-0.7661, -0.5202,  0.7810, -0.2053, -0.9145]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0993 cost = 0.000474\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9171, -0.8631, -0.9348,  0.8872, -0.1950],\n",
      "         [-0.3826,  0.6306,  0.1359, -0.3228,  0.6109]],\n",
      "\n",
      "        [[-0.7661, -0.5202,  0.7810, -0.2054, -0.9145],\n",
      "         [ 0.9210,  0.9138, -0.9387,  0.9157,  0.8991]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0994 cost = 0.000355\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5261,  0.5002,  0.1003, -0.5058,  0.7055],\n",
      "         [-0.4587,  0.6379,  0.0366, -0.3746,  0.6562]],\n",
      "\n",
      "        [[ 0.8724,  0.8668, -0.8921,  0.8495,  0.8909],\n",
      "         [ 0.9002,  0.9277, -0.9247,  0.9254,  0.9029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0994 cost = 0.001144\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9604, -0.6992, -0.7526,  0.9439, -0.7094],\n",
      "         [ 0.9013, -0.9546, -0.9632,  0.8242, -0.0449]],\n",
      "\n",
      "        [[-0.4571, -0.5367,  0.5773, -0.1828, -0.7263],\n",
      "         [-0.7197, -0.5069,  0.7324, -0.1816, -0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0994 cost = 0.001546\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5260,  0.5003,  0.1003, -0.5057,  0.7055],\n",
      "         [ 0.9604, -0.6993, -0.7527,  0.9439, -0.7095]],\n",
      "\n",
      "        [[ 0.8724,  0.8668, -0.8921,  0.8495,  0.8909],\n",
      "         [-0.4572, -0.5367,  0.5773, -0.1829, -0.7263]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0995 cost = 0.001244\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3825,  0.6307,  0.1359, -0.3226,  0.6109],\n",
      "         [ 0.9172, -0.8631, -0.9348,  0.8872, -0.1954]],\n",
      "\n",
      "        [[ 0.9210,  0.9138, -0.9387,  0.9157,  0.8992],\n",
      "         [-0.7662, -0.5203,  0.7811, -0.2056, -0.9145]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0995 cost = 0.000472\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9014, -0.9546, -0.9632,  0.8244, -0.0450],\n",
      "         [-0.4586,  0.6381,  0.0367, -0.3744,  0.6562]],\n",
      "\n",
      "        [[-0.7198, -0.5070,  0.7325, -0.1817, -0.8828],\n",
      "         [ 0.9002,  0.9277, -0.9247,  0.9254,  0.9029]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0995 cost = 0.000550\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9014, -0.9547, -0.9632,  0.8245, -0.0451],\n",
      "         [ 0.9172, -0.8632, -0.9348,  0.8873, -0.1955]],\n",
      "\n",
      "        [[-0.7198, -0.5071,  0.7325, -0.1818, -0.8828],\n",
      "         [-0.7662, -0.5204,  0.7812, -0.2057, -0.9146]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0996 cost = 0.000554\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9604, -0.6996, -0.7528,  0.9440, -0.7096],\n",
      "         [-0.5260,  0.5004,  0.1003, -0.5056,  0.7056]],\n",
      "\n",
      "        [[-0.4572, -0.5369,  0.5774, -0.1831, -0.7263],\n",
      "         [ 0.8724,  0.8669, -0.8921,  0.8495,  0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0996 cost = 0.001668\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4587,  0.6382,  0.0367, -0.3742,  0.6562],\n",
      "         [-0.3824,  0.6308,  0.1360, -0.3225,  0.6109]],\n",
      "\n",
      "        [[ 0.9002,  0.9277, -0.9247,  0.9254,  0.9029],\n",
      "         [ 0.9210,  0.9138, -0.9387,  0.9157,  0.8992]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0996 cost = 0.000519\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3824,  0.6308,  0.1360, -0.3224,  0.6110],\n",
      "         [-0.5260,  0.5004,  0.1003, -0.5055,  0.7056]],\n",
      "\n",
      "        [[ 0.9210,  0.9138, -0.9387,  0.9157,  0.8992],\n",
      "         [ 0.8724,  0.8669, -0.8921,  0.8495,  0.8910]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0997 cost = 0.002610\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9604, -0.6998, -0.7530,  0.9440, -0.7096],\n",
      "         [ 0.9173, -0.8632, -0.9349,  0.8873, -0.1958]],\n",
      "\n",
      "        [[-0.4573, -0.5370,  0.5775, -0.1832, -0.7264],\n",
      "         [-0.7663, -0.5205,  0.7813, -0.2060, -0.9146]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0997 cost = 0.001280\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9015, -0.9547, -0.9632,  0.8247, -0.0453],\n",
      "         [-0.4587,  0.6384,  0.0368, -0.3741,  0.6563]],\n",
      "\n",
      "        [[-0.7199, -0.5071,  0.7326, -0.1820, -0.8828],\n",
      "         [ 0.9002,  0.9277, -0.9247,  0.9254,  0.9030]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0997 cost = 0.000547\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.4587,  0.6385,  0.0368, -0.3740,  0.6563],\n",
      "         [ 0.9604, -0.6999, -0.7530,  0.9440, -0.7097]],\n",
      "\n",
      "        [[ 0.9002,  0.9277, -0.9247,  0.9254,  0.9030],\n",
      "         [-0.4574, -0.5370,  0.5775, -0.1833, -0.7264]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0998 cost = 0.001103\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.5261,  0.5006,  0.1003, -0.5054,  0.7058],\n",
      "         [ 0.9173, -0.8632, -0.9349,  0.8874, -0.1960]],\n",
      "\n",
      "        [[ 0.8725,  0.8669, -0.8921,  0.8495,  0.8911],\n",
      "         [-0.7664, -0.5205,  0.7813, -0.2062, -0.9146]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0998 cost = 0.000660\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3825,  0.6310,  0.1360, -0.3223,  0.6111],\n",
      "         [ 0.9016, -0.9547, -0.9633,  0.8248, -0.0455]],\n",
      "\n",
      "        [[ 0.9211,  0.9138, -0.9387,  0.9158,  0.8993],\n",
      "         [-0.7199, -0.5072,  0.7327, -0.1822, -0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0998 cost = 0.000480\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3825,  0.6310,  0.1360, -0.3223,  0.6111],\n",
      "         [-0.5261,  0.5007,  0.1003, -0.5053,  0.7058]],\n",
      "\n",
      "        [[ 0.9211,  0.9138, -0.9387,  0.9158,  0.8993],\n",
      "         [ 0.8725,  0.8670, -0.8921,  0.8495,  0.8911]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0999 cost = 0.002599\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9605, -0.7001, -0.7532,  0.9440, -0.7097],\n",
      "         [-0.4588,  0.6387,  0.0369, -0.3738,  0.6564]],\n",
      "\n",
      "        [[-0.4575, -0.5372,  0.5776, -0.1834, -0.7264],\n",
      "         [ 0.9003,  0.9278, -0.9247,  0.9255,  0.9031]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0999 cost = 0.001284\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9173, -0.8632, -0.9349,  0.8874, -0.1963],\n",
      "         [ 0.9016, -0.9547, -0.9633,  0.8248, -0.0456]],\n",
      "\n",
      "        [[-0.7665, -0.5206,  0.7814, -0.2064, -0.9146],\n",
      "         [-0.7200, -0.5072,  0.7328, -0.1823, -0.8828]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 0999 cost = 0.000956\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9174, -0.8632, -0.9350,  0.8874, -0.1963],\n",
      "         [ 0.9605, -0.7002, -0.7532,  0.9440, -0.7098]],\n",
      "\n",
      "        [[-0.7665, -0.5207,  0.7815, -0.2065, -0.9146],\n",
      "         [-0.4576, -0.5373,  0.5777, -0.1835, -0.7264]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 1000 cost = 0.001274\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[-0.3826,  0.6311,  0.1360, -0.3222,  0.6111],\n",
      "         [-0.4589,  0.6388,  0.0370, -0.3737,  0.6564]],\n",
      "\n",
      "        [[ 0.9211,  0.9139, -0.9387,  0.9158,  0.8994],\n",
      "         [ 0.9003,  0.9278, -0.9247,  0.9255,  0.9031]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 1000 cost = 0.000836\n",
      "input.shape torch.Size([2, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 2, 5])\n",
      "lstm中output的shape torch.Size([2, 3, 10])\n",
      "batch_size: 2 torch.Size([2, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9017, -0.9547, -0.9633,  0.8250, -0.0458],\n",
      "         [-0.5262,  0.5010,  0.1003, -0.5052,  0.7060]],\n",
      "\n",
      "        [[-0.7201, -0.5073,  0.7329, -0.1825, -0.8828],\n",
      "         [ 0.8726,  0.8671, -0.8922,  0.8496,  0.8912]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 2, 5])\n",
      "attn_weights torch.Size([2, 3])\n",
      "soft_attn_weights torch.Size([2, 3])\n",
      "context torch.Size([2, 10])\n",
      "Epoch: 1000 cost = 0.000726\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(1000):\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred, attention = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "input.shape torch.Size([1, 3, 2])\n",
      "lstm中h0的shape torch.Size([2, 1, 5])\n",
      "lstm中output的shape torch.Size([1, 3, 10])\n",
      "batch_size: 1 torch.Size([1, 3, 10])\n",
      "hn final_state: tensor([[[ 0.9605, -0.7003, -0.7533,  0.9440, -0.7098]],\n",
      "\n",
      "        [[-0.4577, -0.5374,  0.5778, -0.1836, -0.7265]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([2, 1, 5])\n",
      "attn_weights torch.Size([1, 3])\n",
      "soft_attn_weights torch.Size([1, 3])\n",
      "context torch.Size([1, 10])\n",
      "tensor([[ 3.5253, -2.8781]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5253, -2.8781]])\n",
      "----------\n",
      "torch.Size([1, 1])\n",
      "i hate you : is Bad Mean...\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "# labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "test_text = 'i hate you'\n",
    "tests = [np.asarray([word2idx[n] for n in test_text.split()])]\n",
    "test_batch = torch.LongTensor(tests).to(device)\n",
    "print(test_batch.shape)\n",
    "# X: [batch_size, seq_len]\n",
    "# Predict\n",
    "predict, _ = model(test_batch)\n",
    "print(predict)\n",
    "print(predict.data)\n",
    "print('----------')\n",
    "predict = torch.max(predict.data,dim=1, keepdim=True)[1]\n",
    "print(predict.shape)\n",
    "if predict[0][0] == 0:\n",
    "    print(test_text, \": is Bad Mean...\")\n",
    "else:\n",
    "    print(test_text, \": is Good Mean!!\")\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.matshow(attention.cpu().data, cmap='viridis')\n",
    "# ax.set_xticklabels(['']+['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n",
    "# ax.set_yticklabels(['']+['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}